![Figure 1: We introduce MLLM-Guided Image Editing (MGIE) to improve instruction-based image editing for various editing aspects. The top is the input instruction, and the right is the jointly derived expressive instruction by MGIE.](images/mgie/figure_1.png)

*Figure 1: We introduce MLLM-Guided Image Editing (MGIE) to improve instruction-based image editing for various editing aspects. The top is the input instruction, and the right is the jointly derived expressive instruction by MGIE.*


## 1. Motivation of the Paper
Instruction-based image editing allows users to manipulate images with simple natural language commands (e.g., "make it more healthy"). However, current methods often struggle because these commands can be too brief and ambiguous. For example, "make it more healthy" could mean many different things. The models lack the commonsense reasoning to interpret the user's true intent in the context of the specific image. The authors identify this gap: the need for a model that can understand the visual context, reason about the user's ambiguous instruction, and generate a more explicit plan before performing the edit.

## 2. Key Ideas and Methodology
The paper introduces **MLLM-Guided Image Editing (MGIE)**, a novel framework that leverages a Multimodal Large Language Model (MLLM) to improve instruction-based editing.

*   **Core Hypothesis:** Instead of directly using a brief user instruction to guide an edit, an MLLM can first interpret the instruction in the context of the input image to derive a more detailed and explicit "expressive instruction". This expressive instruction serves as a much better guide for the editing model. For example, given an image of a pizza and the instruction "make it more healthy," the MLLM might generate the expressive instruction, "The pizza includes vegetable toppings, such as tomatoes and herbs."

*   **Methodology:** MGIE consists of two main components trained end-to-end:
    1.  **A Multimodal Large Language Model (MLLM):** This model takes the input image and the user's instruction. It is trained to output a concise, expressive instruction that captures the visual "imagination" of the intended edit. This is achieved by first prompting the MLLM to generate a detailed description, which is then summarized.
    2.  **A Latent Diffusion Model:** This model performs the actual image editing. Crucially, it is not guided by the original user command but by the expressive instruction from the MLLM. The expressive instruction is transformed into a visual guidance latent vector via an "edit head" and injected into the diffusion model's cross-attention layers, steering the editing process.

*   **Key Foundation:** The framework is built on the idea that MLLMs, pre-trained on vast amounts of text and image data, possess latent visual knowledge and reasoning capabilities. MGIE explicitly harnesses this to bridge the comprehension gap between ambiguous human commands and the precise guidance needed by diffusion-based editors.

## 3. Datasets Used / Presented
The model is pre-trained on **IPr2Pr**, a large-scale dataset with 1 million synthetic image-instruction-goal triplets. For comprehensive evaluation and fine-tuning, the authors use four diverse, human-annotated datasets:
*   **EVR:** 5.7K examples of Photoshop-style modifications.
*   **GIER:** 29.9K examples of global photo optimization (e.g., color, exposure).
*   **MA5k:** 24.8K examples of photo adjustments (e.g., contrast, brightness).
*   **MagicBrush:** 10.5K examples of local object alterations.

## 4. Main Results
*   **Quantitative:** In both zero-shot and fine-tuned settings, MGIE significantly outperforms the state-of-the-art baseline (InstructPix2Pix) and a simpler LLM-guided variant (LGIE) across all four evaluation datasets. For instance, on the challenging MagicBrush dataset (fine-tuned), MGIE improves the DINO visual similarity score from 87.99 (InstructPix2Pix) to 90.65 and the CLIP visual similarity (CVS) from 93.83 to 95.28.
*   **Human Evaluation:** Human evaluators consistently ranked MGIE's outputs as superior in three key areas: instruction following, relevance to the ground-truth image, and overall quality. Furthermore, the expressive instructions generated by MGIE were rated as more practical and less prone to hallucination compared to those from a text-only LLM.
*   **Takeaway Sentence:** The authors convincingly demonstrate that using an MLLM to derive explicit, visual-aware guidance from ambiguous instructions is a crucial step that leads to notable improvements in editing quality, realism, and faithfulness to user intent.

## 5. Ablation Studies
The paper presents several insightful ablation studies to validate its design choices.
*   **End-to-End Training:** The authors compared their end-to-end (E2E) training approach with two simpler alternatives: using the expressive instructions on a frozen, pre-trained editor (FZ) and only fine-tuning the editor (FT). The E2E approach performed best, showing that jointly training the MLLM and the diffusion model is critical for them to co-adapt and achieve optimal performance.
*   **Training Loss:** Removing the instruction loss (`L_ins`)—which explicitly trains the MLLM to generate concise guidance—caused a significant drop in performance. This underscores the importance of learning to produce high-quality expressive instructions, rather than relying on lengthy, unrefined MLLM outputs.
*   **Visual Perception:** MGIE (which sees the image) consistently outperformed LGIE (which only sees the text instruction) across all experiments. This confirms that visual context is essential for the MLLM to generate relevant and accurate guidance.
*   **Instruction-Input Trade-off:** The authors analyzed the trade-off between preserving the original image and following the edit instruction. MGIE demonstrated a superior trade-off curve, achieving better edit relevance for any given level of input image consistency compared to the baseline.

## Paper Figures
![Figure 2: Overview of MLLM-Guided Image Editing ( MGIE ), which leverages MLLMs to enhance instruction-based image editing. MGIE learns to derive concise expressive instructions and provides explicit visual-related guidance for the intended goal. The diffusion model jointly trains and achieves image editing with the latent imagination through the edit head in an end-to-end manner.](images/mgie/figure_2.png)

*Figure 2: Overview of MLLM-Guided Image Editing ( MGIE ), which leverages MLLMs to enhance instruction-based image editing. MGIE learns to derive concise expressive instructions and provides explicit visual-related guidance for the intended goal. The diffusion model jointly trains and achieves image editing with the latent imagination through the edit head in an end-to-end manner.*


![Figure 3: Trade-off curve for image editing . We set α X as 7.5 and vary α V in [1 . 0 , 2 . 2] . For both edit (X-axis) and input consistency (Yaxis), higher is better.](images/mgie/figure_3.png)

*Figure 3: Trade-off curve for image editing . We set α X as 7.5 and vary α V in [1 . 0 , 2 . 2] . For both edit (X-axis) and input consistency (Yaxis), higher is better.*


![Figure 4: CLIP-S across images (input / goal) and expressive instructions.](images/mgie/figure_4.png)

*Figure 4: CLIP-S across images (input / goal) and expressive instructions.*


![Figure 5: Human eval of expressive instructions quality.](images/mgie/figure_5.png)

*Figure 5: Human eval of expressive instructions quality.*


![Figure 6: Human eval of image editing results in terms of instruction following, ground-truth relevance, and overall quality.](images/mgie/figure_6.png)

*Figure 6: Human eval of image editing results in terms of instruction following, ground-truth relevance, and overall quality.*
