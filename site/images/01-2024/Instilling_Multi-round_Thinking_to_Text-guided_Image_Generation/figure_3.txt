Qualitative Comparison. We first qualitatively compare our method with two common generative models, e.g ., ControlNet and Stable Diffusion (SD). In the single-round generation, as shown in Figure 3 , the misalignment between text and the generated image is highlighted by a dashed box with the corresponding color. We observe three primary points. (1) The Stable Diffusion and ControlNet have more failure cases in capturing the detailed text meaning, such as editing sleeve length and changing the color depicted. (2) The main difference between the baseline and ControlNet is the semantic condition from the reference image. The enhancement in image generation and local editing is mainly due to the better preservation of semantic patterns by our condition fusion. (3) Besides, we could observe that the baseline model is also imperfect. For instance, the baseline misses keywords like “ruffles” with flat texture, while being sensitive to “casual” with an undefined logo. In contrast, the proposed method with the multi-round loss regularizes the training and shows a more consistent generation quality. On the other hand, we also evaluate these models in a multi-round generation by splitting the text as we elucidated in our methodology. Compared to the single-round generation, the split text is more challenging since it is more sparse in meaning. As shown in Figure 4 , Stable Diffusion and ControlNet lost the original reference information after two rounds of generation. Both models change the dress to the common category, i.e ., a T-shirt. In contrast, the baseline model performs well on “dress”, but it still misses the fine-grained “crinoline dress” pattern from the reference image. The main reason is that the intermediate results of the three methods, i.e ., Stable Diffusion, ContorlNet and baseline already miss some key patterns. Compared with these methods, our multi-round regularized model mainly changes color and texture in the second round of generation, maintaining consistency in local editing. We also could observe that the output result is controllable as it aligns well with the text modification. Quantitative Comparison. We study the generation quality and semantic alignment in this part, consolidating our observation from the qualitative evaluation. In particular, we train and evaluate models on FashionIQ and Fashion200k