Figure 6. Method comparison. We compare SDEdit [ 38 ], DDIB [ 62 ], and Text2LIVE [ 8 ] to our method. Imagic successfully applies the desired edit, while preserving the original image details well. into the 1024 ˆ 1024 resolution. By cascading these 3 models [ 23 ] and using classiﬁer-free guidance [ 24 ], Imagen constitutes a powerful text-guided image generation scheme. We optimize the text embedding using the 64 ˆ 64 diffusion model and the Adam [ 34 ] optimizer for 100 steps and a ﬁxed learning rate of 1 e ´ 3 . We then ﬁne-tune the 64 ˆ 64 diffusion model by continuing Imagen’s training for 1500 steps for our input image, conditioned on the optimized embedding. In parallel, we also ﬁne-tune the 64 ˆ 64 Ñ 256 ˆ 256 SR diffusion model using the target text embedding and the original image for 1500 steps, in order to capture high-frequency details from the original image. We ﬁnd that ﬁne-tuning the 256 ˆ 256 Ñ 1024 ˆ 1024 model adds little to no effect to the results, therefore we opt to use its pre-trained version conditioned on the target text. This entire optimization process takes around 8 minutes per image on two TPUv4 chips. Afterwards, we interpolate the text embeddings according to Equation 3 . Because of the ﬁne-tuning process, using η “ 0 will generate the original image, and as η increases, the image will start to align with the target text. To maintain both image ﬁdelity and target text alignment, we choose an intermediate η , usually residing between 0 . 6 and 0 . 8 (see Figure 9 ). We then generate with Imagen [ 53 ] with its pro-