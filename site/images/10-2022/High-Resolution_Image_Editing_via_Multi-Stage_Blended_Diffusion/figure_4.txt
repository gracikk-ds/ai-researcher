Figure 4: Comparison of our approach to two baselines. Left: we directly apply Blended Diffusion to the highest resolution we can ﬁt into the VRAM (960x960 pixels) and then bilinearly upscale the output. Middle: We use the Dall-E 2 web UI to edit the image at its full resolution. Due to the edited region being larger than the 1024x1024 generation window, we have to apply Dall-E to multiple independent segments. Right: Our proposed approach. We ﬁnd that directly applying Blended Diffusion leads to repeated elements (two heads, two mountains, two pictures) and fails to produce ﬁne details (hair). Dall-E 2 produces visually high-ﬁdelity images, but fails to produce globally coherent images (ﬂoating statues, four paintings). Our method produces globally consistent images while providing a similar visual ﬁdelity. Note that the full images are downscaled. The zoomed-in regions measure 512x512 pixels and are shown at full resolution.