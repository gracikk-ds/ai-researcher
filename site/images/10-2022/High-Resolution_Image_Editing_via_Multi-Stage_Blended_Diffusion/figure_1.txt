Figure 1: Our approach performs high-resolution text-guided image editing in multiple stages. In the ﬁrst stage a), we apply Blended Diffusion [2], given a masked region and a text prompt. In each following stage b), we ﬁrst upscale the image using an off the shelf super-resolution model and then use Blended Diffusion, starting at an intermediate diffusion step, to improve the image quality and ensure consistency with the input prompt. c) When the output resolution of a stage is too large to ﬁt into the GPU memory, we split the image into multiple segments, apply upscaling and Blended Diffusion to them separately and alpha-composite the results.