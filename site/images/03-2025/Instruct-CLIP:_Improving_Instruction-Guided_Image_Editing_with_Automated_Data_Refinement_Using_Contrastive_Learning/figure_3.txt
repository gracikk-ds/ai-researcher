Figure 3. Instruct-CLIP architectures. (a) Overview of Instruct-CLIP (I-CLIP), which embeds the visual change in the original/edited images I o and I e and the edit instruction p into the same feature space through contrastive loss, L contrast (Eq. 2 ). To obtain refined instruction p from its I-CLIP embedding z txt , we adopt the same approach in DeCap [ 12 ] to decode z txt back to p using cross-entropy loss, L DeCap (Eq. 4 ). At inference time, the text decoder takes the embedded visual change from the original to the edited image ( z vis ) and decodes it to produce a new instruction. Due to the significant cosine similarity gap between z vis and z txt even when they are well aligned, directly decoding z vis leads to suboptimal results. To achieve a representation of z vis closer to the text features that the instruction decoder learned during training, we compute ( z vis ) ′ with Eq. 6 and decode it to obtain the refined instruction p ′ , which is used to improve the dataset. (b) The architecture of image encoder I-CLIP vis includes two shared-weighted DINOv2 [ 18 ] modules in front of a standard CLIP vis encoder.