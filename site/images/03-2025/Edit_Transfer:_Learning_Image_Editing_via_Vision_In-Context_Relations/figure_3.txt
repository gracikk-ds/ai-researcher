Figure 3. Visual relation in-context learning for Edit Transfer. (a) We arrange in-context examples in a four-panel layout: the top row (an editing pair ( I s , I t ) ) and the bottom row (the query pair ( ˆ I s , ˆ I t ) ). Our goal is to to learn the transformation from I s →I t , and apply it to the bottom-left image ˆ I s , producing the target ˆ I t , in the bottom-right. (b) We fine-tune a lightweight LoRA in the MMA to better capture visual relations. Noise addition and removal are applied only to z t , while the conditional tokens c T ( derived from ( I s , I t , ˆ I s ) ) remain noise-free. (c) Finally, we cast Edit Transfer as an image generation task by initializing the bottom-right latent token z T with random noise and concatenating it with the clean tokens c I . Leveraging the enhanced in-context capability of the fine-tuned DiT blocks, the model generates I t , effectively transferring the same edits from the top row to the bottom-left image.