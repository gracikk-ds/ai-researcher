Figure 2. We analyze the deviation between fine-tuned weights θ τ f and pre-trained initialization θ p across different layers in the LDM (i.e., Φ τ = || θ τ f − θ p || ) and rank them accordingly. We present the average rank of these deviations across all tasks. The x-axis represents layer depth, while the y-axis indicates the component type. FFN layers show the highest deviation, suggesting they specialize in adapting to downstream tasks.