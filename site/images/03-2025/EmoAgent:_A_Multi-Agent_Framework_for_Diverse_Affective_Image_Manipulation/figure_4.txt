The Editing Agent serves as the executor within the EmoAgent framework—analogous to the ”hands” of an artist—responsible for translating abstract editing plans into concrete visual modifications. It leverages a comprehensive library of specialized editing tools, functioning as a diverse set of brushes, to faithfully implement each step of the editing process with precision and emotional alignment. Input-Output Schema. The Editing Agent takes the editing plan P = { P (1) , P (2) , . . . , P ( k ) } and the source image I o as input, producing emotionally consistent yet visually diverse edited images { I (1) t , I (2) t , . . . , I ( k ) t } aligned with the target emotion E t . Editing Tools Library. To support diverse visual transformations, the Editing Agent is equipped with an extensible library of tools categorized by input modality, as shown in Table I: (1) text-guided tools, (2) tools combining text and spatial masks, and (3) tools integrating spatial masks with reference images. Each category offers unique strengths for specific editing tasks, such as object insertion, background replacement, and fine-grained expression or attribute manipulation. To support precise, localized edits, we further incorporate auxiliary tools, including object detection [37] and segmentation models [38], which provide spatial priors and instance-level understanding. In addition, a VLM [35] is employed as a self-critic module to validate the semantic fidelity of the outputs. Consequently, the Editing Agent dynamically selects the most appropriate tool based on the semantics and complexity of each instruction, ensuring accurate and context-aware execution. Hierarchical Action Execution. To implement each editing plan P ( k ) , the Editing Agent executes a sequence of hierarchical actions that progressively refine the image. As illustrated in Fig. 4, each instruction Ins ( k ) n is handled through three structured stages: