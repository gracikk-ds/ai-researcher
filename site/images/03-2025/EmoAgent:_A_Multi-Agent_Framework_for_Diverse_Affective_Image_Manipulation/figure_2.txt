At the heart of EmoAgent is the Planning Agent, which reformulates the D-AIM task as a multi-solution generation problem . Instead of producing a single optimal edit, it aims to map the same target emotion E t to multiple semantically distinct yet emotionally consistent editing strategies . This diversity is achieved by transforming emotional cues into structured editing instructions via a combination of analysis, retrieval, and planning. Input-Output Schema. The Planning Agent takes as input a source image I o and a target emotion E t , and outputs a set of diverse editing plans P = { P (1) , P (2) , . . . , P ( k ) } . Each P ( k ) = { Ins ( k ) 1 , Ins ( k ) 2 , . . . } defines a coherent sequence of editing instructions, representing a distinct transformation path. Together, these plans offer multiple visually diverse solutions to express the same emotional intent. Emotion-Grounded Analysis and Retrieval. The Planning Agent begins by using a VLM (e.g., Qwen2.5-VL [35]) to extract semantic cues S o and the source emotion E o from the input image I o . These cues guide a Retrieval-Augmented Generation (RAG) [36] process to query an external Emotion-Factor Knowledge (EFK) database, constructed from EmoSet [34] and organized into emotion–element–method triples, as illustrated in Fig. 2(b). Unlike prior approaches that retrieve a single best match, we extract a diverse topk pool of editing