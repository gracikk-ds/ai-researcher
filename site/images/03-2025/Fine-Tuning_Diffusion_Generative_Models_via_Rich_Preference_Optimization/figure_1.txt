Figure 1: (Top) Our RPO pipeline for curating informative preference pairs from images generated from the base diffusion models: (1) Rich Feedback/Critic generation by a Vision Language Model (for which we choose LLaVA-Critic-7B), (2) Actionable editing instruction generation based on the critiques by another VLM (for which we chose Qwen2.5-VL-8BInstruct), (3) Instruction-following image editing from the generated editing instructions (for which we choose ControlNet), and (4) Diffusion DPO training using reward model filtered synthetic preference pairs. (Bottom) Sample images generated from RPO fine-tuned Stable Diffusion XL, by further aligning the model on our generated synthetic preferences.