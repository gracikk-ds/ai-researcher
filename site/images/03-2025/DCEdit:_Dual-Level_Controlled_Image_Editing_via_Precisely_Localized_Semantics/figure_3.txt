Figure 3. Illustration of PSL. The attention map A joint in each MM-DiT layer can be split into four parts. Among them the ˜ A V → T can reflect the corresponding areas of semantics in the image. We further correct this attention map through visual and textual self-attention parts ˜ A V → V and ˜ A T → T and generate the refined attention map M . Notably, the red anchor box on ˜ A T → T marks the competition for attention among text tokens, which results in confusion between semantics. However, as shown by the green anchor box, the inverse of ˜ A T → T can offset this confusion and help correct the semantic errors in the attention map.