Figure 2: Overview of the IMAGHarmony framework. Given a reference image and a text instruction, we first sample multiple candidate noise seeds and evaluate their semantic alignment using a vision-language model (VLM). The topk candidates are retained for inference. The harmony-aware attention (HA) module fuses auxiliary textual and visual features to jointly model object count and spatial layout. An image prompt (IP) attention layer injects these layout-aware features into the UNet backbone without modifying its frozen weights. During inference, the preference-guided noise selection (PNS) strategy selects the best candidate seed to guide the full denoising process, leading to edited images that are consistent in both quantity and layout.