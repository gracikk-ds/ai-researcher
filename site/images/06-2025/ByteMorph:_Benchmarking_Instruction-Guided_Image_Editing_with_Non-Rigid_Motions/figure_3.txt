Figure 3 Overview of ByteMorpherâ€™s training. We fine-tuned the Diffusion Transformer backbone from the pre-trained Flux.1-dev [ 3 ] text-to-image model on the ByteMorph-6M. We feed the source image and target image into the same frozen VAE encoder and obtain source and target latents. The position encoding for both latents is shared with exactly the same embeddings. The target latent is added with noise and further concatenated with the source latent along the sequence length dimension. The DiT is fine-tuned with the MSE loss, where the difference between the noise and the latter half of the output from DiT (corresponding to the noisy target latent input) is minimized.