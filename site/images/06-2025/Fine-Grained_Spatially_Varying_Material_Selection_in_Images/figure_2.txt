Fig. 2. Model architecture. Our model extracts ViT features at different resolutions, for the full image and for each separate tile, leading to a multi-resolution feature encoding. The subsequent spatial processing layers upscale the features before the cross-similarity computes the attention with respect to the clicked image pixel and patch. We exploit the information encoded at different depths by repeating this process across four ViT levels, before fusing the output with a residual CNN and feeding it to our two-level selection head, producing selections at both subtexture and texture level. The ViT is frozen, the red blocks are trained. FC is short for fully-connected network.