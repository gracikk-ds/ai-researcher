Figure 5: We leverage in-context learning to take advantage of the few-shot generalization capabilities of LLMs. We place a relatively small number ( â‰ˆ 15 ) examples for our task in the context of an LLM. Each example contains (a) a serialized image layout, (b) a serialized instruction, (c) a chain of thought composed of multiple task-relevant questions meant to assist the LLM by providing it with additional context, and (d) an annotated layout specifying the relevant transformation. At inference time we place an input image layout and instruction after the in-context examples.