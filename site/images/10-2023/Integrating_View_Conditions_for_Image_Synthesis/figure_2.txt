Figure 2: An illustrative overview of our method, which is designed for synthesizing an object with a user-specified view into a scene. ”3D?” denotes whether a 3D model is available. Our approach consists of three components: Large Language Model (LLM) Planner (Sec. 3.1), Pose Estimation and Synthesis (Sec. 3.2), and Image Synthesis (Sec. 3.3). First, the LLM Planner is adopted to obtain the objects’ names and pose information based on the user’s input. Second, a segmentation module is adopted to remove the background from the specific object, followed by a pose estimation module to obtain its accurate pose. A pose synthesis module is then applied to synthesize the reference object respecting specific view conditions. Third, a personalized pre-trained diffusion model and ControlNets are adopted to produce the final synthesis. They ensure that the target object harmoniously melds with its surroundings, aligning with the user-specified view, while maintaining consistency in the object’s representation. Flames and snowflakes refer to learnable and frozen parameters, respectively.