but it is both time-consuming and inefficient, and may generate inconsistent views. Moreover, due to the unique depth and spatial characteristics of omnidirectional images, conventional 2D editing models struggle to understand the spatial relationships between viewports or even within individual ODI views, making the splitand-edit approach impractical. Therefore, dedicated ODI editing dataset and method are necessary to advance research in this domain. Built upon the rapid progress in 2D image generation and editing, integrating these tasks into a unified framework has become increasingly popular [ 31 , 37 , 39 ]. In this paper, we aim to unify ODI generation and editing into an efficient model. To this end, we first introduce Any2Omni , the first comprehensive dataset for omnidirectional image generation and editing tasks. As shown in Table 1, our dataset integrates multiple input modalities for ODI generation tasks. For the newly defined omnidirectional image editing tasks, we start by proposing a simple yet effective pipeline capable of generating high-quality, object-level indoor editing samples. Additionally, we introduce two scene-level ODI editing tasks utilizing existing ODI datasets [ 44 ]. Overall, our Any2Omni dataset comprises over 60,000 training samples, covering 9 categories of omnidirectional image generation and editing tasks with various input conditions. Based on Any2Omni, we further introduce the first omni model for omnidirectional image generation and editing, termed Omni 2 . Omni 2 adopts a simple yet effetive Transformer-based framework to support 360 ◦ × 180 ◦ high-quality omnidirectional image synthesis under a variety of multimodal input conditions. In contrast to existing diffusion-based ODI generation methods, which incorporate additional attention blocks for multi-view consistency [ 29 , 42 ], we introduce a novel approach by executing viewport-based bidirectional attention within a unified Transformer. Our model demonstrates superior performance across a wide range of ODI generation and editing tasks with various input conditions, as shown in Fig. 1. The main highlights of this work include: