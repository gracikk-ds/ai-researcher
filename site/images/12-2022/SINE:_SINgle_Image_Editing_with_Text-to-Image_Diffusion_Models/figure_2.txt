Figure 2. Overview of our method. (a) Given a source image, we first randomly crop it into patches and get the corresponding latent code z with the pre-trained encoder. At fine-tune time, the denoising model, ϵ θ , takes three inputs: noisy latent z T , language condition c , and positional embedding for the area where the noisy latent is obtained. (b) During sampling, we give additional language guidance about the target domain to edit the image. Also, we sample a noisy latent code z T with the dimension corresponding to the desired output resolution. Language conditioning for ϵ θ and c are given by pre-trained language encoder τ θ with the target language guidance. While for the fine-tuned diffusion model, ˆ ϵ θ , in addition to the language conditioning ˆc , we also input the positional embedding for the whole image. We employ a linear combination between the score calculated by each model for the first K steps and inference only on pre-trained ϵ θ after.