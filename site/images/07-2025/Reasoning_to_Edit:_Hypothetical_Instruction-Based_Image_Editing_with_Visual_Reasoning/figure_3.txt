Figure 3: The overall framework of ReasonBrain. Given an input image I and a hypothetical instruction H , ReasonBrain first encodes them into multi-scale visual features and textual tokens using the image encoder E I ( · ) and text encoder E T ( · ) , respectively. These features are then passed into the FRCE module to learn detailed reasoning cues. Subsequently, all learned features are fed into the MLLM to generate visual guidance, which is further transformed via a QFormer to align with the diffusion model’s latent space. Finally, the resulting visual guidance interacts with the previously extracted fine-grained cues through a CME module to enhance semantic representation, which is then used to condition the diffusion model for final image generation.