Figure 2. Our method consists of two parts: generating an image editing dataset, and training a diffusion model on that dataset. (a) We ﬁrst use a ﬁnetuned GPT-3 to generate instructions and edited captions. (b) We then use StableDiffusion [ 52 ] in combination with Promptto-Prompt [ 17 ] to generate pairs of images from pairs of captions. We use this procedure to create a dataset (c) of over 450,000 training examples. (d) Finally, our InstructPix2Pix diffusion model is trained on our generated data to edit images from instructions. At inference time, our model generalizes to edit real images from human-written instructions.