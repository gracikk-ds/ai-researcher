model architectures which handle different editing subtasks [Couairon et al., 2023, Zhang et al., 2023a]. However, neither of these approaches includes edits requiring more holistic visual understanding of how humans and objects interact or how events unfold , such as ‘make the cook cut the apple in half’ or ‘make the dog jump in the air’ (see Fig. 1). These more action-centric edits are severely understudied in the space of instruction-tuned image editing models [Brooks et al., 2023, Huang et al., 2024]; when they are considered, it is done in isolation, ignoring other image edit subtasks and rigorous semantic evaluation [Souˇcek et al., 2023, Black et al., 2024]. In Sec. 2 we describe a typology of these edit types and how existing datasets currently fail to address them all.