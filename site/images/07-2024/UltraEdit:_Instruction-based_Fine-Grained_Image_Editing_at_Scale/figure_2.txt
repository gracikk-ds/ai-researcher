Stage III: Region-based Editing Samples. Upon the free-form editing samples, we create additional region-based editing data using an automatic editing region extraction method. Given an imageinstruction pair ⟨ I ⋆ , T e ⟩ , we first detect all objects within I ⋆ using recognize-anything [ 74 ] and prompt LLM with the object list , soruce caption T s , target caption T t and editing instruction T e to identify the object to be edited. Then we employ GroundingDINO [ 38 ] and SAM [ 31 ] to obtain bounding boxes and fine-grained mask of this target object. For edits involving transformation over the entire image ( e.g ., “turn this into an oil paint”), we mark the whole image as the editing region. We save an expanded version of the original mask (which becomes a contour) as the editing region I m . Finally, the bounding box and fine-grained mask will be fused into a soft mask (see Figure 2) to help smooth the transition between inpainting area and the rest of the image. While producing the source image I s is the same as in Stage II, we adopt a modified inpainting pipeline to produce the target image I t to take the editing region I m into consideration: