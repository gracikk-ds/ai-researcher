Figure 2: Dynamic Prompt Learning ( DPL ) first transforms the noun words in the text prompt into dynamic tokens. We register them in the token dictionary and initialize the representations by their original words at time T . Using DDIM inversion, we condition the DM-UNet with the original text condition C to get the diffusion trajectory { z t } T 0 and background mask B . In each denoising step ¯ z t − 1 → ¯ z t , we first update the dynamic token set V t with a background leakage loss, a disjoint object attention loss and an attention balancing loss, in order to ensure high-quality cross-attention maps. Then we apply Null-Text inversion to approximate the diffusion trajectory.