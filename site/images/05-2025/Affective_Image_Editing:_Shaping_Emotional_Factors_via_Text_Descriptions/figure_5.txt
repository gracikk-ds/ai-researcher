Fig. 5 Overview of the proposed AIEdiT model. (a) To represent universal emotional priors encapsulated in text descriptions, we build the continuous emotional spectrum and extract nuanced emotional requests (Sec. 4.1 ); (b) To manipulate emotional factors, We design the emotional mapper to translate visually-abstract emotional requests into visually-concrete semantic representations (Sec. 4.2 ); (c) To ensure specific emotions are effectively evoked, we introduce the MLLM to supervise emotional understanding (Sec. 4.3 ); During inference, we strategically distort the visual semantic representations and shape the corresponding emotional factors unser the guidance of text descriptions (Sec. 4.4 ). The emotional mapper is designed as a multi-modal Transformer, facilitating global interaction among input conditions, composed of a stack of Multi-head Self-Attention (MSA) blocks, Multi-head Cross-Attention (MCA) blocks, and Feed-Forward Networks (FFN). During the training, we keep the latent diffusion model frozen to preserve its generative ability.