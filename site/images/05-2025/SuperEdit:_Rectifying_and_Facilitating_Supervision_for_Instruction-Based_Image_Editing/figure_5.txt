Figure 5. (a) Based on the rectified editing instruction and original-edited image pair, we utilize the Vision-Language Models (VLM) to generate various image-related wrong instructions. These involve random substitutions of quantities, spatial locations, and objects within the rectified editing instructions according to the original-edited images context; (b) During each training iteration, we randomly select one wrong instruction c T neg and input it along with the rectified instruction c T pos into the editing model to obtain predicted noises. The goal is to make the rectified instruction’s predicted noise ϵ pos closer to the sampled training diffusion noise ϵ , while ensuring the noise from incorrect instructions ϵ neg is farther. Best viewed in color.