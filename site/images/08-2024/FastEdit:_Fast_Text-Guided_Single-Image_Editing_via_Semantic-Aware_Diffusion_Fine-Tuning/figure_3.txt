Figure 3. Illustration of FastEdit. Given an input image and a target text, we first project them into features using the CLIP model. Then, we calculate the semantic discrepancy between the two to determine the denoising time steps. Further, we fine-tune the lowrank matrixes added to diffusion model for a few iterations. Lastly, we can interpolate the CLIP’s features to generate desired images with the fine-tuned model. • [Text Optimization] The two-minute duration required to optimize text embedding for each image seems unnecessary , as we can project the input image and the target text into the same latent feature space. When applying the fine-tuned model to the same input image but a different target text, the text features and diffusion model can be reused without any adaptation.