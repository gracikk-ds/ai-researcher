Figure 3: Our framework . (a) Given an example before-and-after image pair, we optimize the latent text instruction that converts the “before” image to the “after” image using a frozen image editing diffusion model. (b) We leverage the CLIP embedding space to help learn the editing direction. (c) Once learned, the instruction can be applied to a new image to achieve the same edit. Optionally, the user can also combine the learned instruction with a natural text prompt to create a hybrid instruction.