Figure 5: The overall framework of our approach. The original image x is first encoded into an image condition by the VAE encoder. This image condition c I is then concatenated with latent noise Z t to serve as the query for the reward encoder, with the reward condition c R as the key/value. The resulting latent noise, containing reward information, is used as the input for the U-Net module. Meanwhile, the instruction is encoded into a text condition c T by the text encoder, which is fed into each block of the U-Net. To further enhance reward guidance, we incorporate the reward condition after each block. Finally, the U-Netâ€™s output is decoded by the VAE decoder into the edited image y .