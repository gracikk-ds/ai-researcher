Figure 6. Visual result comparison. The first two columns present the edge and color conditions for editing, while the last column shows the ground truth image that the models aim to recreate. SmartEdit [ 25 ] utilizes natural language for guidance, but lacks precision in controlling shape and color, often affecting non-target regions. SketchEdit [ 79 ], a GAN-based approach [ 20 ], struggles with open-domain image generation, falling short compared to models with diffusion-based generative priors. Although BrushNet [ 28 ] delivers seamless image inpainting, it struggles to align edges and colors simultaneously, even with ControlNet [ 81 ] enhancement. In contrast, our Editing Processor strictly adheres to both edge and color conditions, achieving high-fidelity conditional image editing.