Figure 2: Training and inference process of our proposed universal text editing diffusion model. (a) Given an image, we first extracted all the text and corresponding bounding boxes by the OCR detector. Then, a random area is selected and the corresponding mask and glyph image are generated. We use the embedding of the glyph image extracted by the glyph encoder as the condition, and concatenate the masked image latent vector 𝑥 𝑚 , mask 𝑚 , and noisy image latent vector 𝑧 𝑡 as the input of the model. (b) Users can directly input the content they want to edit, and the large language model will understand their needs and provide the areas to be edited and the target text to DiffUTE, which then completes the text editing.