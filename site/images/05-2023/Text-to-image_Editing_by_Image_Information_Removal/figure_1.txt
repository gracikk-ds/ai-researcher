Figure 1. We aim to edit the specific content of the input image according to text descriptions while preserving text-irrelevant image content. Prior work based on large-scale diffusion models has followed two major approaches for image editing: (A), finetuning the pretrained models or text embeddings ( e.g ., Imagic [ 9 ] or Dreambooth [ 29 ]), or (B), introducing structural guidance as additional constraint to control the spatial information of the generated image ( e.g ., ControlNet [ 40 ] or MaskSketch [ 2 ]). In our work, shown in (C), our approach conditions on both the original image and the structural guidance, to better preserve the text-irrelevant content of the image. E.g ., our model successfully preserves the original attributes of the airplane (outlined by the green bounding box) in the generated image. In contast, previous methods such as Imagic (A) and ControlNet (B) not only alter the sky and background but also modify the attributes of the airplane (outlined by the red bounding boxes), which is unwanted in this example.