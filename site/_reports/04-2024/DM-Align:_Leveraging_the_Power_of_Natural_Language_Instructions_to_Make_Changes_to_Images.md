---
title: DM-Align:_Leveraging_the_Power_of_Natural_Language_Instructions_to_Make_Changes_to_Images
layout: default
date: 2024-04-27
---
![Figure 1: The proposed image editor utilizes a source caption to describe the initial image and a target text instruction to define the desired edited image. To accomplish this task, we employ the two captions to generate a diffusion mask, refining it further by incorporating regions of words that we want to keep or alter in the image.]({{ '/images/04-2024/DM-Align:_Leveraging_the_Power_of_Natural_Language_Instructions_to_Make_Changes_to_Images/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing text-based image editing models often function as "black boxes," leading to unexpected outputs. They particularly struggle with two key issues: preserving the background content of the image and accurately following long, complex natural language instructions. This paper addresses the need for a more transparent and controllable image editing model that can explicitly reason about which parts of an image to change and which to keep unaltered.

## 2. Key Ideas and Methodology
The core idea of the proposed model, **DM-Align (Diffusion Masking with word Alignments)**, is to leverage the semantic difference between two text captions: a *source caption* describing the original image and a *target caption* describing the desired edit.

The methodology follows a five-step pipeline:
1.  **Word Alignment:** A neural semi-Markov CRF model aligns words between the source and target captions to identify which concepts are preserved, substituted, or deleted.
2.  **Image Segmentation:** The aligned nouns are used with a Grounded-SAM model to segment the corresponding object regions in the source image.
3.  **Global Diffusion Mask:** A coarse mask is generated by computing the difference between the noise estimates of a diffusion model conditioned on the source and target captions, respectively.
4.  **Mask Refinement:** The global mask is refined by incorporating the precise object regions from the segmentation step. This involves expanding the mask for altered regions and shrinking it for preserved regions. A "noise cancellation" technique is applied to preserved areas to prevent unintended changes.
5.  **Inpainting:** The final, refined mask is used with a stable diffusion inpainting model to generate the edited image based on the target caption.

## 3. Datasets Used / Presented
The model was evaluated on three datasets with varying caption complexity:
*   **Dream:** A custom-generated set of 100 images from Dream by WOMBO, featuring short text instructions (≤11 words).
*   **Bison:** A standard text-to-image evaluation dataset with more elaborate captions.
*   **Imagen:** A collection of images generated by the Imagen model, described by complex captions with up to 23 words.

## 4. Main Results
*   **Quantitative Analysis:** DM-Align consistently achieved superior performance on image-based metrics (FID, LPIPS, PWMSE) compared to baselines, indicating better preservation of the original image's structure and quality. The performance gap was most significant on the Bison and Imagen datasets, which feature longer and more complex instructions. For instance, on the Imagen dataset, DM-Align improved FID by 23.4% over the best baseline.
*   **Background Preservation:** The model significantly outperformed all baselines in preserving background content, reducing the background FID score by up to 73% on the Bison dataset.
*   **Human Evaluation:** In a study with human annotators, DM-Align was rated higher than all baselines on editing quality, background preservation, and overall image quality.
*   **Author Takeaway:** By leveraging explicit word alignments, DM-Align provides superior text-based control for semantic image editing, especially for complex instructions, while robustly preserving the background.

## 5. Ablation Studies
Ablation tests were performed on the Imagen dataset to validate the contribution of each component of the DM-Align pipeline.
*   **Refinement of Diffusion Mask:** Removing the refinement step (which uses segmented regions) had the most significant negative impact on image similarity metrics (FID, LPIPS, PWMSE), showing its critical role in precise editing.
*   **Global Diffusion Mask:** Removing the initial global diffusion mask resulted in distorted and unnatural outputs, especially when replacing an object with a larger one, as it fails to provide context for the edit.
*   **Noise Cancellation:** Removing noise cancellation for preserved regions led to poorer background preservation.
*   **Handling Modifiers/Deleted Nouns:** Omitting the logic for adjective modifiers or deleted nouns resulted in incorrect edits (e.g., not changing an object's color) or the introduction of random, irrelevant content into the image.

## 6. Paper Figures
![Figure 2: The implementation of DM-Align. The aim is to update the input image described by the text instruction c 1 (“A clear sky and a ship landed on the sand") according to the text instruction c 2 (“A clear sky and a ship landed on the ocean").]({{ '/images/04-2024/DM-Align:_Leveraging_the_Power_of_Natural_Language_Instructions_to_Make_Changes_to_Images/figure_2.jpg' | relative_url }})
![Figure 3: Semantic image editing: Imagen dataset. Source captions: (1) c 1 . A photo of a British shorthair cat wearing a cowboy hat and red shirt riding a bike on a beach. (2) c 1 . An oil painting of a raccoon wearing sunglasses and red shirt playing a guitar on top of a mountain. (3) c 1 . An oil painting of a fuzzy panda wearing sunglasses and red shirt riding a bike on a beach.]({{ '/images/04-2024/DM-Align:_Leveraging_the_Power_of_Natural_Language_Instructions_to_Make_Changes_to_Images/figure_3.jpg' | relative_url }})
![Figure 4: Word alignment example. Blue: identical words, Purple: substituted words, Green: nouns with different modifiers, Red: nouns mentioned only in the source caption c 1 .]({{ '/images/04-2024/DM-Align:_Leveraging_the_Power_of_Natural_Language_Instructions_to_Make_Changes_to_Images/figure_4.jpg' | relative_url }})
![Figure 5: Semantic image editing: Bison dataset. Source captions: (1) c 1 . A man standing next to a baby elephant in the city. (2) c 1 . A wooden plate topped with sliced meat and vegetables. (3) c 1 . A vase filled with red and white flowers.]({{ '/images/04-2024/DM-Align:_Leveraging_the_Power_of_Natural_Language_Instructions_to_Make_Changes_to_Images/figure_5.jpg' | relative_url }})
![Figure 6: Semantic image editing: Dream dataset. Source captions: (1) c 1 . A soldier in front of a building. (2) c 1 . A pot with flowers. (3) c 1 . A girl throwing a volleyball.]({{ '/images/04-2024/DM-Align:_Leveraging_the_Power_of_Natural_Language_Instructions_to_Make_Changes_to_Images/figure_6.jpg' | relative_url }})
