---
title: COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design
layout: default
date: 2023-11-28
---
![Figure 1. Illustrating the multi-layered graphic design images generated by our C OLE system (first row, we display the multi-layer image layers at the top-right corner of each design image) and the combination of DALL · E3 background images and C OLE system (second row). See the appendix for detailed intention prompts. As shown in the second row, our C OLE system skillfully plans design layouts and selects harmonious fonts, colors, sizes, and positions through insightful analysis and reasoning, even with out-of-domain DALL · E3 background images after pre-processing. By default, we do not use DALL · E3 background images in all other results.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing state-of-the-art image generation models like DALL-E3 are not optimized for professional graphic design. They face three key challenges: 1) they often produce inaccurate or garbled text, 2) their output is a flat, uneditable image, making layer-wise adjustments difficult, and 3) they require users to write highly detailed, expert-level prompts to achieve good results. The authors aim to bridge this gap by creating an autonomous system that can transform a simple, vague user intention into a high-quality, multi-layered, and editable graphic design with accurate typography.

## 2. Key Ideas and Methodology
The core idea is to decompose the complex text-to-design process into a hierarchical framework of specialized, collaborative models. This system, named COLE, orchestrates the generation process in distinct stages:

1.  **Design-LLM (Intention Planning):** A fine-tuned Llama 2 model interprets the user's vague intention and generates a structured JSON file. This file acts as a detailed blueprint, containing layer-wise captions (for background, objects), text content, and other design attributes.
2.  **Background Generation:** A fine-tuned diffusion model (DeepFloyd/IF) generates a background image based on its caption from the JSON, specifically trained to leave empty "canvas placeholders" for other elements.
3.  **Object Generation:** A cascaded diffusion model generates an object layer and its corresponding alpha mask, conditioned on the background image. This ensures the object is stylistically coherent and logically placed.
4.  **Typography Generation:** A Large Multimodal Model (Typography-LMM, based on LLaVA-1.5) analyzes the combined background and object image and predicts 15 distinct typography attributes (font, size, color, position, etc.) for each text element.
5.  **Rendering and Editing:** The generated layers are compiled into a single, multi-layered SVG file, allowing for flexible, layer-wise user editing of elements like text and objects.
6.  **Refinement Loop:** A Reflect-LMM is used to analyze the initial output and predict adjustments to the JSON file to further improve the design quality.

## 3. Datasets Used / Presented
The authors curated a large-scale dataset from ~100,000 high-quality graphic design images sourced from the internet. This raw data was processed to create training sets for each specialized model, including `(intention, JSON)` pairs, `(caption, image)` pairs for background and object layers, and `(image, text, typography)` triplets.

Additionally, they introduce a new evaluation benchmark:
*   **DESIGNERINTENTION:** A benchmark consisting of ~500 professional graphic design intention prompts across 5 categories (e.g., marketing, posts, events) and creative prompts. It is used to perform a multi-faceted comparison of COLE against other state-of-the-art systems.

## 4. Main Results
COLE demonstrates superior performance over existing systems in generating high-quality, editable graphic designs from simple intentions.

*   **User Studies:** In a comparison with DALL-E3, professional designers preferred COLE's output 70.5% of the time for text fidelity and message conveyance. COLE was also preferred by designers over the commercial tool CanvaGPT.
*   **GPT-4V Evaluation:** On the DESIGNERINTENTION benchmark, COLE achieved leading performance in three critical dimensions: design layout, typography/color, and innovation, when compared against DALL-E3, SDXL, and CanvaGPT.
*   **Typography Placement:** The Typography-LMM, in a zero-shot setting on the Crello benchmark, outperformed the previous state-of-the-art model (FlexDM), improving the IoU score from 35.7 to 40.2 for single-text placement and 11.0 to 17.2 for multi-text placement.

The authors claim that COLE is a pioneering work that successfully tackles the complex task of autonomous, multi-layered graphic design generation.

## 5. Ablation Studies
The authors performed several ablation studies to validate the contribution of each component in the hierarchical framework.

*   **Text-to-Object Model:** Adding a "composed image" prediction target during the object generation stage significantly improved the model's ability to plan layouts and place objects in logically and stylistically coherent positions on the background.
*   **Text-to-Background Model:** A qualitative comparison showed that COLE's specialized background generator is more effective at creating "canvas placeholders" (i.e., backgrounds with adequate empty space) than general-purpose models like DALL-E3, which tend to fill the entire image.
*   **Reflect-LMM:** Using the Reflect-LMM to refine the initial typography predictions led to a measurable improvement in text box localization accuracy. For instance, the AP50 metric increased from 6.89 to 8.59.
*   **Design LLM:** The custom-trained Design-LLM significantly outperformed general-purpose models like GPT-3.5 and GPT-4 in generating a structured JSON from a user intention, achieving a higher semantic similarity score to the ground truth (e.g., 68.57 vs. 59.56 for GPT-4 on global caption generation).

## 6. Paper Figures
![Figure 10. Illustrating the GPT-4V(ision) scores. We mark the results of our C OLE ( ● ), DeepFloyd/IF+GPT-4 ( ● ), SDXL+GPT-4 ( ● ), DALL · E3 +GPT-4 ( ● ) and CanvaGPT ( ● ), using colored markers. (See the appendix for the zoomed-out version)]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_10.jpg' | relative_url }})
![Figure 11. Comparison to the state-of-the-art systems on D ESIGNER I NTENTION benchmark. We choose the DeepFloyd/IF-XL and SDXL-1.0 + SDXLRefiner-1.0 models due to their superior performance. The superscript † symbol indicates that GPT4 is used to convert user intentions into detailed captions.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_11.jpg' | relative_url }})
![Figure 12. More qualitative results generated by our C OLE system.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_12.jpg' | relative_url }})
![Figure 13. Typography-LMM predictions ( 2 -ed and 4 -th columns) vs. ground-truth ( 1 -st and 3 -th columns).]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_13.jpg' | relative_url }})
![Figure 14. Illustrating the variety within the graphic design images generated by our system when simply using different seeds for the diffusion models.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_14.jpg' | relative_url }})
![Figure 2. Illustrating the design images generated by DALL · E3 (augmented with GPT4 ), using our D ESIGNER I NTENTION .]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_2.jpg' | relative_url }})
![Figure 3. Comparison with DALL · E3 and CanvaGPT based on user study.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_3.jpg' | relative_url }})
![Figure 5. Illustrating the example of generated intention-to-JSON pairs data for the given image. We can see that the user intention is vague and the JSON file is much more informative. Readers are kindly suggested to zoom into the figure for a clearer view. The image originates from our training dataset.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_5.jpg' | relative_url }})
![Figure 6. Illustrating the decomposition of the graphic design images into the combination of background image layers, object image layers, and object alpha masks.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_6.jpg' | relative_url }})
![Figure 7. Illustrating the Typography JSON file. The text highlighted in red is predicted by the Design LLM. The image on the left depicts the effects of rendering visual text accordingly.]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_7.jpg' | relative_url }})
![Figure 9. Examples of flexible user editing. Left to right: original image (1-st, 5-th), object editing (2-ed, 6-th), text editing (3-rd, 7-th), and combined editing (4-th, 8-th).]({{ '/images/11-2023/COLE:_A_Hierarchical_Generation_Framework_for_Multi-Layered_and_Editable_Graphic_Design/figure_9.jpg' | relative_url }})
