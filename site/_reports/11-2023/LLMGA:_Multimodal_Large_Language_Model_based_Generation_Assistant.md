---
title: LLMGA:_Multimodal_Large_Language_Model_based_Generation_Assistant
layout: default
date: 2023-11-27
---
![Fig. 1: Some examples of LLMGA for assisting in image generation and editing. (1) T2I generation. LLMGA can refine the user’s generation prompt to produce more vivid and vibrant images. (2) Similar image generation. LLMGA can understand the component and layout of the input images and generate a similar image. (3) Inpainting & Outpainting. LLMGA can provide detailed generation prompts based on user preferences and input images. (4) Instruction based editing. LLMGA can understand user instructions and realize accurate editing. (5) Interactive image generation and editing exemplify the comprehensive capabilities of LLMGA. Users can design satisfactory images by engaging in interactions with LLMGA, leveraging its vast knowledge.]({{ '/images/11-2023/LLMGA:_Multimodal_Large_Language_Model_based_Generation_Assistant/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the limitations of existing Multimodal Large Language Models (MLLMs) used for image generation. Current methods typically rely on generating fixed-size embeddings to control diffusion models like Stable Diffusion (SD). This approach is often noisy, lacks interpretability for the user, and is inflexible for interactive, multi-turn conversations. The paper aims to create a "Multimodal Large Language Model-based Generation Assistant" (LLMGA) that leverages the vast knowledge and reasoning capabilities of LLMs to make image generation and editing more intuitive, flexible, and capable of producing higher-quality results.

## 2. Key Ideas and Methodology
The core idea of LLMGA is to use detailed, human-readable language prompts to control the SD model, rather than relying on noisy embeddings. This approach makes the generation process transparent, allows for more precise control, and is better aligned with the text-centric pre-training of LLMs.

The methodology is built on a two-stage training scheme:
1.  **MLLM Training**: An MLLM (initialized from LLaVA) is fine-tuned on a custom-curated dataset. This stage teaches the model to understand various generation and editing tasks (e.g., prompt refinement, inpainting, instruction-based editing) and to output a detailed generation prompt enclosed in special tokens (`<gen_img>`, `</gen_img>`).
2.  **SD Alignment**: The UNet of the SD model is then fine-tuned to better comprehend and execute the long, detailed prompts generated by the trained MLLM. The MLLM's parameters are frozen during this stage.

Additionally, the paper introduces a **Diffusion-based Reference Restoration Network (DiffRIR)** to correct texture, brightness, and contrast artifacts in edited images by using the preserved, unedited regions as a reference.

## 3. Datasets Used / Presented
The authors curate a new, comprehensive dataset for the first training stage, which includes four main components:
1.  **Prompt Refinement**: Pairs of brief and detailed image descriptions from the MSCOCO dataset, where detailed descriptions are generated by GPT-4V.
2.  **Similar Image Generation**: Images from MSCOCO paired with their detailed GPT-4V descriptions to teach the model to regenerate what it sees.
3.  **Inpainting & Outpainting**: Masked images paired with detailed descriptions of the complete, unmasked scene.
4.  **Instruction-based Editing**: Editing instructions and corresponding target captions, automatically generated by a fine-tuned Mixtral-8x7B model.

The model is also trained on the LLaVA v1.5 and Alpaca datasets to retain its general Visual Question Answering (VQA) and conversational abilities. Evaluation is performed on standard benchmarks: **MSCOCO** (for text-to-image), **MagicBrush** (for editing), and **Places** (for inpainting/outpainting).

## 4. Main Results
LLMGA demonstrates significant improvements over baseline and existing methods across various tasks.
-   **Text-to-Image Generation (MSCOCO)**: LLMGA-13B with a fine-tuned SD1.5 achieves a Fréchet Inception Distance (FID) of **18.41**, a notable improvement over the baseline SD1.5 (FID 24.01) and the embedding-based GILL model (FID 25.11).
-   **Instruction-based Editing (MagicBrush)**: In a zero-shot setting, LLMGA achieves a CLIP-T score of **0.3137**, surpassing specialized models like InstructPix2Pix (0.2909) and MagicBrush (0.2979).
-   **Outpainting (Places)**: LLMGA reduces the FID on wide-mask outpainting from 5.01 (baseline SD1.5) to **2.37**.

The authors conclude that using detailed language prompts allows LLMs to act as powerful and interactive assistants, enhancing both the quality of generated images and the overall user experience.

## 5. Ablation Studies
The paper reports two key ablation studies:
1.  **Training Data Contribution**: The authors evaluated the importance of each of the four custom-curated data components by reducing their size to 10% during training. Removing a significant portion of any component resulted in performance degradation. For example, reducing the "Prompt Refinement" data worsened the T2I FID from 18.52 to 19.19. This confirms that each data component is crucial for teaching the MLLM its respective capabilities.
2.  **Control Scheme Comparison**: The paper compared its language-prompt-based control against an alternative embedding-based control scheme. The results showed that the image quality from embedding-based methods deteriorated rapidly over multiple conversational turns. In contrast, LLMGA's language-prompt method maintained stable and superior performance, demonstrating its robustness for interactive use.

## 6. Paper Figures
![Fig. 2: The overview of LLMGA. (a) In the first training stage, we train the MLLM to produce generation prompts based on provided instructions. Moreover, we construct a training dataset including four categories: prompt refinement, similar image generation, inpainting & outpainting, and instruction-based editing. (b) In the second training stage, we optimize SD to adapt to the detailed generation prompts from MLLM. (c) In the inference stage, LLMGA can respond to user queries and assist in various tasks, such as image generation, inpainting & outpainting, and editing.]({{ '/images/11-2023/LLMGA:_Multimodal_Large_Language_Model_based_Generation_Assistant/figure_2.jpg' | relative_url }})
![Fig. 3: T2I visual comparison. LLMGA can produce accurate and high-quality results.]({{ '/images/11-2023/LLMGA:_Multimodal_Large_Language_Model_based_Generation_Assistant/figure_3.jpg' | relative_url }})
