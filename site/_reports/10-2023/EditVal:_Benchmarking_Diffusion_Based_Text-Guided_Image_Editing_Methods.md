---
title: EditVal:_Benchmarking_Diffusion_Based_Text-Guided_Image_Editing_Methods
layout: default
date: 2023-10-03
---
![Figure 1: Qualitative Examples from Image Editing on E DIT V AL . We find that for non-spatial edits (e.g., Changing background, color of an object, adding objects ), Instruct-Pix2Pix performs well, while other methods struggle. For spatial edits (e.g., Position-replacement), none of the editing methods lead to effective edits. apart particular aspects of an edit, for example, if changing the position of a particular object leaves the rest of the image unchanged (Gokhale et al., 2023). These gaps could be addressed by using human evaluators, but this is usually not scalable and thus limits the scope of edits and datasets that can be considered. Moreover, human studies often lack a standardized protocol, making it difficult to fairly compare methods.]({{ '/images/10-2023/EditVal:_Benchmarking_Diffusion_Based_Text-Guided_Image_Editing_Methods/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The paper addresses the lack of a standardized and scalable evaluation protocol for the rapidly growing field of text-guided image editing. Existing methods are often compared using unreliable automated metrics like CLIP scores, which struggle with fine-grained details, or through small-scale, non-standardized human studies. This makes it difficult to fairly assess and compare the performance of different models, particularly across a diverse range of complex and subtle edits (e.g., spatial manipulation). The authors aim to fill this gap by creating a comprehensive benchmark to enable rigorous, quantitative, and reproducible evaluation.

## 2. Key Ideas and Methodology
The core contribution is **EDITVAL**, a standardized benchmark for text-guided image editing. Its methodology is built on three key components:
1.  **A Curated Dataset and Edit Suite**: The benchmark uses a curated set of 92 real-world images from MS-COCO, spanning 19 object classes. For these images, it provides a suite of 648 unique edit operations covering 13 distinct categories, such as object addition/replacement, color/style/texture change, and spatial manipulations like position replacement. These edit operations were systematically generated using ChatGPT and refined by human annotators.
2.  **Automated Evaluation Pipeline**: For a subset of 6 object-centric edit types, EDITVAL provides an automated pipeline that produces a binary success score. This pipeline leverages the **OwL-ViT** vision-language model, which was chosen over CLIP for its superior ability to handle fine-grained spatial localization and object detection, making the evaluation more robust.
3.  **Standardized Human Study**: To complement the automated pipeline and cover all 13 edit types, the authors designed a large-scale human study template. Participants on Amazon Mechanical Turk assess edited images based on three criteria: (i) the accuracy of the applied edit, (ii) the preservation of the main object's other properties, and (iii) the preservation of the overall image context. The results from this study are used to validate the automated pipeline.

## 3. Datasets Used / Presented
The paper introduces the **EDITVAL** benchmark dataset.
*   **Name**: EDITVAL
*   **Content**: It consists of 92 carefully selected images from the MS-COCO dataset and a corresponding set of 648 unique edit instructions. These instructions cover 13 edit types across 19 object classes.
*   **Domain**: Real-world images with natural language editing prompts.
*   **Usage**: The benchmark was used to rigorously evaluate and compare 8 recent, state-of-the-art diffusion-based image editing methods (including Instruct-Pix2Pix, SINE, Null-Text, and Dreambooth).

## 4. Main Results
The evaluation of 8 editing methods on EDITVAL yielded several key insights:
*   **Top Performers**: Instruct-Pix2Pix, Null-Text, and SINE were the highest-performing methods on average across all edit types. However, no single method was the "winner" across every category.
*   **Editing vs. Preservation Fidelity**: Instruct-Pix2Pix and Null-Text excelled at both applying the requested edit and preserving the unedited parts of the image. In contrast, methods like SINE and Dreambooth, while effective at editing, often introduced significant unwanted changes to the original image content and context.
*   **Failure on Spatial Edits**: A major finding was that all tested methods perform poorly on edits requiring spatial reasoning. For tasks like `position-replacement` (e.g., "move the stop sign to the center"), editing accuracy was extremely low, typically between 0% and 15%.
*   **Automated Pipeline Validation**: The automated evaluation scores showed a strong correlation with human preference scores, confirming that the OwL-ViT-based pipeline is a reliable and scalable proxy for human judgment on supported edit types.

## 5. Ablation Studies
*   **Choice of Evaluation Model (CLIP vs. OwL-ViT)**: To justify their choice of model for the automated pipeline, the authors compared OwL-ViT against the more common CLIP model on a spatial evaluation task. OwL-ViT demonstrated far superior performance, achieving ~88% accuracy in identifying correct spatial relationships, while CLIP only managed ~56%. This confirmed OwL-ViT's suitability for evaluating the spatially-aware edits in EDITVAL.
*   **Fidelity Metric Validation (DINO vs. Human Score)**: The paper validated the use of DINO score as an automated metric for image content preservation. They found a strong correlation between DINO similarity scores and the human ratings for "image context preserved," confirming that DINO is an effective proxy for this aspect of evaluation.
*   **Sensitivity of `position-replacement` Evaluation**: The authors analyzed the distance threshold (`Î´`) used to judge the success of `position-replacement` edits. They found that a small threshold could be misleading, as models often introduce minor positional shifts even without an edit prompt. Using a larger, stricter threshold confirmed that all methods genuinely struggle with this task, validating their conclusion about the poor performance on spatial edits.

## 6. Paper Figures
![Figure 2: E DIT V AL contains 648 unique image-edit operations across 19 classes from MS-COCO spanning a variety of real-world edits. Edit types span simple categories like adding or replacing an object to more complex ones such as changing an action, viewpoint or replacing the position of an object.]({{ '/images/10-2023/EditVal:_Benchmarking_Diffusion_Based_Text-Guided_Image_Editing_Methods/figure_2.jpg' | relative_url }})
![Figure 3: Template for the AMT Human Study: A single task displays an edit operation, a source image, an edited image from a given image-editing method and a set of questions to assess edit fidelity. Note that our human-study template does not require edited images from other methods to compare the given text-guided image editing method under evaluation (For e.g., TedBench (Kawar et al., 2023) requires edited images from all other methods). This makes our human-study template scalable and it can be independently used with any new editing method.]({{ '/images/10-2023/EditVal:_Benchmarking_Diffusion_Based_Text-Guided_Image_Editing_Methods/figure_3.jpg' | relative_url }})
![Figure 5: Evaluation on E DIT V AL using OwL-ViT across eight state-of-the-art text-guided image editing methods. We find that while the text-guided image editing methods perform satisfactorily for edits corresponding to object manipulation, they suffer on edits requiring spatial knowledge such as positional-addition or position-replacement . Overall, we find Instruct-Pix2Pix , Null-Text and SINE to perform well across the majority of the editing types.]({{ '/images/10-2023/EditVal:_Benchmarking_Diffusion_Based_Text-Guided_Image_Editing_Methods/figure_5.jpg' | relative_url }})
![Figure 6: E DIT V AL correlation with human-score from AMT Study for six edit-types . We obtain human annotation scores falling in the range of { 0 , 1 , 2 , 3 } for all the images involving a given edit-type; the correlation is then computed b/w these scores and E DIT V AL binary scores. The general trend depicts a moderate-to-strong correlation b/w two evaluations.]({{ '/images/10-2023/EditVal:_Benchmarking_Diffusion_Based_Text-Guided_Image_Editing_Methods/figure_6.jpg' | relative_url }})
![Figure 7: Fidelity of Edited Images to Original Images. (a) Average FID Heusel et al. (2017) score computed between the original images and the edited images across all the 8 methods tested on E DIT V AL . Lower FID score quantitatively indicates a better image quality. (b) Average DINO score between the original images and the edited images across all the 8 methods tested on E DIT V AL . We find that for certain methods such as Textual-Inversion, the edited images change significantly from the original images across all edit categories. For spatial changes, we find that the edited images do not change significantly across different methods. a strong noteworthy correlations, as can be seen with object-addition having correlation between 0.6 and 0.7, while positional-addition and alter-parts attains only moderate correlations ranging from 0.45 to 0.6. These scores support the alignment of our automated pipeline with human ground-truth annotations.]({{ '/images/10-2023/EditVal:_Benchmarking_Diffusion_Based_Text-Guided_Image_Editing_Methods/figure_7.jpg' | relative_url }})
