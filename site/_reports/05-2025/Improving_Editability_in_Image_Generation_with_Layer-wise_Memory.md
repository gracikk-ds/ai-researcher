---
title: Improving_Editability_in_Image_Generation_with_Layer-wise_Memory
layout: default
date: 2025-05-02
---
![Figure 1. Overview. Our framework enables the interactive generation of images with enhanced control but in a simple manner, by rough mask and prompt, through iterative scene editing. We utilize the background scene generated by our framework to edit in HD Painter [ 32 ] or Blended Latent Diffusion (BLD) [ 3 ] for comparison and commercial products like Photoshop [ 1 ] and Pincel [ 37 ].]({{ '/images/05-2025/Improving_Editability_in_Image_Generation_with_Layer-wise_Memory/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address a significant gap in current image editing technologies, which are primarily designed for single-object modifications and struggle with sequential, multi-step editing tasks. When users attempt to add or modify multiple objects iteratively, existing methods often fail to maintain the consistency of previous edits or naturally integrate new elements into the scene, especially in cases of occlusion. This limitation hinders the creation of complex, layered images and requires cumbersome workflows, such as providing precise segmentation masks. The paper aims to create a framework that supports complex, iterative editing with minimal user effort while preserving scene coherence and contextual relationships across multiple modifications.

## 2. Key Ideas and Methodology
The core of the proposed framework is a **Layer-wise Memory** mechanism that stores the editing history—including latent representations, prompt embeddings, and masks—for each sequential modification. This memory enables consistent and efficient editing through three key technical components built upon a transformer-based diffusion model (PixArt-α):

1.  **Layer-wise Memory:** It systematically stores and manages information from previous edits. This allows the model to reference past states, ensuring that new objects are generated in context with existing ones and preserving the overall composition.
2.  **Background Consistency Guidance (BCG):** This component leverages the stored latents to preserve unedited background regions. By selectively blending new content only in the masked areas, it avoids re-computing the entire image, which maintains background integrity and improves computational efficiency.
3.  **Multi-Query Disentangled Cross-Attention (MQD):** To ensure new objects integrate naturally and respect spatial ordering (e.g., occlusion), MQD disentangles the attention mechanism. It processes queries for the current object, previously edited objects, and the background separately, allowing for seamless blending and accurate interpretation of the user's intended layering, guided only by rough masks and prompts.

## 3. Datasets Used / Presented
The authors introduce a new benchmark dataset named **Multi-Edit Bench** specifically designed to evaluate the challenges of sequential image editing.

*   **Description:** The benchmark consists of complex scenes with 3 to 6 layered objects, featuring significant occlusion and intricate spatial relationships.
*   **Generation:** It was created by selecting object classes from ImageNet-1K and using the GPT-4 API to generate realistic, template-based prompts that describe both the global scene and the individual object layers with their spatial relations. The layouts (masks) for these objects are generated randomly with constraints to ensure plausible compositions.
*   **Usage:** It is used to quantitatively evaluate the model's ability to maintain semantic and visual alignment across multiple, sequential editing steps, a scenario that existing benchmarks like EditBench do not adequately cover.

## 4. Main Results
The proposed method demonstrates superior performance in sequential editing tasks compared to state-of-the-art image editing and layout-to-image generation models.

*   **Quantitative Results:** On the Multi-Edit Bench (1024x1024 resolution), the proposed framework outperformed all baselines, achieving a BLEU-4 score of 36.59 and a METEOR score of 0.1513, compared to 36.28 and 0.1484 for the next-best method, HD-Painter.
*   **Human Evaluation:** In a preference study, the method was rated significantly higher than competitors like HD-Painter and Blended Latent Diffusion. On a 5-point scale, it scored 4.59 for background consistency and 4.28 for natural adaptation of new objects, highlighting its ability to produce coherent and high-quality results.
*   **Author Takeaway:** The framework successfully addresses the core challenges of iterative editing, enabling users to build complex scenes with high fidelity and consistency using only simple, rough mask inputs.

## 5. Ablation Studies
The authors performed an ablation study to validate the contribution of each key component, starting from a baseline PixArt-α inpainting model.

1.  **Baseline + BCG:** Adding Background Consistency Guidance (BCG) improved semantic alignment (BLEU-4 score increased from 33.06 to 35.20) and visual alignment (CLIP score from 64.05 to 64.10). This confirms its role in preserving background context.
2.  **Baseline + QD:** Adding a simplified Query Disentanglement (QD) without full memory integration further improved BLEU scores, demonstrating the benefit of separating attention for object integration.
3.  **Full Method (Ours):** The complete framework, which extends QD to Multi-Query Disentanglement (MQD) and integrates it with the full layer-wise memory, achieved the highest performance across semantic metrics (BLEU-4 of 36.59) while maintaining a strong CLIP score (64.29). This shows that combining background preservation with a memory-aware, disentangled attention mechanism is critical for handling complex, multi-step edits.

## 6. Paper Figures
![Figure 2. Overview. (a) The left denotes an illustration of how Multi Query Disentanglement is performed in the cross-attention layer. (b) The upper right figure shows Background Consistency Guidance with recalled latents, conducting latent blending with the saved latents. (c) The right below shows the layer-wise memory, saving the previous editing steps’ latents, masks, and prompt embeddings.]({{ '/images/05-2025/Improving_Editability_in_Image_Generation_with_Layer-wise_Memory/figure_2.jpg' | relative_url }})
![Figure 3. Overview of our proposed Multi-Edit Benchmark for evaluation of iterative editing scenario. (a) explains the dataset generation pipeline through GPT-4 API, and (b) explains the evaluation methodology in visual alignment using CLIP and semantic alignment using LLaVa for single-image and in a layer-wise manner.]({{ '/images/05-2025/Improving_Editability_in_Image_Generation_with_Layer-wise_Memory/figure_3.jpg' | relative_url }})
![Figure 4. Qualitative comparison on the effect of Query Disentanglement (QD).]({{ '/images/05-2025/Improving_Editability_in_Image_Generation_with_Layer-wise_Memory/figure_4.jpg' | relative_url }})
![Figure 5. Comparison in image editing capability with latest image editing models. [ 3 , 32 ] Note that the initial image is generated by our framework, which is equivalent to PixArtα [ 13 ] with no mask input.]({{ '/images/05-2025/Improving_Editability_in_Image_Generation_with_Layer-wise_Memory/figure_5.jpg' | relative_url }})
![Figure 6. Improved editability of image through Background Consistency Guidance and Multi-Query Disentangled cross attention. Through recycling the previous step’s latents, we can remove the object that is behind the foreground object, enabling enhanced editability of the image.]({{ '/images/05-2025/Improving_Editability_in_Image_Generation_with_Layer-wise_Memory/figure_6.jpg' | relative_url }})
![Figure 7. Comparison in interactive scenarios with existing T2I generative models. Stable Diffusion XL [ 38 ] and PixArtα [ 13 ] use text input only.]({{ '/images/05-2025/Improving_Editability_in_Image_Generation_with_Layer-wise_Memory/figure_7.jpg' | relative_url }})
