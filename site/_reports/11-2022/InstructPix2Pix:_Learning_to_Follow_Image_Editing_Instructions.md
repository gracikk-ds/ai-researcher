---
title: InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions
layout: default
date: 2022-11-17
---
## InstructPix2Pix: Learning to Follow Image Editing Instructions
**Authors:**
- Tim Brooks, h-index: 9, papers: 11, citations: 2735
- Alexei A. Efros, h-index: 107, papers: 232, citations: 107382

**ArXiv URL:** http://arxiv.org/abs/2211.09800v2

**Citation Count:** 1835

**Published Date:** 2022-11-17

![Figure 1. Given an image and an instruction for how to edit that image, our model performs the appropriate edit. Our model does not require full descriptions for the input or output image, and edits images in the forward pass without per-example inversion or ﬁne-tuning.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Current text-based image editing methods typically require users to provide a full description of the desired output image, rather than a simple instruction on what to change. This is often unintuitive and cumbersome. Furthermore, training a model to follow direct editing instructions is challenging due to the lack of large-scale, paired training data (i.e., triplets of an input image, an edit instruction, and the corresponding output image). The authors aim to solve this data scarcity problem and create a model that can edit images from simple, human-written commands in a fast and intuitive manner.

## 2. Key Ideas and Methodology
The core idea is to generate a large-scale, synthetic dataset for instruction-based image editing by programmatically combining the capabilities of two large pretrained models. The methodology consists of two main stages:

1.  **Dataset Generation:**
    *   A large language model (GPT-3) is first fine-tuned on a small, human-created set of 700 examples (input caption, instruction, output caption).
    *   This fine-tuned GPT-3 is then used to generate a large dataset (>450,000) of these text triplets.
    *   A text-to-image model (Stable Diffusion) combined with Prompt-to-Prompt is used to convert the generated caption pairs into corresponding before-and-after image pairs, ensuring structural consistency between them.

2.  **Model Training:**
    *   A conditional diffusion model, named **InstructPix2Pix**, is trained on this generated dataset. The model architecture is based on Stable Diffusion.
    *   It is conditioned on both the input image and the text instruction. To control the output, the authors introduce a dual classifier-free guidance mechanism with two separate scales: one to control similarity to the input image (`s_I`) and another to control adherence to the text instruction (`s_T`).

## 3. Datasets Used / Presented
*   **Human-written text triplets:** A small dataset of 700 examples, each containing an input caption, an edit instruction, and an output caption. This was created by the authors to fine-tune GPT-3.
*   **Generated training dataset:** The primary dataset created and used for training the InstructPix2Pix model. It consists of over 450,000 examples, where each example is a triplet of (input image, edit instruction, output image). The text instructions were generated by the fine-tuned GPT-3, and the image pairs were generated using Stable Diffusion and Prompt-to-Prompt. The initial captions were sourced from the LAION-Aesthetics dataset.

## 4. Main Results
The authors demonstrate qualitatively that InstructPix2Pix can perform a diverse range of edits on real images, such as changing artistic styles, replacing objects, altering backgrounds, and modifying settings, all from natural language instructions.

Quantitatively, the model is compared against SDEdit. The results show that InstructPix2Pix achieves a superior trade-off between image consistency (how much the output resembles the input) and edit consistency (how well the output follows the instruction). For a given level of adherence to the edit instruction, InstructPix2Pix produces images that are more faithful to the original input image structure compared to the baseline.

## 5. Ablation Studies
The authors performed several ablation studies to validate their design choices:

*   **Dataset Size:** Training the model on smaller subsets of the data (1% and 10%) resulted in a decreased ability to perform significant edits. The models tended to only make subtle, stylistic changes while maintaining high similarity to the input image.
*   **Dataset Filtering:** Removing the CLIP-based filtering step during data generation (which selects for high-quality image pairs) led to a noticeable reduction in the model's ability to preserve the structure of the input image during editing.
*   **Guidance Scales:** The effect of the two guidance scales was analyzed. Increasing the text guidance scale (`s_T`) results in a stronger, more pronounced edit, while increasing the image guidance scale (`s_I`) helps preserve the spatial structure of the original image. Fine-tuning these scales allows for a balance between edit strength and image consistency.

## 6. Paper Figures
![Figure 11. Applying our model recurrently with different instructions results in compounded edits.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_11.jpg' | relative_url }})
![Edit instruction: “in a race car video game” Figure 12. By varying the latent noise, our model can produce many possible image edits for the same input image and instruction.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_12.jpg' | relative_url }})
![“Have the people swap places” Figure 13. Failure cases. Left to right: our model is not capable of performing viewpoint changes, can make undesired excessive changes to the image, can sometimes fail to isolate the speciﬁed object, and has difﬁculty reorganizing or swapping objects with each other.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_13.jpg' | relative_url }})
![Figure 2. Our method consists of two parts: generating an image editing dataset, and training a diffusion model on that dataset. (a) We ﬁrst use a ﬁnetuned GPT-3 to generate instructions and edited captions. (b) We then use StableDiffusion [ 52 ] in combination with Promptto-Prompt [ 17 ] to generate pairs of images from pairs of captions. We use this procedure to create a dataset (c) of over 450,000 training examples. (d) Finally, our InstructPix2Pix diffusion model is trained on our generated data to edit images from instructions. At inference time, our model generalizes to edit real images from human-written instructions.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_2.jpg' | relative_url }})
![(b) With Prompt-to-Prompt. Figure 3. Pair of images generated using StableDiffusion [ 52 ] with and without Prompt-to-Prompt [ 17 ]. For both, the corresponding captions are “photograph of a girl riding a horse” and “photograph of a girl riding a dragon” .]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_3.jpg' | relative_url }})
![Figure 4. Classiﬁer-free guidance weights over two conditional inputs. s I controls similarity with the input image, while s T controls consistency with the edit instruction.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_4.jpg' | relative_url }})
![Figure 5. Mona Lisa transformed into various artistic mediums.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_5.jpg' | relative_url }})
![Figure 6. The Creation of Adam with new context and subjects (generated at 768 resolution).]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_6.jpg' | relative_url }})
![Figure 7. The iconic Beatles Abbey Road album cover transformed in a variety of ways.]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_7.jpg' | relative_url }})
![“Industrial design bedroom furniture...” “add a bedroom” Figure 9. Comparison with other editing methods. The input is transformed either by edit string (last two columns) or the groundtruth output image caption (middle two columns). We compare our method against two recent works, SDEdit [ 39 ] and Text2Live [ 6 ]. We show SDEdit in two conﬁgurations: conditioned on the output caption (OP) and conditioned on the edit string (E).]({{ '/images/11-2022/InstructPix2Pix:_Learning_to_Follow_Image_Editing_Instructions/figure_9.jpg' | relative_url }})
