---
title: Edit_Everything:_A_Text-Guided_Generative_System_for_Images_Editing
layout: default
date: 2023-04-27
---
## Edit Everything: A Text-Guided Generative System for Images Editing
**Authors:**
- Defeng Xie, h-index: 2, papers: 3, citations: 34
- Xiaodong Lin

**ArXiv URL:** http://arxiv.org/abs/2304.14006v1

**Citation Count:** 32

**Published Date:** 2023-04-27

![Figure 1: The network architecture of Edit Everything. The original image is separated into several segments with the help of Segment Anything model (SAM). Next, These segments are ranked based on the source prompt, and the target segment is chosen based on the highest score calculated by our trained CLIP model. The source prompt is a text that describes the target object and editing styles. Finally, guided by the target prompt, Stable Diffusion (SD) generates the replacement object for the mask segment. This process is seamless and efﬁcient, resulting in high-quality image editing.]({{ '/images/04-2023/Edit_Everything:_A_Text-Guided_Generative_System_for_Images_Editing/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the practical problem that creating and editing high-quality visual content typically requires specialized skills and significant time. While generative models, particularly text-guided diffusion models, have shown great promise, there is a need for a system that offers precise, intuitive control over image editing using natural language. The paper specifically aims to create an effective system for Chinese language scenarios, a domain where existing open-source models may have limitations.

## 2. Key Ideas and Methodology
The core of the paper is a new system called "Edit Everything," which integrates three powerful pre-existing models into a sequential pipeline for text-guided image editing. The methodology is as follows:
1.  **Segmentation:** The Segment Anything Model (SAM) is first applied to an input image to generate masks for all potential objects within it.
2.  **Selection:** A CLIP model then ranks these segments by comparing them to a user-provided "source prompt" (text describing the object to be replaced). The segment with the highest similarity score is selected as the target for editing.
3.  **Generation:** Finally, a Stable Diffusion (SD) model uses the selected mask and a "target prompt" (text describing the new object and style) to generate and inpaint a new object, seamlessly blending it into the surrounding context.

A key aspect of their work is training their own CLIP and SD models on a large-scale Chinese dataset to enable high-fidelity editing with Chinese text prompts.

## 3. Datasets Used / Presented
The authors trained their Chinese-specific models on a large corpus aggregated from multiple sources, totaling over 300 million image-text pairs:
*   **Wukong:** 100 million Chinese image-text pairs from a large-scale cross-modal benchmark.
*   **Zero and R2D2:** 23 million high-quality Chinese image-text pairs extracted from this dataset.
*   **Laion-5b:** 80 million Chinese image-text pairs obtained after filtering and pre-processing the full dataset.
*   **Crawled Data:** 100 million proprietary image-text pairs crawled from the web, covering domains like shopping and common knowledge.

## 4. Main Results
The paper presents its results qualitatively through visual examples. The main achievements are:
*   The system can successfully edit specific objects within an image, change artistic styles (e.g., from a photograph to an illustration), and generate realistic results that match the original image's context, including shadows and reflections.
*   It can handle complex requests by breaking them down into a series of iterative, single-object edits, providing users with highly accurate control.
*   The authors claim their trained models outperform standard open-source models, primarily by demonstrating the ability to successfully process Chinese text inputs to guide the editing process, which the baseline models could not.

## 5. Ablation Studies
Not performed.

## 6. Paper Figures
![Figure 2: Text-guided image editing examples created by Edit Everything. Our advanced system detects the dark region, and erases them by the source target. And then we apply SD to ﬁll it based on the target prompt. Our system is able to produce various styles and seamlessly match the surrounding context.]({{ '/images/04-2023/Edit_Everything:_A_Text-Guided_Generative_System_for_Images_Editing/figure_2.jpg' | relative_url }})
![Figure 3: Iteratively replacing objects of an image step by step using Editing Everything.]({{ '/images/04-2023/Edit_Everything:_A_Text-Guided_Generative_System_for_Images_Editing/figure_3.jpg' | relative_url }})
![Figure 4: Comparisons of images generated by open-source models and our trained models. Our models could support Chinese inputs.]({{ '/images/04-2023/Edit_Everything:_A_Text-Guided_Generative_System_for_Images_Editing/figure_4.jpg' | relative_url }})
