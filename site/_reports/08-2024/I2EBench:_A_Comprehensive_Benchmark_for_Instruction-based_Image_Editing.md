---
title: I2EBench:_A_Comprehensive_Benchmark_for_Instruction-based_Image_Editing
layout: default
date: 2024-08-26
---
![Figure 1: Overview of I 2 EBench, an automated system for evaluating the quality of editing results generated by instruction-based image editing (IIE) models. We collected a dataset of over 2000+ images from public datasets Lin et al. [ 2014 ], Guo et al. [ 2023b ], Martin et al. [ 2001 ], Chen et al. [ 2021 ], Ancuti et al. [ 2019 ], Liu et al. [ 2021b , a ], Qu et al. [ 2017 ], Nah et al. [ 2017 ], Shen et al. [ 2019 ], Wei et al. [ 2018 ] and annotated them with corresponding original editing instructions. To diversify the instructions, we used ChatGPT Achiam et al. [ 2023 ] to generate varied versions. With the collected images and the original/diverse editing instructions, we utilized existing IIE models to generate edited images. Subsequently, we developed an evaluation methodology to automatically assess the adherence of edited images to the provided instructions under different dimensions. We also implemented human evaluation to obtain human preferences for editing results of different IIE models. Finally, we analyzed the correlation between automated evaluation and human evaluation, confirming alignment with human perception.]({{ '/images/08-2024/I2EBench:_A_Comprehensive_Benchmark_for_Instruction-based_Image_Editing/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the significant challenge of evaluating Instruction-based Image Editing (IIE) models. Existing evaluation methods are inadequate: conventional metrics like CLIP Score or PSNR are not universally applicable to the diverse range of editing tasks; user studies are expensive and not reproducible; and current benchmarks are limited in scope, either by size, focus on specific editing types (e.g., mask-guided), or lack of automation. This creates a critical gap for a comprehensive, automated, and human-aligned benchmark to accurately assess model performance and guide future research.

## 2. Key Ideas and Methodology
The paper introduces **I2EBench**, a comprehensive benchmark designed to automatically evaluate IIE models from multiple dimensions.

-   **Core Principle:** The benchmark is built on the idea that different editing tasks require specialized evaluation methods, rather than a one-size-fits-all metric.
-   **Methodology:**
    1.  **Comprehensive Dimensions:** I2EBench defines 16 distinct evaluation dimensions, categorized into **High-level Editing** (e.g., Object Removal, Background Replacement, Counting) and **Low-level Editing** (e.g., Deblurring, Haze Removal, Denoising).
    2.  **Specialized Evaluation:** For most high-level tasks that require semantic understanding, the benchmark leverages the vision-language capabilities of GPT-4V to assess results by asking it targeted questions. For low-level tasks, it employs established metrics like SSIM.
    3.  **Human Perception Alignment:** The benchmark's validity is established by conducting extensive human evaluations and demonstrating a strong positive correlation between the automated I2EBench scores and human preference rankings.

## 3. Datasets Used / Presented
The paper presents the **I2EBench dataset**, which was created to facilitate the benchmark.

-   **Name:** I2EBench
-   **Size:** It consists of over 2,000 input images and over 4,000 corresponding instructions. This includes 2,000+ original, human-annotated instructions and 2,000+ "diverse" instructions generated by ChatGPT to test model robustness to linguistic variations.
-   **Domain and Composition:** The images are curated from various public datasets to cover the 16 editing dimensions, with approximately 140 images per dimension. The dataset is also annotated with content categories (e.g., Animal, Scenery, Human) and includes the necessary questions and answers for the GPT-4V-based evaluation. It was used to evaluate 8 different open-source IIE models.

## 4. Main Results
The evaluation of 8 IIE models on I2EBench yielded several key insights.

-   **No Single Best Model:** The results clearly show that no single model excels across all 16 dimensions. For low-level editing, `InstructDiffusion` performs well broadly, while `MGIE` is superior in deblurring. For high-level editing, `MagicBrush` and `InstructAny2Pix` are the strongest performers.
-   **Strong Human-Score Correlation:** The I2EBench rank scores show a very high positive correlation with human judgment across all dimensions (e.g., correlation coefficients `ρ` > 0.9 for 8 of 16 dimensions), validating its alignment with human perception.
-   **Author-claimed Impact:** I2EBench provides a robust framework to reveal the specific strengths and weaknesses of IIE models, offering valuable insights for future development in data selection, model architecture, and training strategies.

## 5. Ablation Studies
While not a formal ablation section, the paper performs several critical analyses to test model robustness under different conditions.

-   **Original vs. Diverse Instructions:** The authors compared model performance on simple "original" instructions versus more complex "diverse" instructions. They found that models incorporating Large Language Models (LLMs) like `InstructAny2Pix` and `MGIE` were significantly more robust to variations in instructions, whereas models like `InstructPix2Pix` showed a large drop in performance.
-   **Performance Across Content Categories:** The analysis showed that most models perform better on "Scenery" and "Global" categories. This is because these edits are often global and do not require precise object localization, highlighting a common weakness in fine-grained editing capabilities.

## 6. Paper Figures
![Figure 2: Visualization of the editing results on the proposed 16 evaluation dimensions using different IIE models, including InstructAny2Pix Li et al. [ 2023d ], HIVE Zhang et al. [ 2023a ], InstructEdit Wang et al. [ 2023b ], InstructDiffusion Geng et al. [ 2023 ], InstructPix2Pix Brooks et al. [ 2023 ], MagicBrush Zhang et al. [ 2024a ], MGIE Fu et al. [ 2024 ], and HQEdit Hui et al. [ 2024 ]. A detailed version can be found in supplementary materials.]({{ '/images/08-2024/I2EBench:_A_Comprehensive_Benchmark_for_Instruction-based_Image_Editing/figure_2.jpg' | relative_url }})
![Figure 3: Word cloud visualization (a,b) and image quantity statistics (c) of I 2 EBench.]({{ '/images/08-2024/I2EBench:_A_Comprehensive_Benchmark_for_Instruction-based_Image_Editing/figure_3.jpg' | relative_url }})
![Figure 4: Comparison of radar charts for I 2 EBench scores in different dimensions using (a) original instructions and (b) diverse instructions.]({{ '/images/08-2024/I2EBench:_A_Comprehensive_Benchmark_for_Instruction-based_Image_Editing/figure_4.jpg' | relative_url }})
![Figure 6: I 2 EBench change rate using original instructions and diverse instructions.]({{ '/images/08-2024/I2EBench:_A_Comprehensive_Benchmark_for_Instruction-based_Image_Editing/figure_6.jpg' | relative_url }})
![Figure 7: Comparison of radar charts for I 2 EBench scores in different categories using (a) original instructions and (b) diverse instructions. The scores of all dimensions are normalized and averaged.]({{ '/images/08-2024/I2EBench:_A_Comprehensive_Benchmark_for_Instruction-based_Image_Editing/figure_7.jpg' | relative_url }})
