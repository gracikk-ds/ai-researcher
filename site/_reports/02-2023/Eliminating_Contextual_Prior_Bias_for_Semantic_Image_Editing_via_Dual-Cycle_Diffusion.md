---
title: Eliminating_Contextual_Prior_Bias_for_Semantic_Image_Editing_via_Dual-Cycle_Diffusion
layout: default
date: 2023-02-05
---
![Fig. 1. The overview of the proposed Dual-cycle Diffusion framework for the semantic image editing task. The pipeline and details of the Bias Elimination Cycle (BE-Cycle) and mask-guided unbiased editing are shown on the left. The modifications on cat ears , caused by the contextual prior bias derived from the pre-trained model , are illustrated in the left-top corner of the images. Given a source image, a source text, and a target text, we first leverage BE-Cycle to produce an unbiased mask, which is then used to guide image editing. On the right, the details of the Structural Consistency Cycle (SCCycle) [7] and the procedure of mask computing are shown. ⊙ is the elementwise product.]({{ '/images/02-2023/Eliminating_Contextual_Prior_Bias_for_Semantic_Image_Editing_via_Dual-Cycle_Diffusion/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the problem of "contextual prior bias" in pre-trained text-to-image diffusion models. This bias, learned from massive datasets, causes models to make spurious or unintended modifications to image regions that should remain unchanged during semantic editing. For example, when editing an image of a cat to add a scarf, the model might also incorrectly alter the shape of the cat's ears. The paper aims to eliminate these unwanted side effects and ensure edits are localized only to the intended areas.

## 2. Key Ideas and Methodology
The core idea is **Dual-Cycle Diffusion**, a method designed to generate an unbiased mask that precisely isolates regions for editing. The methodology is centered around a **Bias Elimination Cycle (BE-Cycle)**, which consists of two paths:
1.  A **Forward Path** that uses a pre-trained diffusion model to generate the edited image (`x_target`) from a source image and a target text prompt.
2.  An **Inverted Path** that takes the edited image (`x_target`) and attempts to reconstruct the original image using the source text prompt, resulting in an inverted image (`x_inv`).

The key assumption is that both the forward and inverted paths will exhibit the same contextual prior bias from the pre-trained model. By extracting features with a CLIP encoder and contrasting the outputs of these two paths (`x_target` and `x_inv`), the shared bias is canceled out, leaving only the intended differences. This difference map is then binarized to create an "unbiased mask" that guides a final, clean editing process, ensuring only the desired regions are modified.

## 3. Datasets Used / Presented
The experiments were conducted on the **zero-shot semantic image editing dataset** introduced by CycleDiffusion [7]. This dataset is specifically designed for this task and contains **150 tuples**, where each tuple consists of a source image, a corresponding source text description, and a target text prompt describing the desired edit. It was used to quantitatively and qualitatively evaluate the proposed method against other state-of-the-art editing models.

## 4. Main Results
The proposed Dual-Cycle Diffusion method demonstrated superior performance compared to existing methods like SDEdit, DiffEdit, and CycleDiffusion.
- On the primary metric for edit consistency, the directional CLIP score (SD-CLIP), the method achieved a score of **0.283**, improving upon the previous best of 0.272 (from CycleDiffusion) when using the Stable Diffusion v1-4 model.
- The method also achieved the highest or competitive scores in other metrics like PSNR (22.08) and SSIM (0.730), indicating better preservation of the original image's content.
- The authors claim their method effectively eliminates contextual prior bias, leading to more faithful and accurate semantic image edits.

## 5. Ablation Studies
The authors performed two main ablation studies to validate their design choices:

1.  **Unbiased vs. Biased Mask:** They compared their proposed "unbiased mask" (generated by contrasting the forward and inverted path outputs) against a "biased mask" (generated by simply contrasting the source and final edited images). The unbiased mask led to better image content preservation, improving the PSNR score from 21.85 to **22.08** and the SSIM score from 0.724 to **0.730**. This confirms that the BE-Cycle is effective at isolating the intended edit from model bias.
2.  **Role of CLIP Encoder:** The study tested the impact of using the CLIP encoder for feature extraction during mask generation. Models using the CLIP encoder achieved higher PSNR and SSIM scores, demonstrating that leveraging its rich visual features is crucial for accurately identifying content differences and creating a high-quality mask.

## 6. Paper Figures
![Fig. 2. The masks and edited samples generated by our proposed method. The first two rows show comparisons with two most representative baseline models: SDEdit [13], DiffEdit [23], and CycleDiffusion [7]. Among all samples, our method outperforms the other models in image content preservation. Yellow boxes mark unexpected modifications, and the masks reveal the specific edited areas. The last row shows more results generated by the proposed method.]({{ '/images/02-2023/Eliminating_Contextual_Prior_Bias_for_Semantic_Image_Editing_via_Dual-Cycle_Diffusion/figure_2.jpg' | relative_url }})
![Fig. 3. The BE-Cycle’s forward path and inverted path’s masks, outputs, and final edited results. Specifically, the mask is generated by comparing the input and output of the forward or inverted path. To remove the effect of random noise in the mask computing process, we averaged all the masks generated with different hyperparameter combinations. From the comparisons, we can obtain an ice-cream similar to the source image with the unbiased mask.]({{ '/images/02-2023/Eliminating_Contextual_Prior_Bias_for_Semantic_Image_Editing_via_Dual-Cycle_Diffusion/figure_3.jpg' | relative_url }})
