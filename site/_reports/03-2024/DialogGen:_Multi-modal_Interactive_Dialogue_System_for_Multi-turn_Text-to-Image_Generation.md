---
title: DialogGen:_Multi-modal_Interactive_Dialogue_System_for_Multi-turn_Text-to-Image_Generation
layout: default
date: 2024-03-13
---
## DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation
**Authors:**
- Minbin Huang, h-index: 6, papers: 11, citations: 188
- Wei Liu

**ArXiv URL:** http://arxiv.org/abs/2403.08857v3

**Citation Count:** 10

**Published Date:** 2024-03-13

![Figure 1: Illustration of Multi-modal Interactive Dialogue System (MIDS) built by our proposed DialogGen that can perform multi-turn multi-modal tasks responding to user’s natural language instructions to meet the users’ needs for image generation, image editing, and chatting.]({{ '/images/03-2024/DialogGen:_Multi-modal_Interactive_Dialogue_System_for_Multi-turn_Text-to-Image_Generation/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the challenge that while Text-to-Image (T2I) models are powerful, they are difficult for average users to interact with effectively. This is due to two main issues: 1) the need for specialized "prompt engineering" skills to get desired results, and 2) the inability of most models to handle multi-turn, iterative conversations where a user might want to refine or edit a previously generated image. This gap hinders a dynamic and intuitive creative process, which the authors aim to solve by creating a more user-friendly, conversational image generation system.

## 2. Key Ideas and Methodology
The paper introduces **DialogGen**, a pipeline to build a Multi-modal Interactive Dialogue System (MIDS) by aligning an off-the-shelf Multi-modal Large Language Model (MLLM) with a T2I model. The methodology is composed of three main stages:

1.  **Drawing Prompt Alignment:** The MLLM is first used to re-caption the T2I model's original training data. This step ensures that the prompts generated by the MLLM during conversation are in a format and style that the T2I model is optimized to understand, leading to better image generation.
2.  **Training Data Curation:** The MLLM is fine-tuned on a carefully curated dataset that includes a mix of general instruction-tuning data, bilingual (English/Chinese) conversations, and data specifically designed to guarantee object consistency across multiple conversational turns.
3.  **Error Correction:** A powerful "teacher" MLLM (GPT-4) corrects the errors made by the "student" MLLM (the model being trained). The student model is then further fine-tuned on this correction data, allowing it to learn from its own mistakes and better understand user intentions.

## 3. Datasets Used / Presented
The authors introduce a new comprehensive benchmark named the **Multi-modal Dialogue Benchmark (DialogBen)**.

*   **Name & Size:** DialogBen contains 9957 three-turn, bilingual (English/Chinese) multi-modal conversations.
*   **Domain & Composition:** The dataset covers 7 distinct image editing instruction types (e.g., add/remove object, style transfer) and 13 diverse topic types (e.g., animals, landmarks, food).
*   **Usage:** It is designed to evaluate MIDS on two key capabilities: **Modality Switching Accuracy** (correctly deciding whether to output text or an image) and **Generation Coherence** (assessing the semantic quality and relevance of the generated image within the dialogue context). A subset of this benchmark, **DialogBen-train** (~4.5k samples), is also used to train the final DialogGen model.

## 4. Main Results
*   DialogGen significantly outperforms existing MIDS baselines like NEXT-GPT and SEED-LLaMA on the DialogBen benchmark.
*   In **Modality Switching**, DialogGen achieved a high average accuracy of 97.24%, demonstrating its reliability in understanding whether a user wants to chat or generate an image.
*   In **Generation Coherence**, the DialogGen-X model (paired with a proprietary T2I model) achieved the top Coherence VQA score of 0.6514, which was shown to correlate strongly with human preference evaluations (0.7559).
*   The authors claim that DialogGen is an effective pipeline for creating powerful conversational T2I systems and that DialogBen is a crucial benchmark for advancing research in this area.

## 5. Ablation Studies
*   **Error Correction:** The impact of training with error correction data was evaluated. Adding this data dramatically improved the average Modality Switching accuracy of the DialogGen-A model from 88.4% to 95.3% (a +6.9% absolute improvement), showing the effectiveness of learning from corrected mistakes.
*   **Bilingual Training:** The study showed that training on a mix of English and Chinese data improved Modality Switching performance on both English (+1.3%) and Chinese (+3.9%) test sets compared to models trained on only one language, indicating that multilingual data enhances the model's reasoning abilities.
*   **Training Data Composition:** The paper compared a model trained only on general conversational data (DialogGen-A) with one also trained on their curated benchmark data (DialogGen-B). DialogGen-B performed better, highlighting the value of the high-quality, multi-turn, multi-modal data in DialogBen-train.

## 6. Paper Figures
![Precisely identifying users’ intentions and producing outputs in the suitable modality, whether with text or images, is essential for Multi-modal Interactive Dialog Systems (MIDS). In this way, the systems can deliver more relevant, informative, and contextually fitting responses. Therefore, we think it’s important to check how well these systems can switch between modes of communication in our benchmark test. We propose to evaluate the Modality Switching accuracy for each turn and focus on image and text modalities, as shown at the left bottom of Fig. 3. When a user inputs an image, it is usually accompanied by text describing the user’s query related to the image. The system’s output typically consists of image and text modalities. For each round, we assess 4 different modality switching scenarios mentioned in Sec. 3.1, to ensure a comprehensive evaluation of the system’s ability to generate the right multi-modal outputs. After inference on the benchmark, we calculate the modality switching accuracy as follows: suppose in the i th round, there are n ij samples belonging to j th modality switching scenario, let c ( k ) ij be the binary variable, such that c ( k ) ij = 1 indicates the model producing the correct output modalities. Then, the Modality Switching Accuracy Acc ij in the i th round for j th scenarios is calculated by]({{ '/images/03-2024/DialogGen:_Multi-modal_Interactive_Dialogue_System_for_Multi-turn_Text-to-Image_Generation/figure_3.jpg' | relative_url }})
![Figure 4: The overall pipeline of DialogGen which consists of Drawing Prompt Alignment, Training Data Curation, and Error Correction. In Drawing Prompt Alignment, re-captioning on D G is performed to ensure the alignment between transformed prompts and the T2I model. Then we carefully curate the training data such as adding object consistency guarantee, bilingual data and mixed instruction tuning data during training. Finally, we employ an error correction mechanism on student model M s to make the model learn from its mistakes.]({{ '/images/03-2024/DialogGen:_Multi-modal_Interactive_Dialogue_System_for_Multi-turn_Text-to-Image_Generation/figure_4.jpg' | relative_url }})
![Figure 5: Visualization of output results of NexTGPT, DialogGen-SD, and DialogGen-Hunyuan on the DialogBen benchmark. DialogGen has better performance in generating output of correct modality and higher semantic coherence.]({{ '/images/03-2024/DialogGen:_Multi-modal_Interactive_Dialogue_System_for_Multi-turn_Text-to-Image_Generation/figure_5.jpg' | relative_url }})
