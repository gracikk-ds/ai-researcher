---
title: Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models
layout: default
date: 2024-03-13
---
![Figure 1: The generated images evoke a sense of happiness in viewers, contrasting with the negative emotions elicited by the source images. Given a source image that triggers negative emotions (framed in green), our method (Ours) synthesizes a new image that elicits the given positive target emotions (in red), while maintaining the essential elements and structures of the scene. For instance, in the first quadrant where trash on the ground evokes negative emotions, our model performs image editing by eliminating the trash while preserving the outdoor context, including trees and grass, in the same spatial arrangement as the original scene. For comparisons, we include other competitive methods (Sec.4). Zoom in for better views.]({{ '/images/03-2024/Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the under-explored problem of **emotional image editing**. While image generation models have advanced rapidly, techniques for explicitly modifying an image to evoke a specific target emotion in a viewer are lacking. The goal is to create a method that can synthesize a new image that elicits a desired emotion (e.g., changing a "disgusting" scene to one of "contentment") while preserving the core structure and semantic content of the original image. This has practical applications in areas like psychological therapy, advertising, and artistic design.

## 2. Key Ideas and Methodology
The paper introduces **EmoEditor**, a novel diffusion model designed for emotion-evoked image generation. Its core methodology is built on three key ideas:

-   **Dual-Branch Architecture:** EmoEditor uses a dual-branch architecture during the reverse diffusion process. A "global branch" processes the target emotion (as a one-hot vector), while a "local branch" processes the source image's visual features. This allows the model to integrate the desired emotional context while remaining faithful to the original scene's structure.
-   **Neuro-Symbolic Alignment Loss:** During training, the model's behavior is guided by an alignment loss that matches its internal emotion embeddings with CLIP text embeddings of human-written editing instructions. This allows the model to implicitly learn how to translate an abstract emotion into concrete visual edits, emulating human reasoning without needing explicit text prompts during inference.
-   **Iterative Emotion Inference:** At inference time, EmoEditor employs a recurrent critic mechanism. A pre-trained emotion predictor evaluates the generated image. If the image doesn't match the target emotion with high confidence or deviates too much structurally, it is iteratively refined until the criteria are met, ensuring high-quality and emotionally coherent output.

## 3. Datasets Used / Presented
The authors curate and introduce a new large-scale dataset called **EmoPair**, which contains ~340,000 image pairs. It is the first of its kind for paired emotional image editing and consists of two subsets:

-   **EmoPair-Annotated Subset (EPAS):** Contains 331,595 image pairs sourced from the InstructPix2Pix dataset. The authors annotated these pairs with source and target emotion labels using a custom-trained emotion predictor.
-   **EmoPair-Generated Subset (EPGS):** Contains 6,949 image pairs generated by the authors. They used GPT-3 to create emotion-driven text instructions and applied the InstructPix2Pix model to images from the EmoSet dataset to generate corresponding target images.

The model was trained on EmoPair and evaluated on a separate test set of 504 images from the EmoSet dataset.

## 4. Main Results
EmoEditor demonstrated superior performance over five competitive baselines, including style transfer, text-to-image models (Ip2p), and a pipeline of large models (LMS).

-   **Human Evaluation:** In a psychophysics experiment with 417 participants, images generated by EmoEditor were preferred 56% of the time over all baselines (where 50% is chance), indicating its superior ability to evoke the intended emotions.
-   **Quantitative Metrics:** Using newly proposed metrics, EmoEditor achieved the highest **Emotion Matching Ratio (EMR)** of 50.2% (vs. 11.5% for the next best, LMS) and the highest **Emotion Similarity Ratio (ESR)** of 92.9% (vs. 92.0% for NST). This shows it is more effective at generating images that match the target emotion while maintaining a good balance of structural preservation.

The authors conclude that EmoEditor effectively understands and edits emotional cues in images, outperforming existing methods.

## 5. Ablation Studies
The paper reports several key ablation studies to validate its design choices:

-   **Removing the Iterative Emotion Predictor (P):** Disabling the iterative inference critic caused a massive performance drop. The EMR fell from 50.2% to just 5.1%, and ESR fell from 92.9% to 69.2%. This highlights the critical role of the iterative refinement process in achieving emotionally accurate results.
-   **Removing the Alignment Loss (L_emb):** Training without the neuro-symbolic alignment loss also degraded performance, with EMR dropping from 50.2% to 43.4%. This confirms the loss is crucial for teaching the model to make human-aligned, emotionally relevant edits.
-   **Varying Inference Iterations:** An analysis showed that performance metrics like EMR and ESR improve significantly as the number of inference iterations increases from 1 to 30, eventually stabilizing. This demonstrates that the iterative process is key to the model's effectiveness and stability.

## 6. Paper Figures
![Figure 2: Emotions are influenced by both global and local cues from the source images. We present two image generation examples showcasing the transition from positive to negative emotions in Column 1 (shaded in blue) and two examples from negative to positive emotions in Column 2 (shaded in orange). In each example, the target emotions are in red, and source images are framed in green. Generated results from three methods are presented. Zoom in for better views.]({{ '/images/03-2024/Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models/figure_2.jpg' | relative_url }})
![Figure 3: Pipeline for Curating Our EmoPair Dataset. The dataset comprises two subsets: EmoPair-Annotated Subset (EPAS, left blue box) and EmoPair-Generated Subset (EPGS, right orange box). Each subset includes schematics depicting the creation, selection, and labeling of image pairs in the upper quadrants, with two example pairs in the lower quadrants. Each example pair comprises a source image (framed in green) and a target image. The classified source and target emotion labels (highlighted in red) and target-emotion-driven text instructions for image editing are provided. See more examples in Supp. Sec.S1. See Sec.3.1 for EmoPair dataset.]({{ '/images/03-2024/Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models/figure_3.jpg' | relative_url }})
![Figure 4: Architecture of Our Proposed EmoEditor. EmoEditor is an image diffusion model, consisting of local (shaded in green) and global (shaded in blue) branches. The pre-trained VAE’s encoder E and decoder D remain fixed throughout training and inference. Exclusively employed during inference, the fixed emotion predictor P predicts the emotions on generated images for the iterative emotion inference. See Sec.3 for details.]({{ '/images/03-2024/Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models/figure_4.jpg' | relative_url }})
![Figure 5: Human Psychophysics Experiment Results. The average proportion of images that human participants prefer our EmoEditor over other methods is 56%. Chance is 50% (red dotted line). Error bars are standard errors.]({{ '/images/03-2024/Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models/figure_5.jpg' | relative_url }})
![Figure 6: Visualization of Generated Images from Different Methods. The target emotion is highlighted in red and the source image is framed with green. See Supp. Fig.S10 for more examples.]({{ '/images/03-2024/Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models/figure_6.jpg' | relative_url }})
![Figure 7: Our EmoEditor can generalize to more challenging emotion editing scenarios. (a) Three examples are showcased, highlighting our EmoEditor’s ability to produce images evoking emotions of the same positive valence as the source images. (b) Two examples demonstrate how our EmoEditor can transform neutral real-world images to evoke either positive or negative emotions. Source images are framed in green. Target emotions are in red. See Sec.5.3 for the result analysis.]({{ '/images/03-2024/Make_Me_Happier:_Evoking_Emotions_Through_Image_Diffusion_Models/figure_7.jpg' | relative_url }})
