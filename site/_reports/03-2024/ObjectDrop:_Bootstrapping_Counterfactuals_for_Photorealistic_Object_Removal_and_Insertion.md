---
title: ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion
layout: default
date: 2024-03-27
---
![Figure 1: Object removal and insertion. Our method models the effect of an object on the scene including occlusions, reflections, and shadows, enabling photorealistic object removal and insertion. It significantly outperforms state-of-theart baselines.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing diffusion models for image editing, while powerful, often fail to produce physically plausible results for tasks like object removal. They struggle to model the effects an object has on its environment, such as casting shadows or creating reflections. This leads to unrealistic edits where an object is removed, but its shadow remains. The authors argue this is a fundamental limitation of self-supervised methods, which lack access to "counterfactual" data (i.e., what a scene looks like with and without an object). This paper aims to solve this by training models on such counterfactual data to achieve photorealistic object removal and insertion that respects physical laws.

## 2. Key Ideas and Methodology
The core idea is to supervise a diffusion model using a meticulously collected "counterfactual dataset." This dataset contains pairs of images: a "factual" image with an object and a "counterfactual" image of the exact same scene immediately after the object was physically removed.

For **object removal**, the authors fine-tune a pre-trained inpainting diffusion model on this dataset. The model learns to generate the counterfactual (object-free) image from the factual image and an object mask.

For **object insertion**, which is a more complex task, the small dataset is insufficient. The authors propose a "bootstrap supervision" method:
1.  They use their trained object removal model to process a large, unlabeled image dataset, creating a vast *synthetic* counterfactual dataset.
2.  They pre-train an object insertion model on this large synthetic dataset to learn how to add effects like shadows and reflections.
3.  Finally, they fine-tune this model on the small, high-quality real counterfactual dataset to achieve photorealistic results.

## 3. Datasets Used / Presented
*   **Counterfactual Dataset:** A novel dataset created by the authors consisting of 2,500 high-quality image pairs. Each pair shows a scene before and after a single object was physically removed, captured with a tripod to minimize other changes. It was used to train the removal model and fine-tune the insertion model.
*   **Bootstrapped Synthetic Dataset:** A large-scale synthetic dataset of 350,000 image pairs, generated by applying the authors' object removal model to a large external dataset of images. This was used to pre-train the object insertion model.

## 4. Main Results
The proposed method, ObjectDrop, significantly outperforms state-of-the-art baselines in both qualitative and quantitative evaluations.
*   **Object Removal:** In a user study against general editing methods, ObjectDrop was preferred over Emu Edit 64.1% of the time and over MGIE 86.5% of the time for correctly removing an object and its effects.
*   **Object Insertion:** On a held-out test set, ObjectDrop achieved a PSNR of 21.63, surpassing AnyDoor (19.50) and Paint-by-Example (17.52). User studies showed an overwhelming preference for ObjectDrop, with over 95% preference against baselines on out-of-distribution images.
The key takeaway is that supervising on real-world counterfactual data, even a small amount, is highly effective for modeling object-scene interactions.

## 5. Ablation Studies
*   **Bootstrapping:** The contribution of the bootstrap supervision for object insertion was tested. A model trained without bootstrapping achieved a PSNR of 20.18, while the full model with bootstrapping reached 21.63, demonstrating that the large synthetic dataset is crucial for high-quality insertion.
*   **Dataset Size:** The object removal model was trained on subsets of the counterfactual dataset (from 0 to 2,500 samples). Performance scaled with data size; using the pre-trained model with 0 samples failed to remove object effects, while results became compelling around 1,000 samples and continued to improve with the full dataset.
*   **Pre-trained Backbone:** The authors compared initializing their model from a general text-to-image (T2I) backbone versus an inpainting backbone. The inpainting model was slightly better at filling occluded regions for removal, but both were comparable at handling shadows and reflections.

## 6. Paper Figures
![Figure 2: Generalization. Our counterfactual dataset is relatively small and was captured in controlled settings, yet the model generalizes exceptionally well to out-of-distribution scenarios such as removing buildings and large objects.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_2.jpg' | relative_url }})
![Figure 3: Overview of our method. We collect a counterfactual dataset consisting of photos of scenes before and after removing an object, while keeping everything else fixed. We used this dataset to fine-tune a diffusion model to remove an object and all its effects from the scene. For the task of object insertion, we bootstrap bigger dataset by removing selected objects from a large unsupervised image dataset, resulting in a vast, synthetic counterfactual dataset. Training on this synthetic dataset and then fine tuning on a smaller, original, supervised dataset yields a high quality object insertion model.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_3.jpg' | relative_url }})
![Figure 4: Object removal comparison with inpainting. Our model successfully removes the masked object, while the baseline inpainting model replaces it with a different one. Using a mask that covers the reflections (extended mask) may obscure important details from the model.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_4.jpg' | relative_url }})
![Figure 5: Object removal comparison with general editing methods. We compare to general editing methods: Emu Edit and MGIE. These methods often replace the object with a new one and introduce unintended changes to the input image. For this comparison we used a text-based segmentation model to mask the object according to the instruction and passed the mask as input to our model.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_5.jpg' | relative_url }})
![Figure 6: Intra-image object insertion baseline comparison. We preserve the object identity better and achieve more photorealistic shadows and reflections than baselines Paint-by-Example and AnyDoor.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_6.jpg' | relative_url }})
![Figure 7: Cross-image object insertion. Similarly to the results of intra-image object insertion, our method preserves object identity better and synthesizes more photorealistic shadows and reflections than the baselines.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_7.jpg' | relative_url }})
![Figure 8: Bootstrapping ablation. Bootstrap supervision improves model quality.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_8.jpg' | relative_url }})
![Figure 9: Counterfactual dataset size. Increasing the size of the training dataset improves object removal performance. The results are of high quality with 2500 examples, but may improve further with more images.]({{ '/images/03-2024/ObjectDrop:_Bootstrapping_Counterfactuals_for_Photorealistic_Object_Removal_and_Insertion/figure_9.jpg' | relative_url }})
