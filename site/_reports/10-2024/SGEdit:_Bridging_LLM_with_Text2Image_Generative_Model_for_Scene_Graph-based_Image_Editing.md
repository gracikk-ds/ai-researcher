---
title: SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing
layout: default
date: 2024-10-15
---
![Fig. 1. Our approach predicts a scene graph as a user interface, enabling modifications to nodes and edges for various tasks, such as changing relationships or replacing, adding, and removing elements. By integrating LLM and Text2Image generative models, users can explore diverse compositions in their images, ensuring that these alterations accurately reflect the structural changes depicted in the modified scene graph. Input images are sourced from ©Unsplash.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the limitations of existing image editing methods. Traditional text-based editing often lacks the precision to modify specific objects or their relationships without complex user input like masks. Conversely, prior scene graph-based methods suffer from poor image quality, are restricted to a closed set of object categories, and cannot handle diverse, in-the-wild images. The paper aims to bridge this gap by creating a framework that allows for precise, flexible, and high-quality image editing using an intuitive scene graph interface, applicable to a wide variety of open-vocabulary objects and scenes.

## 2. Key Ideas and Methodology
The core idea is to integrate a Large Language Model (LLM) with a Text2Image diffusion model, using a scene graph as the structured medium for user interaction. The methodology is a two-stage process:

1.  **Scene Parsing and Concept Learning:** An LLM-driven scene parser first analyzes an input image to automatically construct a scene graph. For each object (node), it extracts a segmentation mask (via Grounded-SAM) and a detailed text description. These annotations are then used to fine-tune a diffusion model to learn the specific visual concept of each object. This is achieved through a novel combination of an optimized token (similar to Textual Inversion) to capture unique identity and a detailed text prompt to encode general attributes, which improves both visual preservation and editability.

2.  **Image Editing:** The user modifies the scene graph. An LLM-based "editing controller" interprets these changes, translating them into a sequence of "remove" and "insert" operations with corresponding text prompts and target locations. These operations are executed by an "Attention-modulated Diffusion Editor." Object removal is performed by modulating self-attention to inpaint the erased region using context from the preserved areas. Object insertion uses modulated self- and cross-attention to precisely place new objects within specified bounding boxes, ensuring strong spatial control and coherence.

## 3. Datasets Used / Presented
The method is primarily evaluated on its editing capabilities rather than trained on a specific dataset. The evaluations were conducted on:

*   **Custom Internet Images:** A set of 30 diverse images collected from sources like Unsplash and iStockphoto, containing 2-6 foreground objects each. A total of 120 unique editing tasks (adding, replacing, removing, and modifying relationships) were created for evaluation.
*   **PIE-Bench:** A public image editing benchmark used to evaluate object deletion, replacement, and relationship modification tasks. Standard quantitative metrics (e.g., PSNR, LPIPS, Structure Distance) were used.
*   **EditVal:** Another public benchmark used to assess performance on spatial and non-spatial editing tasks, measured with metrics like OwL-ViT and DINO scores.

## 4. Main Results
The proposed method, SGEdit, demonstrated superior performance across all evaluations compared to baselines like SIMSG, SGDiff, InstructP2P, and Break-a-Scene.

*   **Human User Study:** In a study on 120 edits, SGEdit was preferred by users a majority of the time for Element Composition (80% win rate), Relationship Compliance (82% win rate), and overall Image Quality (69% win rate).
*   **Quantitative Benchmarks:** On PIE-Bench, SGEdit achieved the best scores for structure preservation (0.06 distance), background preservation (22.45 PSNR), and text-image alignment (24.19 CLIP score). On EditVal, it scored highest in both editing accuracy (0.53 OwL-ViT) and visual fidelity (0.83 DINO).
*   **Author-claimed Impact:** The framework significantly outperforms existing methods in editing precision and scene aesthetics by successfully bridging the reasoning power of LLMs with the generative capabilities of diffusion models through a scene graph interface.

## 5. Ablation Studies
The authors conducted extensive ablation studies to validate their design choices:

*   **Concept Learning:** Comparing their combined approach (detailed prompt + optimized token) against using either one alone showed that their method best preserves object identity while maintaining editability (e.g., allowing pose changes).
*   **Object Removal:** Removing their attention modulation resulted in inconsistent background filling, while omitting the non-object prompt left visual artifacts ("afterimages") of the removed object. Their full method produced clean erasures.
*   **Object Insertion:** Compared to methods like ControlNet and DenseDiffusion, their attention modulation approach was more effective at generating correctly shaped and sized objects within the specified bounding boxes without artifacts.
*   **Layout Generation:** The LLM controller was shown to be more accurate at predicting object placement than a baseline LayoutTransformer, as it could better interpret relational context (e.g., "on top of").
*   **Multi-Object Insertion:** A dedicated multi-sampler strategy was shown to be crucial for preventing "attribute leakage" (e.g., concepts blending) when adding multiple new objects to a scene.

## 6. Paper Figures
![Fig. 10. Ablation study on the object removal module. From left to right: (a) Input images; (b) Simplified scene graphs and user edits; (c) SD inpainting; (d) Without non-object prompt; (e) Without attention guidance; (f) Ours. The input images are from ©Unsplash.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_10.jpg' | relative_url }})
![Fig. 2. Our pipeline consists of two main stages: scene parsing and image editing. In the scene parsing stage, the input image is processed by our LLM-driven scene parser, which creates a scene graph and annotations for nodes such as object masks and captions. The node annotations allow for the fine-tuning of the diffusion model, representing each object in the scene with an optimized token and a specific prompt. During the image editing stage, the LLM editing controller translates user manipulations on the scene graph into a sequence of operations with text prompts and directs the targeted edits to specific regions. These edits are implemented by applying attention modulation to the fine-tuned diffusion model, enabling object additions, removals, replacements, and relationship modifications in the scene. The input image is from ©iStockphoto.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_2.jpg' | relative_url }})
![Fig. 3. Screenshot of our interface. The input image is from © iStockphoto.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_3.jpg' | relative_url }})
![Fig. 4. Contents represented by the detailed description and optimized token. The leftmost column shows the input images, while the three right columns display images generated using the optimized token, detailed description, and their combination. The combination best preserves the woman’s visual identity. The input image is from © Unsplash.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_4.jpg' | relative_url }})
![Fig. 5. The illustration of our Attention Modulated Object Removal and Insertion. The left part shows the attention modulation in self-attention for object removal, and the right part shows the attention modulation in both self and cross-attention for object insertion.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_5.jpg' | relative_url }})
![Fig. 6. Qualitative comparison with other baseline methods. From left to right: (a) Input images; (b) Scene graphs and user edits; (c) SIMSG [Dhamo et al . 2020]; (d) SGDiff [Yang et al . 2022]; (e) Break-a-scene [Avrahami et al . 2023a]; (f) InstructPix2Pix [Brooks et al . 2023]; (g) Ours. Input images: the 1 st , 4 th , and 6 th rows are from ©iStockphoto; the 2 nd , 3 rd , 5 th , 7 th , and 8 th rows are from ©Unsplash.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_6.jpg' | relative_url }})
![Fig. 7. Ablation study on different concept learning methods. From left to right: (a) Input images; (b) Simplified scene graphs and user edits; (c) Detailed prompt without textual inversion; (d) Textual inversion without detailed prompt; (e) Ours, which uses a detailed description plus textual inversion. The input images are from ©Unsplash.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_7.jpg' | relative_url }})
![Fig. 8. Ablation study on concept learning with prompts of varying levels of detail. From left to right: (a) Input images; (b) No detail: "A photo of a <opt>". (c) Medium detail: row 1, "<opt>...She is wearing a motorcycle helmet, a tight red dress."; row 2, "<opt>...The character has brown hair." (d) Full detail: row 1, "<opt>...She is wearing a motorcycle helmet, a tight red dress and black knee-high boots."; row 2: "<opt>...The character has brown hair and is wearing a teal cap." The input images are from ©Unsplash.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_8.jpg' | relative_url }})
![Fig. 9. Ablation study on scene graph-to-layout generation. From left to right: (a) Input images; (b) Simplified scene graphs and user edits; (c) Layouts generated by Layout-Transformer (LT); (d) Images generated from LT layout; (e) Layouts generated by LLM; (f) Images generated from LLM layout (ours). Input images: the 1 st row is from ©Unsplash; the 2 nd row is from ©iStockphoto.]({{ '/images/10-2024/SGEdit:_Bridging_LLM_with_Text2Image_Generative_Model_for_Scene_Graph-based_Image_Editing/figure_9.jpg' | relative_url }})
