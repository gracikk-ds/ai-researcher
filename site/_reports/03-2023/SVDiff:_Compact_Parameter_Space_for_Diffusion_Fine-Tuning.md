---
title: SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning
layout: default
date: 2023-03-20
---
![Figure 1. Applications of SVDiff. Style-Mixing : mix styles from personalized objects and create novel renderings; Multi-Subject : generate multiple subjects in the same scene; Single-Image Editing : text-based editing from a single image.]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the key limitations of fine-tuning large-scale text-to-image diffusion models for personalization. Existing methods like DreamBooth require updating the entire model, resulting in massive storage costs (billions of parameters), a high risk of overfitting to the few provided images, and "language drift," where the model loses its general knowledge and ability to follow diverse text prompts. Furthermore, these methods struggle to generate coherent scenes containing multiple personalized subjects, especially when the subjects are from similar categories (e.g., a specific cat and a specific dog).

## 2. Key Ideas and Methodology
The paper introduces a compact and efficient fine-tuning method called **SVDiff**.

-   **Core Hypothesis:** The core idea is that significant personalization can be achieved by modifying only the singular values of the model's weight matrices, rather than the full matrices. This is termed "spectral shift."
-   **High-level Approach:** The method first performs a one-time Singular Value Decomposition (SVD) on the pre-trained weight matrices. During fine-tuning, instead of learning billions of parameters, it only optimizes a small vector of "spectral shifts" (`δ`) that are added to the original singular values. The updated weights are then reconstructed from the original singular vectors and the new singular values. This drastically reduces the number of trainable parameters, acting as a strong regularizer against overfitting.
-   **Key Techniques:**
    -   **Cut-Mix-Unmix:** A data augmentation technique for multi-subject generation. It involves creating composite training images by cutting and pasting multiple subjects into one scene and using a corresponding text prompt (e.g., "a [V1] dog and a [V2] cat"). This explicitly teaches the model to disentangle and compose different personalized concepts.
    -   **CoSINE (Compact parameter space for SINgle image Editing):** An application of SVDiff for text-based editing of a single real image. The compact parameter space prevents the model from overfitting to the single image, thus preserving its editability.

## 3. Datasets Used / Presented
The paper does not introduce new datasets. All experiments are conducted by fine-tuning the publicly available **Stable Diffusion** model. For personalization tasks, the authors follow the standard setup of using small sets of 3-5 images per subject. These subject images were sourced from:
-   The dataset provided with the **Custom Diffusion** paper (e.g., teddy bear, cat, dog images).
-   Publicly available image hosting sites like **Unsplash** and **Getty Images** for other concepts (e.g., specific chairs, cars, buildings).

## 4. Main Results
-   **Parameter Efficiency:** SVDiff achieves a massive reduction in model size. The resulting checkpoint for a personalized concept is approximately 2,200 times smaller than a full DreamBooth checkpoint (e.g., 1.7MB for SVDiff vs. 3.66GB for a full model).
-   **Generation Quality:** In single-subject generation, SVDiff produces images of comparable quality to DreamBooth while better preserving subject identity than other compact methods like Custom Diffusion.
-   **Multi-Subject Generation:** The Cut-Mix-Unmix technique enables the model to successfully generate multiple distinct subjects in one image, even for semantically similar categories. A user study found that SVDiff's multi-subject generations were preferred over those from a fully fine-tuned model 60.9% of the time.
-   **Improved Editability:** In single-image editing, SVDiff successfully performs text-guided modifications where full fine-tuning fails due to language drift (e.g., it can remove an object from a scene when the fully fine-tuned model cannot).

## 5. Ablation Studies
-   **Parameter Subsets:** The authors experimented with fine-tuning spectral shifts in different subsets of the UNet's layers. They found that fine-tuning only the cross-attention layers yielded the best identity preservation, while fine-tuning only the up-blocks or down-blocks of the UNet was insufficient.
-   **Weight Combination and Interpolation:** The learned spectral shifts from different subjects were arithmetically combined (added or interpolated). This successfully created novel compositions and style transfers (e.g., rendering a personalized dog in the style of a personalized sculpture), demonstrating the semantic properties of the compact parameter space.
-   **Rank of Spectral Shifts:** The rank of the spectral shifts was varied during training. A lower rank (e.g., rank 1) was sufficient for basic subject reconstruction but struggled to capture fine details in complex edited images, for which higher ranks were necessary.
-   **Cut-Mix-Unmix Augmentation:** The effectiveness of the data augmentation was confirmed. Without it, the model tends to blend or merge features of similar subjects. The technique was shown to be robust to different spatial layouts (e.g., "left-right" vs. "up-down").

## 6. Paper Figures
![Figure 10. (a) Correlation of individually learned spectral shifts for different subjects. The cosine similarities between the spectral shifts of two subjects are averaged across all layers and plotted. The diagonal shows average similarities between two runs with different learning rates. (b) Textand image-alignment for singlesubject generation. The generated image is denoted as ˜ x . The text-alignment is measured by the CLIP score [ 14 , 51 ] cos(˜ x , c ) , and the image-alignment is defined as 1 −L LPIPS (˜ x , x ∗ ) [ 84 ].]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_10.jpg' | relative_url }})
![Figure 11. Effects of interpolating spectral shifts ( Σ δ ′ = diag ( ReLU ( σ + α δ 1 + (1 − α ) δ 2 )) ) or weight deltas ( W ′ = W + α ∆ W 1 + (1 − α )∆ W 2 = αW 1 + (1 − α ) W 2 ).]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_11.jpg' | relative_url }})
![Figure 2. Performing singular value decomposition (SVD) on weight matrices. In an intermediate layer of the model, (a) the convolutional weights W tensor (b) serve as an associative memory [ 8 ]. (c) SVD is performed on the reshaped 2-D matrix W .]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_2.jpg' | relative_url }})
![Figure 3. Cut-Mix-Unmix data-augmentation for multi-subject generation . The figure shows the process of Cut-Mix-Unmix data augmentation for training a model to handle multiple concepts. The method involves (a) manually constructing image-prompt pairs where the image is created using a CutMix-like data augmentation [ 82 ] and the corresponding prompt is written as, for example, “photo of a [ V 2 ] sculpture and a [ V 1 ] dog”. The prior preservation image-prompt pairs are created in a similar manner. The objective is to train the model to separate different concepts by presenting it with explicit mixed samples. (b) To perform unmix regularization, we use MSE on non-corresponding regions of the cross-attention maps to enforce separation between the two subjects. The goal is to encourage that the dog’s special token should not attend to the panda and vice versa. (c) During inference, a different prompt, such as “photo of a [ V 1 ] dog sitting besides a [ V 2 ] sculpture”.]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_3.jpg' | relative_url }})
![Figure 4. Pipeline for single image editing with a text-to-image diffusion model. (a) The model is fine-tuned with a single imageprompt pair, where the prompt describes the input image without a special token. (b) During inference, desired edits are made by modifying the prompt. For edits with no significant structural changes, the use of DDIM inversion [ 65 ] has been shown to improve the editing quality.]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_4.jpg' | relative_url }})
![Figure 5. Results for single subject generation . DreamBooth [ 57 ] and Custom Diffusion [ 37 ] are implemented in StableDiffusion with Diffusers library [ 75 ]. Each subfigure consists 3 samples: a large one on the left and 2 small one on the right. The text prompt under input images are used for training and the text prompt under sample images are used for inference. We observe that SVDiff performs similarly as DreamBooth (full-weight fine-tuning), and preserves subject identities better than Custom Diffusion for row 2, 3, 5.]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_5.jpg' | relative_url }})
![Figure 6. Results for multi-subject generation . (a-d) show the results of fine-tuning on two subjects and (e-g) show the results of fine-tuning on three subjects. Both full weight (“Full”) fine-tuning and SVDiff (“SVD”) can benefit from the Cut-Mix-Unmix dataaugmentation. Without Cut-Mix-Unmix, the model struggles to disentangle subjects of similar categories, as demonstrated in the last two columns of (a,b,c,d,g).]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_6.jpg' | relative_url }})
![Figure 7. Results for single image editing . SVDiff (“Ours”) enables successful image edits despite slight misalignment with the original image. SVDiff performs desired modifications when full model fine-tuning (“Full”) fails, such as removing an object (2nd edit in (a)), adjusting pose (2nd edit in (c)), or zooming in (3rd edit in (d)). The backgrounds in some cases may be affected, however, the subject of the image remains well-preserved. For ours, we use DDIM inversion [ 65 ] for all edits in (a,c,e) and the first edit in (d).]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_7.jpg' | relative_url }})
![Figure 8. Effects of combining spectral shifts ( Σ δ ′ = diag ( ReLU ( σ + δ 1 + δ 2 )) ) and weight deltas ( W ′ = W + ∆ W 1 + ∆ W 2 ) in one model. The combined model retains individual subject features but may mix styles for similar subjects. The results also suggests that the task arithmetic property [ 30 ] of language models also holds in StableDiffusion.]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_8.jpg' | relative_url }})
![Figure 9. Results of style transfer. Changing coarse class word: (d) and (i); Appending “in style of”: (e) and (j); Combined spectral shifts: (a,b,f,g).]({{ '/images/03-2023/SVDiff:_Compact_Parameter_Space_for_Diffusion_Fine-Tuning/figure_9.jpg' | relative_url }})
