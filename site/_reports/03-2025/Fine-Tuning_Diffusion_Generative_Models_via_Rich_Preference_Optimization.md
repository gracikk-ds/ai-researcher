---
title: Fine-Tuning_Diffusion_Generative_Models_via_Rich_Preference_Optimization
layout: default
date: 2025-03-13
---
## Fine-Tuning Diffusion Generative Models via Rich Preference Optimization
**Authors:**
- Hanyang Zhao, h-index: 6, papers: 10, citations: 97
- Wenpin Tang, h-index: 5, papers: 9, citations: 82

**ArXiv URL:** http://arxiv.org/abs/2503.11720v3

**Citation Count:** 0

**Published Date:** 2025-03-13

![Figure 1: (Top) Our RPO pipeline for curating informative preference pairs from images generated from the base diffusion models: (1) Rich Feedback/Critic generation by a Vision Language Model (for which we choose LLaVA-Critic-7B), (2) Actionable editing instruction generation based on the critiques by another VLM (for which we chose Qwen2.5-VL-8BInstruct), (3) Instruction-following image editing from the generated editing instructions (for which we choose ControlNet), and (4) Diffusion DPO training using reward model filtered synthetic preference pairs. (Bottom) Sample images generated from RPO fine-tuned Stable Diffusion XL, by further aligning the model on our generated synthetic preferences.]({{ '/images/03-2025/Fine-Tuning_Diffusion_Generative_Models_via_Rich_Preference_Optimization/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the limitations of current methods for fine-tuning text-to-image diffusion models, such as Diffusion-DPO. These methods typically rely on preference data labeled by reward models, which provide opaque, numerical scores. This type of feedback offers limited insight into *why* one image is preferred over another, making the alignment process inefficient and susceptible to issues like reward hacking. The paper identifies a gap in leveraging "rich feedback"—detailed, informative critiques that provide clear direction for improvement—for vision tasks, a practice more established in natural language processing.

## 2. Key Ideas and Methodology
The core idea is **Rich Preference Optimization (RPO)**, a novel pipeline designed to generate high-quality, synthetic preference pairs for fine-tuning diffusion models. The methodology is inspired by a student-teacher learning dynamic and breaks down the improvement process into explicit steps:

1.  **Critique Generation:** A Vision-Language Model (VLM), **LLaVA-Critic**, is used as a "critic" to analyze an image generated by a base model and produce a detailed textual critique highlighting misalignments with the input prompt.
2.  **Instruction Generation:** This critique is then fed to another VLM, **Qwen2.5-VL-Instruct**, which translates the high-level feedback into a set of concise, actionable editing instructions. This two-step process is reminiscent of Chain-of-Thought reasoning.
3.  **Image Editing:** The editing instructions are executed by **ControlNet**, which modifies the original image. Using ControlNet ensures that the edits are applied while maintaining coherence with the original image's composition.
4.  **Preference Pair Curation:** The original and the newly refined image form a preference pair. These pairs are filtered using a reward model (ImageReward) to ensure quality and are then used as an enhanced dataset to fine-tune diffusion models via the Diffusion-DPO algorithm.

## 3. Datasets Used / Presented
*   **Pick-a-Pic v2:** A large-scale human preference dataset. It was used to train the baseline Diffusion-DPO models (e.g., DPO-SD1.5-100k was trained on the first 100k pairs). The test set (containing 500 unique prompts) was used for the final evaluation of all models.
*   **RichHF dataset:** The test set (1.6k samples) was used during ablation studies to compare the authors' feedback generation approach with an existing source of rich feedback.
*   **RPO100k (Presented):** The authors created a new synthetic dataset of 100k preference pairs using their RPO pipeline. This dataset was used to further fine-tune the baseline models to demonstrate the effectiveness of the proposed method.

## 4. Main Results
The main results demonstrate that fine-tuning with the RPO-generated dataset significantly improves model performance and data efficiency.

*   Models fine-tuned with RPO data (**+RPO100k**) consistently outperformed their corresponding baselines across multiple metrics, including ImageReward, HPSv2, and Aesthetic Score. For instance, DPO-SD1.5-100k+RPO100k achieved an ImageReward score of 0.4395, a substantial improvement over the baseline's 0.2784.
*   The RPO pipeline proved to be highly data-efficient. A model trained on a mix of 100k human-labeled and 100k RPO-synthetic pairs (**DPO-SD1.5-100k+RPO100k**) outperformed a model trained on 200k human-labeled pairs (**DPO-SD1.5-200k**), particularly on the ImageReward metric (0.4395 vs. 0.3638).
*   The authors' takeaway is that their pipeline for synthesizing preference pairs from rich feedback is an effective method to augment high-quality human-annotated datasets, enhancing fine-tuning performance, especially when human data is limited.

## 5. Ablation Studies
*   **Feedback Mechanism:** The authors compared different methods for generating editing instructions. They found that using a dedicated critic model (LLaVA-Critic) to first generate a critique, which was then converted into instructions by GPT-4o, yielded significantly better results than having GPT-4o generate instructions directly (1-step) or via self-critique (2-step). This validated their two-stage, "critique-then-instruct" approach.
*   **VLM for Instruction Generation:** Several open-source VLMs were evaluated on their ability to convert critiques into editing instructions. **Qwen2.5-VL-7B-Instruct** produced the best results, as measured by the reward scores of the subsequently edited images, leading to its selection for the final RPO pipeline.
*   **Image Editing Model:** A qualitative comparison between **ControlNet** and InstructPix2Pix showed that ControlNet produced more precise, fine-grained edits that better followed instructions while preserving the overall structure of the original image, confirming its suitability for the pipeline.

## 6. Paper Figures
![Figure 2: (L) We utilize ControlNet by using the same input image as the conditional image, and concatenating the prompt with the editing instruction as the textual control. (R) We compare ControlNet with InstructPix2Pix and also ablate the necessity of concatenating the prompt with the editing instructions, which are generated by ChatGPT-4o. (Top) : The prompt is “Italian coastline, buildings, ocean, architecture, surrealism by Michiel Schrijver.” The editing instruction is “Incorporate iconic Italian elements like olive trees or Vespa scooters. Enhance the coastline with more distinct Mediterranean features. Add intricate architectural details typical of Italian structures. Intensify the ocean’s depth with gradient blues, and ensure the surrealism reflects Michiel Schrijver’s style by blending dreamlike elements.” In this case, InstructPix2Pix struggles to make any fine-grained modifications. (Bottom) : The prompt is “Mickey Mouse in a Superman outfit bodybuilding.” The editing instruction is “Adjust the character to have Mickey Mouse’s face, including distinct ears, in a Superman outfit. Include bodybuilding elements such as visible muscles or weights. Ensure the outfit is accurate with the Superman logo prominently displayed.” In this case, InstructPix2Pix distorts the image. In both cases, adding the image prompt to the editing instruction for the final instruction yields better results in terms of following the instructions while keeping most of the original image unchanged.]({{ '/images/03-2025/Fine-Tuning_Diffusion_Generative_Models_via_Rich_Preference_Optimization/figure_2.jpg' | relative_url }})
![Figure 3: Comparison of RichFB generated informative feedback and our adopted textual criticism generate by carefully prompting a capable VLM.]({{ '/images/03-2025/Fine-Tuning_Diffusion_Generative_Models_via_Rich_Preference_Optimization/figure_3.jpg' | relative_url }})
![Figure 4: Comparisons of feedback approaches and VLM performance for enhanced image editing, evaluated by ImageReward, HPSv2 and PickScore.]({{ '/images/03-2025/Fine-Tuning_Diffusion_Generative_Models_via_Rich_Preference_Optimization/figure_4.jpg' | relative_url }})
![Figure 5: Model performance evaluated by PickScore, ImageReward, Aesthetic, and HPSv2.]({{ '/images/03-2025/Fine-Tuning_Diffusion_Generative_Models_via_Rich_Preference_Optimization/figure_5.jpg' | relative_url }})
![Figure 6: A comparison of generations made by SDXL1.0, DPO-SDXL-100k, and DPOSDXL-100k+RPO100k. Prompts (from left to right): “tiger wearing casual outfit”, “an adventurer walking along a riverbank in a forest during the golden hour in autumn”, “samurai pizza cat”, “anime portrait of a beautiful vamire witch, sci fi suit, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by grep rutkowski and” (truncated due to the limit of number of tokens).]({{ '/images/03-2025/Fine-Tuning_Diffusion_Generative_Models_via_Rich_Preference_Optimization/figure_6.jpg' | relative_url }})
