---
title: Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning
layout: default
date: 2025-03-24
---
![Figure 1. Results showcasing the strength of Instruct-CLIP (I-CLIP) compared to state-of-the-art InstructPix2Pix (IP2P) [ 1 ].]({{ '/images/03-2025/Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the problem of low-quality results in instruction-guided image editing models. They identify the primary cause as flawed training datasets, which are often generated automatically. In these datasets, there is frequently a misalignment between the textual edit instruction and the actual visual change depicted in the original-edited image pairs. This "noisy" supervision negatively impacts the performance of models trained on them, leading to inaccurate edits or unintended changes to irrelevant image regions.

## 2. Key Ideas and Methodology
The core idea is **Instruct-CLIP (I-CLIP)**, a self-supervised method designed to automatically refine the instructions in existing datasets to better align with the visual edits.

The methodology consists of four main parts:
1.  **Contrastive Learning**: Inspired by CLIP, I-CLIP learns a shared embedding space for visual changes and text instructions. It uses a novel visual encoder (`I-CLIPvis`) that takes both the original and edited images as input to specifically encode their difference, and a text encoder (`I-CLIPtxt`) for the instruction. These are trained with a contrastive loss to align the two modalities.
2.  **Instruction Refinement**: A text decoder is trained to translate the learned embeddings back into text. At inference, it takes the visual-change embedding from an image pair and generates a new, more accurate text instruction that better describes the edit.
3.  **Diffusion Model Integration**: To work with modern latent diffusion models (LDMs), the I-CLIP visual encoder is adapted to handle noisy latent images. A specialized backbone (`LD-DINOv2`) is trained to extract features from noisy latents as if they were clean, making it compatible with the diffusion process.
4.  **Guided Training**: The final editing model is fine-tuned using a composite loss function. This combines the standard LDM training objective (MSE noise prediction) with a novel **I-CLIP-guided loss**, which explicitly forces the visual change in the generated image to semantically match the refined text instruction.

## 3. Datasets Used / Presented
*   **InstructPix2Pix (IP2P)**: An existing dataset with approximately 450,000 samples (later filtered to ~313K) of original images, edited images, and corresponding edit instructions. This was used as the base dataset for refinement.
*   **Refined IP2P Dataset (Presented)**: A key contribution of the paper. The authors applied I-CLIP to the IP2P dataset to create a new version containing over **120,000 refined instructions**. This dataset was used to train their final model.
*   **MagicBrush (MagBr) & ZONE**: Two existing benchmarks used for quantitative and qualitative evaluation of the final editing model against state-of-the-art methods.

## 4. Main Results
*   **Quantitative Performance**: On the MagicBrush single-turn editing benchmark, the proposed model significantly outperformed the InstructPix2Pix (IP2P) baseline. It achieved a **24.5% relative improvement in DINO-I similarity** (a measure of visual alignment with the ground truth) and a 6.7% relative improvement in CLIP-I similarity.
*   **User Study**: In a pairwise comparison study with 104 participants, the authors' method was strongly preferred over both IP2P (winning 29.7% of matchups vs. IP2P's 11.9%) and MagicBrush (winning 54.2% vs. MagBr's 29.8%), indicating a clear improvement in perceived visual quality.
*   **Author-claimed Impact**: The paper demonstrates that by first refining noisy training data and then using a semantic guidance loss during training, it is possible to produce an editing model that generates results more faithfully aligned with user instructions.

## 5. Ablation Studies
The authors conducted an ablation study to validate the impact of their two main contributions: (1) the refined dataset and (2) the I-CLIP guidance loss.

*   **Impact of Refined Data**: A model trained on the *refined* dataset (but without the I-CLIP loss) performed significantly better than a model trained on the original IP2P data. On the MagicBrush dataset, this change alone increased the DINO-I score from 0.671 to 0.782, confirming that data quality is critical.
*   **Impact of I-CLIP Guidance Loss**: Adding the I-CLIP guidance loss during training on the refined dataset provided a further performance boost, increasing the DINO-I score from 0.782 to 0.803. This shows that enforcing semantic alignment during the diffusion model's training is also beneficial.

The study concluded that both the data refinement process and the novel guidance loss are crucial components that independently and collectively contribute to the final model's superior performance.

## 6. Paper Figures
![Figure 2. Problems with existing instruction-guided imageediting datasets [ 1 ]. As shown, there are many examples where the dataset’s original edit instruction does not match the actual changes in the images. Our I-CLIP approach refines edit instructions to match the visual change better and allows us to train a system that produces better outputs. The values in parentheses are the cosine similarity between the visual change from the original to the edited image and the edit instruction from I-CLIP.]({{ '/images/03-2025/Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning/figure_2.jpg' | relative_url }})
![Figure 3. Instruct-CLIP architectures. (a) Overview of Instruct-CLIP (I-CLIP), which embeds the visual change in the original/edited images I o and I e and the edit instruction p into the same feature space through contrastive loss, L contrast (Eq. 2 ). To obtain refined instruction p from its I-CLIP embedding z txt , we adopt the same approach in DeCap [ 12 ] to decode z txt back to p using cross-entropy loss, L DeCap (Eq. 4 ). At inference time, the text decoder takes the embedded visual change from the original to the edited image ( z vis ) and decodes it to produce a new instruction. Due to the significant cosine similarity gap between z vis and z txt even when they are well aligned, directly decoding z vis leads to suboptimal results. To achieve a representation of z vis closer to the text features that the instruction decoder learned during training, we compute ( z vis ) ′ with Eq. 6 and decode it to obtain the refined instruction p ′ , which is used to improve the dataset. (b) The architecture of image encoder I-CLIP vis includes two shared-weighted DINOv2 [ 18 ] modules in front of a standard CLIP vis encoder.]({{ '/images/03-2025/Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning/figure_3.jpg' | relative_url }})
![Figure 4. Training our LD-DINOv2 model. To use I-CLIP as part of the training objective for Stable Diffusion [ 23 ], it needs to handle noisy latent images. Therefore, we replace the original DINOv2 backbone in Fig. 3b with a latent-diffusion version of it we call LD-DINOv2, which takes both the noisy latent image ˜ L k from SD VAE encoding and forward-diffusion (FD) timestep t k . We then train LD-DINOv2 to “ignore” the noise and the latentspace compression and to extract the original DINOv2 features using the training objective L LD-DINO v2 (Eq. 7 ).]({{ '/images/03-2025/Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning/figure_4.jpg' | relative_url }})
![Figure 5. Comparison with state-of-the-art approaches for instruction-guided image editing, including HIVE [ 33 ], Inst-Inpaint (I-Inp) [ 30 ], Watch Your Steps (WYS) [ 16 ], ZONE [ 11 ], MagicBrush (MagBr) [ 32 ], InstructPix2Pix (IP2P) [ 1 ] showcasing the strength of our approach. CLIP-T value of each output is shown at its top-left corner, with the best value per row underlined. Note that the image with the best CLIPT score is not necessarily the visually best result, underscoring the deficiencies of conventional metrics (including CLIP-I and DINO-I shown in the supplemental) for measuring the quality of image edits.]({{ '/images/03-2025/Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning/figure_5.jpg' | relative_url }})
![“Replace the truck with a tractor” Figure 6. Effect of refined instructions and Instruct-CLIP guidance loss. Compared to variants of our model trained without refined instructions (“Ours (w/o data)”) or without the guidance loss (“Ours (w/o loss)”), our model produce superior results.]({{ '/images/03-2025/Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning/figure_6.jpg' | relative_url }})
![Figure 7. Examples of failure cases compared with InstructPix2Pix (IP2P) [ 1 ] and MagicBrush (MagBr) [ 32 ] performed on the original (Ori) images.]({{ '/images/03-2025/Instruct-CLIP:_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_Contrastive_Learning/figure_7.jpg' | relative_url }})
