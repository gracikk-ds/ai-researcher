---
title: DCEdit:_Dual-Level_Controlled_Image_Editing_via_Precisely_Localized_Semantics
layout: default
date: 2025-03-21
---
## DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics
**Authors:**
- Yihan Hu
- Yunchao Wei, h-index: 13, papers: 48, citations: 711

**ArXiv URL:** http://arxiv.org/abs/2503.16795v1

**Citation Count:** 0

**Published Date:** 2025-03-21

![“… with sunglasses …” “… Astronaut ” “… card saying Stop! ” “… a card ” “… Iron Man ” Figure 1. Given a real-world image as input, DCEdit manipulates the original content into a different semantics without additional training or tuning and could be applied to existing DiTs-based methods in a plug-and-play manner. All above results are derived from our welldesigned editing benchmark RW-800.]({{ '/images/03-2025/DCEdit:_Dual-Level_Controlled_Image_Editing_via_Precisely_Localized_Semantics/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address a key challenge in text-guided image editing with modern Diffusion Transformer (DiT) models: the imprecise localization of target semantics. Previous methods often struggle to accurately identify the specific region to be edited, leading to incomplete modifications or unintended alterations to the background. This problem is particularly pronounced in complex, real-world images with detailed descriptions. The paper aims to improve editing precision by refining the model's internal attention maps to serve as accurate regional guides.

## 2. Key Ideas and Methodology
The paper introduces **DCEdit**, a novel approach that operates in a plug-and-play manner on existing DiT-based editors. The core methodology consists of two main components:

1.  **Precise Semantic Localization (PSL):** This strategy refines the raw cross-attention maps generated by the DiT model. It leverages two key insights:
    *   It uses the **visual self-attention** map to complete and clarify the boundaries of the target object's activation region.
    *   It uses the **inverse of the textual self-attention** map to disentangle competing semantics in the text prompt, preventing incorrect activations on background objects.

2.  **Dual-Level Control (DLC):** This mechanism uses the refined attention map from PSL to guide the editing process at two distinct levels:
    *   **Feature-Level Control:** It performs a *soft integration* of features from the source and edited images during the diffusion process, guided by the continuous attention map. This selectively preserves necessary features while allowing for effective editing.
    *   **Latent-Level Control:** It employs a binarized version of the attention map to blend the noisy latents, robustly preserving the background content, especially during the initial steps of image generation.

## 3. Datasets Used / Presented
The method is evaluated on two benchmarks:

*   **PIE-Bench:** A standard public benchmark for image editing, containing 692 images (512x512) and 700 editing prompt pairs. It is used for comparison against a wide range of existing methods.
*   **RW-800 Benchmark (Presented):** The authors introduce this new, more challenging benchmark. It consists of 207 high-resolution (avg. 3840x3690), real-world images with 800 editing pairs. It features longer, more descriptive prompts and a new "text editing" task, making it better suited for evaluating modern DiT models.

## 4. Main Results
The proposed DCEdit method demonstrates superior performance in both background preservation and editing accuracy.

*   On **PIE-Bench**, when applied to the FireFlow editor, DCEdit improves background preservation by reducing the Mean Squared Error (MSE) by 25% and LPIPS by 18%, while also slightly increasing the editing accuracy (CLIP score).
*   On the more challenging **RW-800 benchmark**, the improvements are even more significant. DCEdit reduces the background MSE of FireFlow by 38% and RF-Edit by 20%, showcasing its robustness on high-resolution, complex scenes.

The authors conclude that their approach significantly enhances the precision and quality of DiT-based image editing without requiring additional training.

## 5. Ablation Studies
The authors performed ablation studies on the RW-800 benchmark to validate the contribution of each component of their method.

*   **PSL Components:** Evaluating the localization capability, the raw FLUX cross-attention achieved an Intersection-over-Union (IoU) of 38%. Adding visual self-attention refinement (+VSA) boosted the IoU to 54%. The full PSL strategy, which also includes textual self-attention refinement, further improved the IoU to 56%, confirming that both parts of PSL contribute to more precise localization.
*   **Feature-Level Control:** Using the proposed soft attention map for guidance resulted in better structure preservation and editing quality compared to using a hard binary mask, which degraded performance.
*   **Latent-Level Control:** Integrating latent-level control significantly improved background consistency. Applying it for 5 steps reduced the structural distance to the source image by over 50% compared to having no latent control, with only a minor impact on editing fidelity.

## 6. Paper Figures
![Figure 2. Improved Semantic Localization. (1) UNet based diffusion models such as SD-1.5 and SD-XL fail to capture detailed semantics due to limitations in the model architectures. (2) Models based on MM-DiT such as FLUX have the ability to perceive these semantics but have defects in localization. (3) Our PSL produces precise localization for semantics]({{ '/images/03-2025/DCEdit:_Dual-Level_Controlled_Image_Editing_via_Precisely_Localized_Semantics/figure_2.jpg' | relative_url }})
![Figure 3. Illustration of PSL. The attention map A joint in each MM-DiT layer can be split into four parts. Among them the ˜ A V → T can reflect the corresponding areas of semantics in the image. We further correct this attention map through visual and textual self-attention parts ˜ A V → V and ˜ A T → T and generate the refined attention map M . Notably, the red anchor box on ˜ A T → T marks the competition for attention among text tokens, which results in confusion between semantics. However, as shown by the green anchor box, the inverse of ˜ A T → T can offset this confusion and help correct the semantic errors in the attention map.]({{ '/images/03-2025/DCEdit:_Dual-Level_Controlled_Image_Editing_via_Precisely_Localized_Semantics/figure_3.jpg' | relative_url }})
![Figure 4. The proposed Editing Pipeline. Given the source image I s and prompt pair { P s , P t } , we obtain the target noise Z t K through inversion process (up row). PSL works on the first inversion step incorporated with a blended word provided by diff ( P s , P t ) . In the sample process (down row), we guide the editing through Feature-level control and Latent-level control, which receive the M generated by PSL and the binary mask M λ respectively. Notably Feature-level control only works on last r layers.]({{ '/images/03-2025/DCEdit:_Dual-Level_Controlled_Image_Editing_via_Precisely_Localized_Semantics/figure_4.jpg' | relative_url }})
![Figure 6. Comparison between editing methods in our RW-800 dataset. Please zoom in for a better view.]({{ '/images/03-2025/DCEdit:_Dual-Level_Controlled_Image_Editing_via_Precisely_Localized_Semantics/figure_6.jpg' | relative_url }})
