---
title: Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations
layout: default
date: 2025-03-17
---
![Figure 1. Edit Transfer aims to learn a transformation from a given source–target editing example, and apply the edit to a query image. Our framework can effectively transfer both (b) single and (c) compositional non-rigid edits via proposed visual relation in-context learning.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
    - The authors address the limitations of existing image editing methods. Text-based Image Editing (TIE) struggles to describe and execute precise, complex spatial transformations like specific human poses. Reference-based Image Editing (RIE) methods typically focus on transferring low-level properties like style and texture, and fail to handle non-rigid geometric changes. The paper aims to bridge this gap by learning transformations directly from a visual example pair, enabling complex non-rigid edits that are difficult to achieve with text or simple reference images.

## 2. Key Ideas and Methodology
    - The core idea is **Edit Transfer**, a new task where a model learns a transformation from a single source-target example pair (Is → It) and applies it to a new query image (Îs).
    - The methodology is a **visual relation in-context learning** paradigm. The authors arrange the example source (Is), example edited (It), query source (Îs), and the target output (Ît) into a 2x2 four-panel composite image.
    - This composite is processed by a DiT-based text-to-image model (FLUX). The model is trained to generate the bottom-right panel (Ît) conditioned on the other three. To enable this, they apply lightweight Low-Rank Adaptation (LoRA) fine-tuning to the Multi-Modal Attention (MMA) layers of the model, which helps it capture the complex visual relationships between the panels.
    - The key assumption is that transformer-based vision models, like LLMs, can perform in-context learning, deducing a transformation rule from a few visual examples.

## 3. Datasets Used / Presented
    - The authors created a small, custom dataset for few-shot fine-tuning.
    - **Name**: Custom dataset for Edit Transfer.
    - **Size**: 42 images in total, comprising 21 distinct editing types (e.g., "jumping," "clapping," "smiling"). Each editing type is represented by two diverse four-panel training examples.
    - **Domain**: Human-centric images showing various poses and facial expressions.
    - **Usage**: The dataset was used to fine-tune the FLUX model to learn the Edit Transfer task.

## 4. Main Results
    - The proposed Edit Transfer method substantially outperforms state-of-the-art TIE (P2P, RF-Solver-Edit) and RIE (MimicBrush) methods on complex non-rigid editing tasks.
    - Quantitatively, the method achieved superior scores on automatic metrics: CLIP-T of 22.58 (vs. ~20 for others) and PickScore of 21.50 (vs. ~20.3-20.9).
    - In a user study, the proposed method was preferred over 80% of the time compared to baselines for text alignment, reference alignment, and overall quality.
    - The author-claimed impact is that sophisticated, non-rigid editing behaviors can be learned from a minimal number of examples (as few as 42), demonstrating the effectiveness of few-shot visual relation learning.

## 5. Ablation Studies
    - **Influence of dataset scale**: The authors varied the number of editing types (NT) and examples per type (Nc). They found that training with only one example per type (Nc=1) was insufficient for the model to learn the transformation. However, using two examples (Nc=2) was effective. Increasing the diversity of editing types from NT=10 to NT=21 further improved the model's ability to capture fine-grained details and generalize to misaligned images.
    - **Effect of fine-tuning**: The authors compared their fine-tuned model against the base FLUX model without fine-tuning. The base model failed to learn the specific transformation from the example pair (Is → It) and instead just copied some semantic aspects from the example edited image (It) onto the query. In contrast, the fine-tuned model successfully learned and applied the precise visual relationship. This demonstrates that the proposed few-shot fine-tuning is essential for the Edit Transfer task.

## 6. Paper Figures
![Figure 10. Generalization performance of Edit Transfer. Our model demonstrates remarkable generalization by: (b) Generating novel pose variations within a given editing type, even if such variations were unseen during training; (c) Flexibly combining different editing types; (d) Transferring its capabilities across other species.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_10.jpg' | relative_url }})
![Figure 11. Limitations. Our method struggles with low-level properties transfer e.g. color.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_11.jpg' | relative_url }})
![Figure 2. Comparisons with existing editing paradigms. (a) Existing TIE methods [ 20 – 25 ] rely solely on text prompts to edit images, making them ineffective for complex non-rigid transformations that are difficult to describe accurately. (b) Existing RIE methods [ 6 – 15 ] incorporate visual guidance via a reference image but primarily focus on appearance transfer, failing in non-rigid pose modifications. (c) In contrast, our proposed Edit Transfer learns and applies the transformation observed in editing examples to a query image, effectively handling intricate non-rigid edits.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_2.jpg' | relative_url }})
![Figure 3. Visual relation in-context learning for Edit Transfer. (a) We arrange in-context examples in a four-panel layout: the top row (an editing pair ( I s , I t ) ) and the bottom row (the query pair ( ˆ I s , ˆ I t ) ). Our goal is to to learn the transformation from I s →I t , and apply it to the bottom-left image ˆ I s , producing the target ˆ I t , in the bottom-right. (b) We fine-tune a lightweight LoRA in the MMA to better capture visual relations. Noise addition and removal are applied only to z t , while the conditional tokens c T ( derived from ( I s , I t , ˆ I s ) ) remain noise-free. (c) Finally, we cast Edit Transfer as an image generation task by initializing the bottom-right latent token z T with random noise and concatenating it with the clean tokens c I . Leveraging the enhanced in-context capability of the fine-tuned DiT blocks, the model generates I t , effectively transferring the same edits from the top row to the bottom-left image.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_3.jpg' | relative_url }})
![Figure 4. Edit Transfer exhibits impressive versatility to transfer visual exemplar pairs’edit into the requested source image, delivering high-quality (a) single-edit transformations as well as (b) effective compositional edits that seamlessly combine multiple modifications.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_4.jpg' | relative_url }})
![Figure 5. Qualitative comparisons. Compared with TIE and RIE methods, our method consistently outperforms in various non-rigid editing tasks. We provide the detailed text prompt of TIE methods in Section B.1 .]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_5.jpg' | relative_url }})
![Figure 6. Results of user study and VLM evaluation. We compare our Edit Transfer (IV) with P2P [ 20 ] (I), RF-Solver-Edit [ 23 ] (II) and MimicBrush [ 13 ] (III). (a) The values show the proportion of users who prefer our method over the others. (b) The values represent the average scores given to each method by GPT-4o [ 31 ].]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_6.jpg' | relative_url }})
![Figure 7. Influence of dataset scale. (a) Setting the number of training samples per editing type to N c = 2 is sufficient for learning effective non-rigid edits, even when the total number of editing types N T = 10 . (b) Increasing N T = 21 further improves the model’s ability to capture subtle local edits and enhances its generalization to cases where the editing example and query image are spatially misaligned.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_7.jpg' | relative_url }})
![Figure 8. Ours vs. w/o fine-tuning. Without fine-tuning, Flux can only capture some of the pose information identified in I t regardless of the relation between I s → I t and ˆ I s . In contrast, with our few-shot fine-tuning, the model effectively learns the visual relation from example pairs and applied to ˆ I s .]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_8.jpg' | relative_url }})
![Figure 9. Investigating the alignment between text and visual example pairs. When the text prompt and visual demonstrations convey different semantics, the generated images ˆ I t tend to (a)(b) exhibit mixed semantics from both sources, and either (c) follow the text or (d) the visual demonstrations. Note that the red label indicates misalignment, while green label indicates alignment.]({{ '/images/03-2025/Edit_Transfer:_Learning_Image_Editing_via_Vision_In-Context_Relations/figure_9.jpg' | relative_url }})
