---
title: EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation
layout: default
date: 2025-03-14
---
![Fig. 1. EmoEdit [1] vs. EmoAgent on the D-AIM task. (a) Existing approaches typically follow a fixed one-to-one mapping from emotion to visual output, leading to limited diversity. (b) In contrast, EmoAgent performs multi-path planning and editing to produce multiple semantically distinct yet emotionally consistent results for the same input and target emotion. editing operations, injecting the specified visual elements into the source image with step-wise precision. However, the reliability of AIM is not fully assured by these two agents alone. A third component, the Critic Agent, acting as the “eyes”, evaluates intermediate results and collaborates with the Editing Agent in an iterative loop to refine outputs during the optimization stage, ensuring that the target emotion is conveyed both accurately and coherently. Through this collaborative design, EmoAgent not only enhances emotional alignment but also supports emotion-aware visual diversity—capturing multiple valid interpretations of the same affective goal, as demonstrated in Fig. 1(b). Our contributions are summarized as follows:]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing Affective Image Manipulation (AIM) methods typically rely on rigid, one-to-one mappings between emotions and visual cues. This approach fails to capture the subjective and diverse nature of human emotional expression, where a single emotion (e.g., "awe") can be evoked by many different visual scenes (e.g., mountains, sunsets, stage lights). This limitation results in generated images that lack visual diversity for a given emotional target. To address this, the authors introduce a new task, Diverse AIM (D-AIM), which aims to generate multiple visually distinct yet emotionally consistent image edits from a single source image and target emotion.

## 2. Key Ideas and Methodology
The core of the paper is **EmoAgent**, a novel multi-agent framework designed for the D-AIM task. It decomposes the complex editing process into three specialized, collaborative agents:
1.  **Planning Agent:** This agent analyzes the input image and target emotion. Using a Vision-Language Model (VLM) and an external Emotion-Factor Knowledge (EFK) database, it generates multiple, diverse editing plans, each representing a unique strategy to convey the target emotion.
2.  **Editing Agent:** This agent acts as the executor, translating the abstract plans from the Planning Agent into concrete visual modifications. It uses a comprehensive library of specialized image editing tools to precisely execute each step of a plan.
3.  **Critic Agent:** This agent evaluates the emotional accuracy of the edited images. Using Chain-of-Thought (CoT) reasoning, it identifies mismatches between the intended and perceived emotion. For unsatisfactory results, it provides targeted feedback to the Editing Agent, initiating an iterative refinement loop until the output is emotionally faithful.

This collaborative, two-stage workflow (pre-creation and optimization) enables the exploration of a diverse solution space while ensuring the final outputs are emotionally accurate.

## 3. Datasets Used / Presented
The authors evaluate their framework using the test set from **EmoEditSet**, which contains **405 real-world images** with diverse content. For the Planning Agent, they construct an **Emotion-Factor Knowledge (EFK) database** by clustering images from the **EmoSet** dataset based on CLIP embeddings and generating descriptive text for each cluster using GPT-4o. This database provides the agent with a rich source of emotion-to-visual-element mappings.

## 4. Main Results
EmoAgent significantly outperforms existing state-of-the-art SIM and AIM methods in both emotional fidelity and visual diversity.
*   **Emotion Alignment:** In quantitative comparisons, EmoAgent achieved a 97.59% Emotional Shift Ratio (ESR), surpassing the previous SOTA, EmoEdit (89.64%).
*   **Visual Diversity:** EmoAgent demonstrated far greater diversity, achieving a LPIPS score of 0.488 and a Semantic Diversity (Sem-D) score of 0.180, more than double EmoEdit's scores of 0.205 and 0.086, respectively.
*   **Human Evaluation:** In a user study comparing EmoAgent to EmoEdit, participants overwhelmingly preferred EmoAgent's results for Emotion Accuracy (80.77% preference), Semantic Plausibility (76.92%), and Emotion-Aware Diversity (80.74%).

The authors conclude that their multi-agent approach effectively generates diverse, context-aware, and emotionally resonant image manipulations.

## 5. Ablation Studies
The authors performed systematic ablations to validate the contribution of each core component:
*   **w/o EFK Retriever:** Removing the knowledge retriever from the Planning Agent caused a significant drop in emotional accuracy (Emo-A score fell from 61% to 40%) and diversity (Sem-D fell from 0.180 to 0.161), as the agent struggled to generate relevant and varied editing plans.
*   **w/o Editing Tools Library:** Restricting the Editing Agent to a single tool severely limited its ability to perform precise manipulations. This resulted in weaker emotional alignment (Emo-A dropped from 61% to 35%) and produced lower-quality visual outputs.
*   **w/o Critic Agent:** Removing the Critic Agent led to the most substantial performance degradation, with the Emo-A score plummeting from 61% to 28%. This highlights the critic's essential role in iteratively refining results to ensure final emotional accuracy.

## 6. Paper Figures
![At the heart of EmoAgent is the Planning Agent, which reformulates the D-AIM task as a multi-solution generation problem . Instead of producing a single optimal edit, it aims to map the same target emotion E t to multiple semantically distinct yet emotionally consistent editing strategies . This diversity is achieved by transforming emotional cues into structured editing instructions via a combination of analysis, retrieval, and planning. Input-Output Schema. The Planning Agent takes as input a source image I o and a target emotion E t , and outputs a set of diverse editing plans P = { P (1) , P (2) , . . . , P ( k ) } . Each P ( k ) = { Ins ( k ) 1 , Ins ( k ) 2 , . . . } defines a coherent sequence of editing instructions, representing a distinct transformation path. Together, these plans offer multiple visually diverse solutions to express the same emotional intent. Emotion-Grounded Analysis and Retrieval. The Planning Agent begins by using a VLM (e.g., Qwen2.5-VL [35]) to extract semantic cues S o and the source emotion E o from the input image I o . These cues guide a Retrieval-Augmented Generation (RAG) [36] process to query an external Emotion-Factor Knowledge (EFK) database, constructed from EmoSet [34] and organized into emotion–element–method triples, as illustrated in Fig. 2(b). Unlike prior approaches that retrieve a single best match, we extract a diverse topk pool of editing]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_2.jpg' | relative_url }})
![Fig. 3. Three-layer decision space . The agent designates the target emotion as the root node, identifies appropriate editing elements as branch nodes, and assigns corresponding editing methods as leaf nodes.]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_3.jpg' | relative_url }})
![The Editing Agent serves as the executor within the EmoAgent framework—analogous to the ”hands” of an artist—responsible for translating abstract editing plans into concrete visual modifications. It leverages a comprehensive library of specialized editing tools, functioning as a diverse set of brushes, to faithfully implement each step of the editing process with precision and emotional alignment. Input-Output Schema. The Editing Agent takes the editing plan P = { P (1) , P (2) , . . . , P ( k ) } and the source image I o as input, producing emotionally consistent yet visually diverse edited images { I (1) t , I (2) t , . . . , I ( k ) t } aligned with the target emotion E t . Editing Tools Library. To support diverse visual transformations, the Editing Agent is equipped with an extensible library of tools categorized by input modality, as shown in Table I: (1) text-guided tools, (2) tools combining text and spatial masks, and (3) tools integrating spatial masks with reference images. Each category offers unique strengths for specific editing tasks, such as object insertion, background replacement, and fine-grained expression or attribute manipulation. To support precise, localized edits, we further incorporate auxiliary tools, including object detection [37] and segmentation models [38], which provide spatial priors and instance-level understanding. In addition, a VLM [35] is employed as a self-critic module to validate the semantic fidelity of the outputs. Consequently, the Editing Agent dynamically selects the most appropriate tool based on the semantics and complexity of each instruction, ensuring accurate and context-aware execution. Hierarchical Action Execution. To implement each editing plan P ( k ) , the Editing Agent executes a sequence of hierarchical actions that progressively refine the image. As illustrated in Fig. 4, each instruction Ins ( k ) n is handled through three structured stages:]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_4.jpg' | relative_url }})
![Fig. 5. Qualitative comparisons with the SIM and AIM baselines. EmoAgent more effectively conveys target emotions while preserving high-level semantic content, outperforming prior SIMand AIM-based methods.]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_5.jpg' | relative_url }})
![Fig. 6. Emotion-aware visual diversity from a single source image. Due to its one-to-one emotion-to-visual mapping, EmoEdit applies fixed editing patterns and struggles to produce diverse outputs. In contrast, EmoAgent introduces varied and context-aware visual elements to generate multiple emotionally consistent yet visually distinct edits from the same image. B. Evaluation Details]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_6.jpg' | relative_url }})
![Fig. 7. Emotion-aware visual diversity across varied source images. Compared to EmoEdit, which tends to reuse fixed emotional elements across different inputs, EmoAgent adapts its edits to each image’s semantics, introducing distinct affective cues and achieving both better emotional fidelity and greater visual diversity.]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_7.jpg' | relative_url }})
![Fig. 8. Ablation study on the EFK retriever. Removing the EFK retriever reduces both emotional accuracy and diversity. With it, the Planning Agent can retrieve and incorporate a wider range of contextually appropriate emotion-related elements into the editing plan.]({{ '/images/03-2025/EmoAgent:_A_Multi-Agent_Framework_for_Diverse_Affective_Image_Manipulation/figure_8.jpg' | relative_url }})
