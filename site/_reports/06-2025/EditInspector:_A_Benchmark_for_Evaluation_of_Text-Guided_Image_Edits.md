---
title: EditInspector:_A_Benchmark_for_Evaluation_of_Text-Guided_Image_Edits
layout: default
date: 2025-06-11
---
## EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits
**Authors:**
- Ron Yosef, h-index: 4, papers: 7, citations: 73
- Dani Lischinski, h-index: 3, papers: 6, citations: 426

**ArXiv URL:** http://arxiv.org/abs/2506.09988v1

**Citation Count:** None

**Published Date:** 2025-06-11

![Figure 1: The assessments for the edit “Let the floor be made of wood” vary across different models, with 2–3 models answering each question correctly. Gemini 1.5 failed to detect any differences between the images, while GPT-4o successfully identified only the main difference. See Appendix A.7 for full-size prompts.]({{ '/images/06-2025/EditInspector:_A_Benchmark_for_Evaluation_of_Text-Guided_Image_Edits/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The paper addresses the lack of a comprehensive framework for evaluating the quality of text-guided image edits. While generative AI has made image editing widely accessible, current evaluation methods are often limited to checking instruction adherence. They tend to overlook critical quality issues such as the introduction of unintended artifacts, poor visual quality, failure to integrate seamlessly with the scene, and deviations from user expectations or common sense. This gap highlights a need for a more holistic and human-aligned verification process.

## 2. Key Ideas and Methodology
The core idea is to introduce **EditInspector**, a novel benchmark for the multi-dimensional evaluation of text-guided image edits. The benchmark is built upon a detailed human evaluation framework that assesses edits across five key dimensions:
1.  **Accuracy:** Does the edit follow the instruction and meet user expectations?
2.  **Artifacts:** Are there unintended distortions or anomalies?
3.  **Technical Quality:** Is the edit technically precise (e.g., resolution, blur)?
4.  **Contextual Consistency:** Does the edit blend seamlessly with the image scene?
5.  **Difference Description:** Can the changes be accurately described?

The methodology involved collecting human annotations for edits from the MagicBrush dataset using this framework. The resulting benchmark was then used to evaluate the performance of state-of-the-art Vision-Language Models (VLMs) as automated "edit inspectors." The authors also propose two new methods—a zero-shot pipeline for difference captioning and an artifact detection method based on segmentation probabilities—and an efficient fine-tuned LLaVA model to address the identified shortcomings of existing VLMs.

## 3. Datasets Used / Presented
*   **EditInspector (Presented):** A new benchmark consisting of 783 edits from the MagicBrush test set, each annotated by three human workers according to the paper's comprehensive evaluation framework.
*   **MagicBrush (Used):** The source of the image edits. The test set was used to create the EditInspector benchmark, while the training set (8,808 edits) was used with data augmentation techniques to train the authors' fine-tuned model.
*   **UltraEdit & Imagen3 (Used):** 100 edits generated by each of these state-of-the-art editing models were annotated and used to test the robustness of VLM inspectors on a wider variety of edits.
*   **IER Dataset (Used):** A set of 120 human-authored edits used for out-of-distribution evaluation of the authors' fine-tuned model.

## 4. Main Results
*   State-of-the-art VLMs perform poorly as edit inspectors. For instance, in describing all differences in an edit, the best models (GPT-4o and Qwen2.5-VL) achieved a Model Precision of only 12% and a Hallucination Rate of around 60%.
*   Models struggle significantly with specific tasks. For example, GPT-4o was the best at identifying the main difference but was correct only 39% of the time. In contrast, the authors' proposed zero-shot captioning pipeline achieved 75% accuracy on the same task.
*   The authors' proposed artifact detection method achieved 64% balanced accuracy, competitive with the best VLM (GPT-4o at 65.7%), demonstrating its effectiveness.
*   The fine-tuned LLaVA model proved competitive with much larger models across several tasks, highlighting the efficacy of the proposed data augmentation and supervision strategy for creating efficient edit inspectors.

## 5. Ablation Studies
*   **Out-of-Distribution (OOD) Evaluation:** To test generalization, the authors evaluated their fine-tuned LLaVA model on the IER dataset, which has a different distribution from the MagicBrush training data. The fine-tuned model achieved a balanced accuracy of 59.2% on the Edit Accuracy task, outperforming the base LLaVA model's 54.2%. More importantly, it overcame the base model's strong bias toward positive predictions, showing it learned the task semantics better and could generalize to unfamiliar editing scenarios.

## 6. Paper Figures
![Figure 2: This is an example of our annotation user interface. The edit appears to be accurately executed but includes unexpected elements, such as differences in the door layers and a tilted star edge. There are mild artifacts, including a shadow behind the wall and a thick gray line beneath the star cutout. Clicking the tree icons opens decision trees that help annotators follow the evaluation guidelines (See Appendix A.16 ).]({{ '/images/06-2025/EditInspector:_A_Benchmark_for_Evaluation_of_Text-Guided_Image_Edits/figure_2.jpg' | relative_url }})
![Figure 4: Example of our pipeline generating an instruction-grounded difference caption with rich metadata. Edit images are split into three zoom levels, with Gemini extracting and prioritizing captions to generate the metadata.]({{ '/images/06-2025/EditInspector:_A_Benchmark_for_Evaluation_of_Text-Guided_Image_Edits/figure_4.jpg' | relative_url }})
![Figure 5: The first method for detecting artifacts using the Detic model for the edit “turn the stop sign to a lollipop”. Comparing Detic probabilities for objects intersecting the turquoise in-painting mask between the pre-edit (left) and post-edit (right) images reveals two artifacts, the truck and small car, whose probability drops exceeds our threshold.]({{ '/images/06-2025/EditInspector:_A_Benchmark_for_Evaluation_of_Text-Guided_Image_Edits/figure_5.jpg' | relative_url }})
