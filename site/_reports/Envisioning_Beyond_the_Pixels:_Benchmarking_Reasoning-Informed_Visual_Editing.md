## 1. Motivation of the Paper
The authors address a significant gap in the evaluation of Large Multi-modality Models (LMMs). While recent models show promising capabilities in visual editing, they struggle with tasks that require complex reasoning. Existing benchmarks do not adequately assess this sophisticated ability, which the authors term "Reasoning-Informed viSual Editing" (RISE). The practical problem is the lack of a standardized framework to systematically measure how well models can follow complex instructions, preserve image consistency, and generate plausible outputs based on temporal, causal, spatial, or logical reasoning, hindering progress in the field.

## 2. Key Ideas and Methodology
The core idea is the introduction of **RISEBench**, the first benchmark dedicated to evaluating reasoning-informed visual editing. The methodology is twofold: benchmark creation and a robust evaluation framework.

-   **Benchmark Creation:** The authors curated 360 high-quality test cases across four fundamental reasoning categories: **Temporal** (e.g., aging of objects), **Causal** (e.g., effect of an action), **Spatial** (e.g., changing viewpoints), and **Logical** (e.g., solving visual puzzles).
-   **Evaluation Framework:** To assess performance, they propose a multi-dimensional evaluation pipeline that measures three key aspects:
    1.  **Instruction Reasoning:** How accurately the model follows the textual command.
    2.  **Appearance Consistency:** How well the model preserves the original image's features that were not meant to be changed.
    3.  **Visual Plausibility:** The realism and physical correctness of the generated image.
    This evaluation is conducted using both human judges and a scalable **LMM-as-a-judge** approach to ensure reliability and efficiency.

## 3. Datasets Used / Presented
The paper introduces a new benchmark dataset:
-   **Name:** **RISEBench**
-   **Size and Domain:** It consists of 360 carefully annotated test cases, each with an input image and a complex instruction. The data is sourced from image generation models, 3D environments, existing datasets, and the internet.
-   **Distribution:** The dataset is divided into four reasoning categories:
    -   Temporal Reasoning: 85 samples
    -   Causal Reasoning: 90 samples
    -   Spatial Reasoning: 100 samples
    -   Logical Reasoning: 85 samples
-   **Usage:** It is used to systematically evaluate and compare the performance of eight prominent open-source and proprietary visual editing models.

## 4. Main Results
The evaluation reveals significant limitations in current state-of-the-art models.
-   The best-performing model, **GPT-4o-Image**, achieved an overall success rate of only **28.9%**.
-   Proprietary models like the Gemini-2.0-Flash series scored 13.3% and 9.4%, while most open-source models (e.g., BAGEL, Step1X-Edit, EMU2) performed near zero.
-   Even the top model, GPT-4o, struggled immensely with **Logical Reasoning**, achieving only a 10.6% accuracy in this category, highlighting it as a critical bottleneck for future research.
-   The authors conclude that RISEBench effectively exposes the shortcomings of current models in reasoning-aware visual editing and points toward necessary future advancements.

## 5. Ablation Studies
The paper does not perform traditional ablation studies on a proposed model. Instead, it conducts a crucial validation study to justify its **LMM-as-a-judge evaluation methodology**.

-   **Experiment:** The authors had six human experts score 100 random outputs from two models (GPT-4o and Gemini-2.0-Flash-Experimental). These human scores were then compared against the scores generated by the LMM-as-a-judge (GPT-4.1).
-   **Impact:** The study confirmed a strong correlation between the LMM judge and human experts. The Mean Absolute Error (MAE) on a 1-5 scale was consistently low across all three evaluation dimensions: 0.5 for Instruction Reasoning, 0.7 for Appearance Consistency, and 0.4 for Visual Plausibility. This result validates the reliability and effectiveness of using an LMM for automated, scalable evaluation in the RISEBench framework.