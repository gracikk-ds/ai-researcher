---
title: Envisioning_Beyond_the_Pixels:_Benchmarking_Reasoning-Informed_Visual_Editing
layout: default
---
![Figure 1: Examples of leading models on the Reasoning-Informed viSual Editing(RISE) benchmark. RISEBench contains complex and various tasks that pose challenges to current models.]({{ '/images/04-2025/Envisioning_Beyond_the_Pixels:_Benchmarking_Reasoning-Informed_Visual_Editing/figure_1.png' | relative_url }})

*Figure 1: Examples of leading models on the Reasoning-Informed viSual Editing(RISE) benchmark. RISEBench contains complex and various tasks that pose challenges to current models.*


## 1. Motivation of the Paper
The authors address a significant gap in the evaluation of Large Multi-modality Models (LMMs). While recent models show promising capabilities in visual editing, they struggle with tasks that require complex reasoning. Existing benchmarks do not adequately assess this sophisticated ability, which the authors term "Reasoning-Informed viSual Editing" (RISE). The practical problem is the lack of a standardized framework to systematically measure how well models can follow complex instructions, preserve image consistency, and generate plausible outputs based on temporal, causal, spatial, or logical reasoning, hindering progress in the field.

## 2. Key Ideas and Methodology
The core idea is the introduction of **RISEBench**, the first benchmark dedicated to evaluating reasoning-informed visual editing. The methodology is twofold: benchmark creation and a robust evaluation framework.

-   **Benchmark Creation:** The authors curated 360 high-quality test cases across four fundamental reasoning categories: **Temporal** (e.g., aging of objects), **Causal** (e.g., effect of an action), **Spatial** (e.g., changing viewpoints), and **Logical** (e.g., solving visual puzzles).
-   **Evaluation Framework:** To assess performance, they propose a multi-dimensional evaluation pipeline that measures three key aspects:
    1.  **Instruction Reasoning:** How accurately the model follows the textual command.
    2.  **Appearance Consistency:** How well the model preserves the original image's features that were not meant to be changed.
    3.  **Visual Plausibility:** The realism and physical correctness of the generated image.
    This evaluation is conducted using both human judges and a scalable **LMM-as-a-judge** approach to ensure reliability and efficiency.

## 3. Datasets Used / Presented
The paper introduces a new benchmark dataset:
-   **Name:** **RISEBench**
-   **Size and Domain:** It consists of 360 carefully annotated test cases, each with an input image and a complex instruction. The data is sourced from image generation models, 3D environments, existing datasets, and the internet.
-   **Distribution:** The dataset is divided into four reasoning categories:
    -   Temporal Reasoning: 85 samples
    -   Causal Reasoning: 90 samples
    -   Spatial Reasoning: 100 samples
    -   Logical Reasoning: 85 samples
-   **Usage:** It is used to systematically evaluate and compare the performance of eight prominent open-source and proprietary visual editing models.

## 4. Main Results
The evaluation reveals significant limitations in current state-of-the-art models.
-   The best-performing model, **GPT-4o-Image**, achieved an overall success rate of only **28.9%**.
-   Proprietary models like the Gemini-2.0-Flash series scored 13.3% and 9.4%, while most open-source models (e.g., BAGEL, Step1X-Edit, EMU2) performed near zero.
-   Even the top model, GPT-4o, struggled immensely with **Logical Reasoning**, achieving only a 10.6% accuracy in this category, highlighting it as a critical bottleneck for future research.
-   The authors conclude that RISEBench effectively exposes the shortcomings of current models in reasoning-aware visual editing and points toward necessary future advancements.

## 5. Ablation Studies
The paper does not perform traditional ablation studies on a proposed model. Instead, it conducts a crucial validation study to justify its **LMM-as-a-judge evaluation methodology**.

-   **Experiment:** The authors had six human experts score 100 random outputs from two models (GPT-4o and Gemini-2.0-Flash-Experimental). These human scores were then compared against the scores generated by the LMM-as-a-judge (GPT-4.1).
-   **Impact:** The study confirmed a strong correlation between the LMM judge and human experts. The Mean Absolute Error (MAE) on a 1-5 scale was consistently low across all three evaluation dimensions: 0.5 for Instruction Reasoning, 0.7 for Appearance Consistency, and 0.4 for Visual Plausibility. This result validates the reliability and effectiveness of using an LMM for automated, scalable evaluation in the RISEBench framework.

## 6. Paper Figures
![Figure 2: Overview of RISEBench. We present illustrative example questions from each of the four problem categories, each demanding profound image understanding and reasoning capabilities.]({{ '/images/04-2025/Envisioning_Beyond_the_Pixels:_Benchmarking_Reasoning-Informed_Visual_Editing/figure_2.png' | relative_url }})

*Figure 2: Overview of RISEBench. We present illustrative example questions from each of the four problem categories, each demanding profound image understanding and reasoning capabilities.*


![Figure 4: Evaluation metrics of RISEBench. RISEBench assesses the quality of generated images along three key dimensions: Instruction Following , Appearance Consistency , and Visual Plausibility . For each dimension, carefully crafted prompts are provided to the evaluator model (GPT-4.1 in this study), which analyzes various inputs and returns scores for each corresponding sub-dimension.]({{ '/images/04-2025/Envisioning_Beyond_the_Pixels:_Benchmarking_Reasoning-Informed_Visual_Editing/figure_4.png' | relative_url }})

*Figure 4: Evaluation metrics of RISEBench. RISEBench assesses the quality of generated images along three key dimensions: Instruction Following , Appearance Consistency , and Visual Plausibility . For each dimension, carefully crafted prompts are provided to the evaluator model (GPT-4.1 in this study), which analyzes various inputs and returns scores for each corresponding sub-dimension.*


![Figure 5: Comparison across models on three evaluation sub-dimensions. GPT-4o-Image demonstrates superior performance, achieving the highest scores across all three evaluation metrics. Gemini2-Flash-Series also exhibits competitive performance on these criteria. In contrast, the performance of many other evaluated models was considerably lower, indicating significant limitations in their ability to follow instructions and maintain visual integrity.]({{ '/images/04-2025/Envisioning_Beyond_the_Pixels:_Benchmarking_Reasoning-Informed_Visual_Editing/figure_5.png' | relative_url }})

*Figure 5: Comparison across models on three evaluation sub-dimensions. GPT-4o-Image demonstrates superior performance, achieving the highest scores across all three evaluation metrics. Gemini2-Flash-Series also exhibits competitive performance on these criteria. In contrast, the performance of many other evaluated models was considerably lower, indicating significant limitations in their ability to follow instructions and maintain visual integrity.*


![Figure 6: Examples of several different models’ outputs on RISEBench. The analyzed models demonstrate distinct characteristics in their responses. Specifically, GPT-4o exhibits instances of instruction misunderstanding, while Gemini sometimes struggles with maintaining image consistency. Other models generally show limited ability to comprehend and execute complex instructions.]({{ '/images/04-2025/Envisioning_Beyond_the_Pixels:_Benchmarking_Reasoning-Informed_Visual_Editing/figure_6.png' | relative_url }})

*Figure 6: Examples of several different models’ outputs on RISEBench. The analyzed models demonstrate distinct characteristics in their responses. Specifically, GPT-4o exhibits instances of instruction misunderstanding, while Gemini sometimes struggles with maintaining image consistency. Other models generally show limited ability to comprehend and execute complex instructions.*


