---
title: Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation
layout: default
date: 2024-12-17
---
![Figure 1. Text-guided image manipulation. Illustrative examples generated by our method (bottom row) with localised manipulations based on given text prompts and input images (top row).]({{ '/images/12-2024/Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address a key challenge in text-guided image editing: the difficulty of performing localized edits that transform image content according to a text prompt while simultaneously preserving the unedited parts of the image. Existing methods often struggle with this dual objective, require domain-specific training, or depend on manually provided or imperfectly generated masks at inference time, which limits their flexibility and performance. The paper aims to create a method that can learn to perform localized edits in a self-supervised manner, without needing paired datasets or inference-time masks.

## 2. Key Ideas and Methodology
The core idea is **Prompt Augmentation**, a self-supervised technique to guide localized image editing. The methodology involves three main steps:

1.  **Prompt Augmentation:** A single input caption for an image is first cleaned using a captioning model (BLIP). Then, a masked language model (BERT) and semantic word relations (NLTK) are used to generate multiple diverse but related target prompts (e.g., "a black car" is augmented to "a blue car," "a red car," etc.).

2.  **On-the-Fly Mask Generation:** During training, the differences in noise predictions from a Latent Diffusion Model (LDM) for these various augmented prompts are calculated. This difference map highlights regions susceptible to change, which is then thresholded to create a dynamic attention mask that localizes the intended edit area.

3.  **Soft Contrastive Loss (Soft-CL):** A novel loss function is introduced to train the model. It has two components guided by the generated mask:
    *   A dissimilarity term pushes the masked (edited) regions of generated images apart, encouraging them to align with their specific target prompts.
    *   A preservation term pulls the unmasked (background) regions of the images closer together, ensuring context is preserved.
    The "soft" aspect weights the loss based on the semantic similarity between prompts (via CLIP embeddings), allowing for more nuanced adjustments for closely related edits.

## 3. Datasets Used / Presented
*   **Training:** The model was trained on a subset of the **LAION-5B** dataset, specifically using images with an aesthetics score greater than 7.
*   **Evaluation:** Performance was evaluated on a custom set of 135 images, which included real images from **COCO** and **ImageNet**, as well as synthetic images generated by Stable Diffusion.

## 4. Main Results
The proposed method was compared against state-of-the-art baselines like SDEdit, DiffEdit, DALL-E 2, and InstructPix2Pix.

*   **Quantitative Results:** The method achieves a competitive CLIPScore of 78.19% (measuring faithfulness to the target prompt) and an SSIM-M score of 70.39% (measuring background preservation). While DALL-E 2 achieves higher background preservation (SSIM-M 96.74%), it does so with manual masks.
*   **Human Evaluation:** In a user study, the proposed method was preferred over all baselines, including DALL-E 2 and InstructPix2Pix. This suggests it strikes a better balance between successful editing and high-fidelity context preservation than captured by automated metrics alone.

The authors claim their method is a significant contribution towards enabling localized image editing by successfully striking a nuanced balance between translation and preservation without relying on paired data or inference-time masks.

## 5. Ablation Studies
The authors performed ablation studies to validate the contribution of each key component of their method.

*   **Baseline vs. Contrastive Loss (CL):** Adding the standard Contrastive Loss to the baseline model (SDEdit) significantly improved all metrics. CLIPScore increased from 76.76% to 77.94%, and SSIM-M (background preservation) rose from 63.07% to 68.37%, demonstrating the effectiveness of the core contrastive learning framework.
*   **Full Model (Soft-CL + Soft Prompt Augmentation):** The full model, incorporating the Soft Contrastive Loss and a soft prompt augmentation strategy (which selects a more diverse set of prompts), achieved the best overall performance (CLIPScore: 78.19%, FID: 133, SSIM-M: 70.39%).
*   **Qualitative Impact:** Qualitative results showed that the baseline model struggled with semantic nuances (e.g., generating similar images for "orange cake" and "mandarin cake"). The Soft-CL component enabled the model to discern these fine-grained differences, and the soft prompt augmentation amplified this effect, leading to more contextually rich and accurate edits.

## 6. Paper Figures
![Figure 2. Overview of our method. (a) Prompt Augmentation: In order to augment the prompts to facilitate localised image editing we start by refining textual descriptions for source images using the BLIP captioning model [ 22 ], resulting in cleaner captions suitable for further processing. Subsequently, we augment this input prompt by generating a range of target prompts using masked language modeling and exploiting word relations. (b) Soft Contrastive Loss (Soft-CL): The augmented prompts are instrumental in computing an attention mask based on the differences between the generated images. This attention mask is used to bring the inverse masked areas of the generated images closer while pushing away masked areas considering their similarity to the prompts.]({{ '/images/12-2024/Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation/figure_2.jpg' | relative_url }})
![Figure 3. Qualitative comparison of our method against SDEdit [ 27 ], DALL-E 2 [ 31 ], DiffEdit [ 11 ] and InstructPixtoPix [ 8 ] using both generated and real images.]({{ '/images/12-2024/Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation/figure_3.jpg' | relative_url }})
![Figure 4. Qualitative comparison of ablation study results.]({{ '/images/12-2024/Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation/figure_4.jpg' | relative_url }})
