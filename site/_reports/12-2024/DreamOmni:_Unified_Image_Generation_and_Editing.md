---
title: DreamOmni:_Unified_Image_Generation_and_Editing
layout: default
date: 2024-12-22
---
![Figure 1. The gallery of DreamOmni. DreamOmni, as a native unified image generation and editing model, can handle various tasks.]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Current text-to-image (T2I) foundation models, while powerful, are not designed for easy integration with downstream tasks like image editing. Adapting them often requires specialized, task-specific plugins (e.g., ControlNet, IP-adapter) or complex framework modifications. This fragmented approach complicates deployment and prevents models from benefiting from synergistic multi-task training. Furthermore, creating large-scale, high-quality, and accurately paired data for editing tasks (like instruction-based or drag-based editing) is a major bottleneck, being both difficult and inefficient.

## 2. Key Ideas and Methodology
The authors introduce DreamOmni, a single, unified model for both image generation and a wide range of editing tasks.

-   **Unified Framework:** The core architecture is a Diffusion Transformer (DiT). It replaces the standard text encoder with a Vision-Language Model (VLM) to natively process both text and image prompts. VLM features are concatenated with the noisy image latent and processed jointly, allowing the model to learn complex, multi-modal relationships. The design incorporates UNet-style long connections for faster training convergence while concentrating computations on higher-resolution latents for improved efficiency.

-   **Synthetic Data Pipeline:** To overcome the data bottleneck, the paper proposes a novel "collage-based" synthetic data pipeline. This method efficiently generates billions of accurate, paired data examples for diverse tasks, including instruction-based editing (add, remove, replace), drag editing (translate, scale, rotate), inpainting, reference-based generation, and more, by programmatically composing sticker-like elements on a canvas.

-   **Joint Training:** DreamOmni is trained jointly on standard T2I data and the synthetic multi-task data. This approach allows T2I training to enhance the model's understanding of concepts, while the editing data teaches it to follow nuanced instructions, leading to significantly improved editing performance.

## 3. Datasets Used / Presented
-   **T2I Training Data:** A total of 125M images, consisting of the 103M-image LAION dataset and 22M privately collected images.
-   **Synthetic Training Data:** A large-scale dataset generated by the proposed pipeline. This includes ~60M images for tasks like T2I attribute enhancement, instruction editing, inpainting, drag editing, and reference generation (12M per task), plus an additional 8M images for segmentation and detection.
-   **Evaluation Benchmarks:** Performance was measured on the standard **GenEval** benchmark for T2I capabilities. Comparisons for editing tasks were made against methods like **InstructP2P** and **MGIE** on their respective datasets, as well as on custom high-quality datasets for inpainting and drag editing.

## 4. Main Results
-   **T2I Generation:** On the GenEval benchmark, DreamOmni achieves a state-of-the-art overall score of 0.70, matching the performance of the larger SD3-Medium model and significantly outperforming models like SDXL (0.55).
-   **Image Editing:** In quantitative tests for inpainting, DreamOmni achieved a FID score of 0.84, substantially better than SD-inpainting (1.35) and ControlNet-inpainting (1.84).
-   **Qualitative Performance:** Visual comparisons across all tasks—including instruction editing, subject-driven generation, and drag editing—show that DreamOmni produces more accurate, coherent, and visually pleasing results than specialized, state-of-the-art editing models.

## 5. Ablation Studies
The authors performed an ablation study on the model framework by comparing several 0.85B parameter model variants under identical training conditions.

-   **Experiment 1 (UNet Connections):** Models with UNet-style long connections (DreamOmni-V2/V3, SDXL) were compared to models without them (DreamOmni-V1, SD3-Medium).
    -   **Impact:** The inclusion of long connections resulted in significantly faster training convergence. The final model, DreamOmni-V3, converged approximately four times faster than the SD3-Medium baseline.

-   **Experiment 2 (Computational Allocation):** DreamOmni-V3, which focuses DiT computations on the 2x downsampled latent, was compared to DreamOmni-V2, which distributes them across 2x and 4x latents.
    -   **Impact:** Concentrating computations on the higher-resolution (2x) latent was found to be more cost-effective and yielded better performance trade-offs.

## 6. Paper Figures
![Figure 2. The overview of DreamOmni. (a) The DreamOmni framework supports unified image generation and editing, with fast training convergence and powerful performance. (b) To overcome the difficulty and inefficiency in data creation and filtering for image editing, we propose a collage-based synthetic data pipeline. This pipeline enables the efficient creation of data for various editing tasks, such as adding, deleting, and replacement operations in instruction-based editing, as well as translation, scaling, and rotation in drag editing. Additionally, it supports reference image generation and segmentation & detection. Furthermore, our synthetic data pipeline enhances the accuracy of T2I generation, particularly for attributes related to text, geometry, color, position, and quantity. Due to space limitations, we have optionally shown the corresponding prompts or instructions for these cases.]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_2.jpg' | relative_url }})
![Figure 4. Visual comparison on T2I generation . Compared to other competitive methods (including SD3-Medium [ 15 ], SDXL [ 39 ], SD-Cascade, and SD1.5 [ 44 ]), our DreamOmni not only better adheres to user prompts but also generate more visually appealing results with delicate details, elegant composition, and so on.]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_4.jpg' | relative_url }})
![Figure 5. Visual comparison on inpainting & outpainting between DreamOmni, ControlNet-Inpainting [ 62 ] and SD-inpainting [ 44 ].]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_5.jpg' | relative_url }})
![Figure 6. Visual comparison on image-conditioned generation . Our DreamOmni excels at using canny, depth, and segmentation maps as conditions for image generation. Compared to the classic ControlNet [ 62 ], DreamOmni not only follows the user’s prompts and image conditions more accurately but also generates content and color schemes that are visually more pleasing.]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_6.jpg' | relative_url }})
![Figure 7. Visual comparison on subject-driven generation . Compared with BLIP-Diffusion [ 28 ] and IP-adapter [ 60 ], DreamOmni excels at accurately following user prompts while preserving the specified subject. Furthermore, its success across both photo and anime cases underscores DreamOmni’s generalization capabilities.]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_7.jpg' | relative_url }})
![Figure 8. Visual comparison on instruction-based editing . Our DreamOmni achieves more precise editing (including addition, removal, and replacement) compared to competitive methods such as MGIE [ 17 ] and InstructP2P [ 8 ].]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_8.jpg' | relative_url }})
![Figure 9. Visual results of drag editing : DreamOmni accurately performs translation, rotation, and scaling edits.]({{ '/images/12-2024/DreamOmni:_Unified_Image_Generation_and_Editing/figure_9.jpg' | relative_url }})
