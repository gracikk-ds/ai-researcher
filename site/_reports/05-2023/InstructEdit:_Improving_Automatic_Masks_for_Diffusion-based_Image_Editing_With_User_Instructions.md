---
title: InstructEdit:_Improving_Automatic_Masks_for_Diffusion-based_Image_Editing_With_User_Instructions
layout: default
date: 2023-05-29
---
![Figure 1: Left: a comparison between DiffEdit and InstructEdit. Right: examples of editing using InstructEdit. Note that InstructEdit only requires user instructions as input, while DiffEdit needs an input caption and an edited caption instead.]({{ '/images/05-2023/InstructEdit:_Improving_Automatic_Masks_for_Diffusion-based_Image_Editing_With_User_Instructions/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing text-guided image editing models, particularly those based on diffusion, struggle to accurately locate the specific regions to be edited and perform precise, fine-grained changes based on user instructions. This is especially challenging in images with multiple objects, where models may edit the wrong object or fail to preserve the background. Methods that automatically generate masks, like DiffEdit, often produce low-quality masks when captions are not sufficiently descriptive, leading to poor editing results. The paper aims to solve this problem by creating a framework that can robustly interpret user instructions and generate high-quality masks to guide fine-grained edits, even in complex scenes.

## 2. Key Ideas and Methodology
The core idea of the paper is to decompose the editing task into three distinct stages, each handled by a powerful, pre-trained model, requiring no additional training. The proposed framework, **InstructEdit**, operates as follows:

1.  **Language Processor:** It first uses a large language model (ChatGPT) to parse the user's natural language instruction. This process extracts a specific *segmentation prompt* (the object to be masked, e.g., "the cat on the right") and generates corresponding *input* and *edited captions* for the diffusion model. To handle ambiguous instructions (e.g., "turn it into a tiger"), it optionally uses a vision-language model (BLIP-2) to provide a description of the input image, giving the language model necessary context.

2.  **Segmenter:** Using the segmentation prompt from the first step, this component employs Grounded Segment Anything (a combination of GroundedDINO and SAM) to generate a precise, high-quality binary mask of the target object(s) in the image.

3.  **Image Editor:** Finally, the framework uses a mask-guided diffusion process (adopting the technique from DiffEdit with a Stable Diffusion backbone). The edit is performed using the input and edited captions but is constrained to only the region defined by the high-quality mask from the segmenter, ensuring other parts of the image remain unchanged.

## 3. Datasets Used / Presented
The paper does not present or use any new datasets for training, as the proposed method is tuning-free. The evaluation was performed on a curated set of challenging image editing examples, shown in the paper's figures, to qualitatively and quantitatively compare against baseline methods. A user study was conducted on these examples.

## 4. Main Results
InstructEdit demonstrated superior performance in fine-grained editing tasks compared to baselines like MDP-et, InstructPix2Pix, and DiffEdit.

*   **Quantitative Metrics:** On a set of 10 test cases, InstructEdit achieved the best (lowest) LPIPS score of 0.121, indicating better preservation of the original image content outside the edited region. It also achieved the highest CLIP score of 27.404, showing the best semantic alignment between the edited image and the text instruction.
*   **User Study:** A user study with 26 participants showed a strong preference for InstructEdit's results. It was preferred over MDP-et, InstructPix2Pix, and DiffEdit in 83.0%, 83.0%, and 84.5% of cases, respectively.

The authors claim that by improving mask quality, their method achieves more accurate and faithful edits than prior work, especially in complex multi-object scenarios.

## 5. Ablation Studies
The paper presents several analyses to validate its design choices:

1.  **Benefit of BLIP-2:** An ablation showed how adding BLIP-2 to the language processor improves results for ambiguous instructions. When an instruction like "Turn it into an alien" was given for an image of the "Girl with a Pearl Earring," the model without BLIP-2 failed to generate meaningful captions. With BLIP-2 providing context, ChatGPT correctly identified the object and generated appropriate prompts, leading to a successful edit.

2.  **Mask Quality Comparison:** The paper directly compares the masks generated by InstructEdit (via Grounded Segment Anything) with those from the baseline DiffEdit. The results show that DiffEdit's masks are often inaccurate, incomplete, or incorrectly located, and are highly sensitive to a threshold parameter. In contrast, InstructEdit consistently produces precise masks that accurately outline the intended object, which is the key reason for its superior editing quality.

## 6. Paper Figures
![Figure 2: Pipeline: given a user instruction, a language processor first parses the instruction into a segmentation prompt , an input caption , and an edited caption . A segmenter then generates a mask based on the segmentation prompt. The mask along with the input and edited captions are then going to an image editor to produce the final output.]({{ '/images/05-2023/InstructEdit:_Improving_Automatic_Masks_for_Diffusion-based_Image_Editing_With_User_Instructions/figure_2.jpg' | relative_url }})
![Figure 3: Comparison of the masks (colored in red and blended with the input image) and the corresponding edited image (below each mask) generated by DiffEdit and InstructEdit.]({{ '/images/05-2023/InstructEdit:_Improving_Automatic_Masks_for_Diffusion-based_Image_Editing_With_User_Instructions/figure_3.jpg' | relative_url }})
![Figure 4: Qualitative results of baselines and our method. Here we use the same form of instruction “Change ... to ...” as an example.]({{ '/images/05-2023/InstructEdit:_Improving_Automatic_Masks_for_Diffusion-based_Image_Editing_With_User_Instructions/figure_4.jpg' | relative_url }})
![Figure 5: Examples of how BLIP2 improves the quality of edited images by improving the generated prompts. The three generated prompts are segmentation prompt , input caption and edited caption , respectively. For the first example the BLIP2 description of the image is “Photo of a dog and a cat”. We show three examples for w./wo. BLIP2 with increasing encoding ratio r from left to right. In the second example the BLIP2 description is “Painting of a girl with a pearl earring by Jan Van Gogh”. (BLIP2 gets the name of the painting correct but the name of the artist wrong.)]({{ '/images/05-2023/InstructEdit:_Improving_Automatic_Masks_for_Diffusion-based_Image_Editing_With_User_Instructions/figure_5.jpg' | relative_url }})
![Figure 6: Examples of the variety of the input user instructions. Under each edited image is the input instruction.]({{ '/images/05-2023/InstructEdit:_Improving_Automatic_Masks_for_Diffusion-based_Image_Editing_With_User_Instructions/figure_6.jpg' | relative_url }})
