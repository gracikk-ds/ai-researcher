---
title: Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation
layout: default
date: 2023-05-25
---
![Figure 1: Overview. Example augmentations using text-to-images generation, traditional data augmentation methods, and our method, Automated Language-guided Image Editing (ALIA) on CUB [ 41 ]. Images generated by ALIA retain task-relevant information while providing more domain diversity as specified by the prompts. Within the prompts, { } indicates the specific class name.]({{ '/images/05-2023/Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the problem of poor generalization in classifiers trained on specialized, fine-grained datasets (e.g., rare animal identification). These datasets often have limited data and lack diversity in domains such as weather, location, or background context. Consequently, models trained on them fail when encountering these variations in the real world. Existing data augmentation methods are either too simple (e.g., random cropping), require expensive model fine-tuning, or generate new images from text that lack visual grounding in the original data, proving ineffective for specialized classes that diffusion models cannot easily recreate from text alone.

## 2. Key Ideas and Methodology
The paper introduces **ALIA (Automated Language-guided Image Augmentation)**, a method to automatically diversify training datasets using large vision and language models.

-   **Core Idea**: Instead of generating images from scratch, ALIA uses language as a bridge to discover underrepresented domains within a dataset and then edits the original images to populate these domains, preserving class-relevant features while increasing domain diversity.
-   **Methodology**: The process involves three main stages:
    1.  **Generate Domain Descriptions**: A pretrained captioning model (BLIP) generates captions for all training images. A large language model (LLM, specifically GPT-4) then summarizes these captions into a concise list of unique, class-agnostic domain descriptions (e.g., "in a grassy field with trees," "near a large body of water").
    2.  **Generate Image Edits**: These domain descriptions are used as text prompts to perform language-guided image editing on the original training images using a pretrained diffusion model (e.g., Stable Diffusion via Img2Img or InstructPix2Pix).
    3.  **Filter Poor Edits**: To ensure data quality, a two-step filtering process removes failed augmentations. First, a CLIP-based semantic filter discards obvious failures. Second, a confidence-based filter uses a classifier trained on the original data to remove edits that are either too similar to the original image or have corrupted the class-specific information.

## 3. Datasets Used / Presented
The method was evaluated on three distinct classification tasks and datasets:
-   **iWildCam**: A subset of the iWildCam dataset was used for a domain generalization task (7-way animal classification). The goal is to generalize to new camera trap locations not seen during training.
-   **CUB-200-2011 (CUB)**: A standard benchmark for fine-grained bird classification, used to demonstrate ALIA's utility even in scenarios without explicit domain shifts.
-   **Waterbirds**: A synthetic dataset designed to measure robustness to contextual bias. The training set has a spurious correlation (e.g., landbirds on land, waterbirds on water), while the test set is balanced.

## 4. Main Results
ALIA demonstrated significant performance improvements over traditional augmentation, text-to-image generation, and in some cases, even adding real out-of-domain data.
-   On **iWildCam**, adding ALIA-generated data improved accuracy from a 67.5% baseline to 84.9%, a 17.4% absolute improvement that surpassed all other methods, including adding real test data (+Real, 84.1%).
-   On **Waterbirds**, ALIA achieved a class-balanced accuracy of 71.4%, drastically improving upon the baseline (62.2%) and other augmentation methods by mitigating contextual bias and boosting out-of-domain accuracy.
-   On **CUB**, ALIA improved accuracy from 70.6% to 72.7%, outperforming most baselines and showing its value for general dataset diversification.

The authors claim that ALIA is an effective strategy for improving dataset diversity by grounding augmentations in the original training data.

## 5. Ablation Studies
-   **Prompt Quality**: ALIA's automatically generated prompts outperformed user-engineered prompts across all datasets, leading to an accuracy gain of up to 6.6% on Waterbirds, showing the effectiveness of automated domain discovery.
-   **Filtering**: The semantic and confidence-based filtering steps were crucial for performance, improving accuracy by 1-2% across all datasets by removing low-quality or corrupting edits. For example, on iWildCam, accuracy increased from 82.6% to 84.9% after filtering.
-   **Choice of Image Editing Method**: The choice of editing technique significantly impacted results. On iWildCam, using Img2Img yielded 84.9% accuracy, while InstructPix2Pix only achieved 76.2%, highlighting the need to select a method appropriate for the dataset's characteristics.
-   **Amount of Augmented Data**: On CUB, ALIA provided accuracy gains when adding up to 20% more data (1000 images), after which performance began to decline. In contrast, adding data from text-to-image generation hurt performance almost immediately.

## 6. Paper Figures
![Figure 2: ALIA. Given a specialized dataset, we caption all images in our dataset using a pretrained captioning model, and feed these captions into a large language model to summarize them into a small (<10) set of natural language descriptions. Utilizing these descriptions, we perform text-guided image augmentation via a pretrained text-to-image model, thereby generating training images that align with the described settings. Finally, we apply two filtering techniques: a CLIP-based semantic filter to eliminate obvious edit failures, and a confidence-based filter that removes more subtle failures by filtering edits confidently predicted by a classifier trained on the original dataset.]({{ '/images/05-2023/Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation/figure_2.jpg' | relative_url }})
![Figure 3: Filtering. Semantic filtering eliminates instances of total failure (left), but often misses instances of minimal edits (center) or edits which corrupt class information (right). Meanwhile, our confidence-based filtering mechanism is able to remove these failures by leveraging the prediction confidence of classifier trained on the original dataset applied to the image edits.]({{ '/images/05-2023/Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation/figure_3.jpg' | relative_url }})
![Figure 4: iWildCam Subset. Left plot visualizes the original training images and the corresponding data generated from ALIA based on the generated prompts. Right plot shows accuracy for adding in generated data from ALIA and baselines as in Section 4.2. ALIA significantly outperforms all the baselines including adding in real data from the test distribution, while +Txt2Img and +RandAug see smaller improvements.]({{ '/images/05-2023/Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation/figure_4.jpg' | relative_url }})
![Figure 5: Bird Classification (CUB). Left plot visualizes the original training images and the corresponding data generated from ALIA based on the generated prompts. Right plot shows accuracy for ALIA and baselines. ALIA outperforms all the baselines aside from RandAug and adding in real data, while adding text-to-image generated data results in similar performance with the baseline.]({{ '/images/05-2023/Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation/figure_5.jpg' | relative_url }})
![Figure 6: Waterbirds Results. Left plot shows class-balanced accuracy for ALIA and baselines. Right plot shows the breakdown of in-domain (waterbirds on water, landbirds on land) and outof-domain performance (waterbirds on land, landbirds on water). ALIA is able to roughly match in-domain accuracy of the other methods while drastically improving out-of-domain accuracy of all other augmentation methods. This results in 7% improvement for class-balanced accuracy when compared to other augmentation baselines.]({{ '/images/05-2023/Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation/figure_6.jpg' | relative_url }})
![Figure 7: InstructPix2Pix edits on iWildCam. Despite offering more color variation, the method often imparts an artificial airbrush-like effect or removes substantial original image details.]({{ '/images/05-2023/Diversify_Your_Vision_Datasets_with_Automatic_Diffusion-Based_Augmentation/figure_7.jpg' | relative_url }})
