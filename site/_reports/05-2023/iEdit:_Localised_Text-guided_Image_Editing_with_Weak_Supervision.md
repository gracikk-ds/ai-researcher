---
title: iEdit:_Localised_Text-guided_Image_Editing_with_Weak_Supervision
layout: default
date: 2023-05-10
---
## iEdit: Localised Text-guided Image Editing with Weak Supervision
**Authors:**
- Rumeysa Bodur, h-index: 4, papers: 7, citations: 58
- Loris Bazzani, h-index: 27, papers: 54, citations: 5272

**ArXiv URL:** http://arxiv.org/abs/2305.05947v1

**Citation Count:** None

**Published Date:** 2023-05-10

![Figure 1: Examples showing that our method can edit images with a textual prompt while preserving image ﬁdelity in the regions not related to the edit.]({{ '/images/05-2023/iEdit:_Localised_Text-guided_Image_Editing_with_Weak_Supervision/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the limited controllability of large-scale text-to-image diffusion models. Existing image editing methods often struggle to perform *localised* edits, frequently altering the entire image instead of just the target region. Furthermore, these methods either require computationally expensive test-time fine-tuning for each new subject or rely on fully annotated datasets of source images, edit prompts, and target images, which are scarce and costly to create. The paper aims to bridge this gap by developing a framework that can perform localised, text-guided editing while preserving the fidelity of the unedited regions, without needing a fully supervised dataset.

## 2. Key Ideas and Methodology
The core idea is a weakly-supervised learning method, named **iEdit**, that fine-tunes a pre-trained Latent Diffusion Model (LDM) for editing. The key contributions are:

*   **Automatic Paired Dataset Construction:** The authors propose a method to automatically create a large-scale dataset of (source image, edit prompt, pseudo-target image) triplets. Starting with image-caption pairs from LAION-5B, they use BLIP to generate a clean source caption, programmatically modify it to create an edit prompt (e.g., changing "cat" to "dog"), and then retrieve a pseudo-target image from LAION-5B that best matches the CLIP embeddings of the source image and the new prompt.
*   **Mask-Guided Localised Editing:** To ensure edits are localised, the model uses segmentation masks (generated by CLIPSeg) during training. A novel loss function is introduced that separates the optimization process: the foreground (masked) region is guided to match the pseudo-target, while the background (unmasked) region is guided to preserve the source image. This encourages high fidelity in areas unrelated to the edit.
*   **Inference:** At inference time, masks can be used to composite the denoised edited region with the noisy unedited region at each step, further preserving background details.

## 3. Datasets Used / Presented
*   **Training Dataset:** The authors constructed a new dataset of ~200,000 image-prompt pairs derived from **LAION-5B**. This dataset was used to fine-tune the iEdit model.
*   **Evaluation Datasets:**
    *   **Generated Images:** A set of 70 image-prompt pairs, using 20 distinct images generated by an LDM.
    *   **Real Images:** A set of 52 image-prompt pairs, using 30 distinct real images from **COCO**, **ImageNet**, and **AFHQ**.

## 4. Main Results
iEdit was compared against SDEdit, DALL-E 2, and InstructPix2Pix.

*   **Quantitative:** On both generated and real images, iEdit (especially `iEdit-M`, which uses masks at inference) achieves the highest CLIPScore (66.36% on generated, 67.44% on real), indicating the best alignment between the final image and the textual edit prompt. It also demonstrates strong performance in preserving the background (SSIM-M score of 80.44% on real images), second only to DALL-E 2 which requires manually provided ground-truth masks.
*   **Qualitative:** Visual examples show that iEdit successfully performs targeted edits (e.g., changing a car's model, adding strawberries to a cake) while maintaining the original image's background and style. Competing methods often change the entire image, fail the edit, or produce unrealistic results.
*   **Author-claimed Impact:** The paper demonstrates that iEdit achieves a superior balance between fidelity to the source image and alignment with the edit prompt, outperforming its counterparts on both generated and real images.

## 5. Ablation Studies
The authors performed several ablation studies to validate their design choices:

*   **Dataset Construction:** Using the automatically generated edit prompts (`LAION-edit-200K`) yielded better results (higher CLIPScore and SSIM-M) than using the original, noisy LAION-5B captions.
*   **Loss Functions:** Introducing the proposed mask-guided loss (`Lmask`) instead of a standard LDM loss provided a significant improvement in both CLIPScore and FID, confirming its effectiveness for localised editing. Adding a perceptual loss (`Lperc`) gave a marginal additional boost.
*   **Inference with Masks:** Applying masks during the inference process gave the most substantial performance gain across all metrics. For instance, it improved the CLIPScore from 66.09% to 66.97% and reduced the FID from 146 to 128, highlighting the benefit of explicitly guiding the model to preserve unedited regions at test time.

## 6. Paper Figures
![Figure 2: Paired dataset construction. To construct a paired dataset for training, we generate captions with BLIP. Then, captions are manipulated by replacing nouns or adjectives with antonyms or co-hyponyms. We assign the pseudo-target image by using CLIP embeddings of the edit prompt and the source image to retrieve nearest neighbours.]({{ '/images/05-2023/iEdit:_Localised_Text-guided_Image_Editing_with_Weak_Supervision/figure_2.jpg' | relative_url }})
![Figure 3: Proposed image editing framework. Our framework takes as input the image pairs from the constructed dataset, their corresponding masks and the edit prompt. We optimise ϵ θ to predict the noises ϵ 1 and ϵ 2 for background and foreground, respectively. ⊙ denotes element-wise multiplication of the image and mask.]({{ '/images/05-2023/iEdit:_Localised_Text-guided_Image_Editing_with_Weak_Supervision/figure_3.jpg' | relative_url }})
![Figure 4: Comparison to state-of-the-art on generated images. Our method produces results with higher ﬁdelity to the source image and the edit prompt compared to SDEdit [ 26 ], DALL-E 2 [ 30 ] and InstructPix2Pix [ 4 ].]({{ '/images/05-2023/iEdit:_Localised_Text-guided_Image_Editing_with_Weak_Supervision/figure_4.jpg' | relative_url }})
![Figure 5: Qualitative Results on Real Images. iEdit outperforms compared methods showing high ﬁdelity to the edit prompt and the input image.]({{ '/images/05-2023/iEdit:_Localised_Text-guided_Image_Editing_with_Weak_Supervision/figure_5.jpg' | relative_url }})
