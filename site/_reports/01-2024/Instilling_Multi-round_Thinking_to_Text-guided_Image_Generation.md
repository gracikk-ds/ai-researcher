---
title: Instilling_Multi-round_Thinking_to_Text-guided_Image_Generation
layout: default
date: 2024-01-16
---
![Fig. 1: (a) A typical use case of multi-round interactive editing. The learned model can understand text instruction and the semantic meaning of images and craft images based on previous user feedback. This real-world scenario often involves multi-round generation rather than single-round generation. (b) Some common failure cases on the prevailing methods [ 46 ], i.e ., long sentence ignorance case, multi-facet forgetting case. We could observe the significant visual difference between generated images and ground-truth targets. (c) Here we show a typical two-round inconsistency case. The final generated results are sensitive to the order of text guidance.]({{ '/images/01-2024/Instilling_Multi-round_Thinking_to_Text-guided_Image_Generation/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing text-guided image editing models are primarily designed for single-round generation. When used in multi-round, interactive scenarios (e.g., a virtual shopping assistant), they face a persistent challenge: minor errors in capturing fine-grained details (like sleeves or patterns) accumulate with each modification. This leads to a significant drop in quality and a failure to follow complex instructions. The authors observe that the final edited image is often sensitive to the order in which textual modifications are applied, whereas ideally, the order should not affect the outcome. The paper aims to address this gap by instilling "multi-round thinking" into models to improve consistency and fidelity in sequential editing tasks.

## 2. Key Ideas and Methodology
The core idea is a self-supervised regularization method called **multi-round regularization**, which is built on the principle that the order of modifications should be invariant to the final result.

The methodology involves:
- **Splitting Instructions:** A single, multifaceted text instruction (e.g., "make it blue and add ruffles") is decomposed into a sequence of simpler sub-instructions (e.g., 1. "make it blue", 2. "add ruffles").
- **Sequential Generation:** The model first generates an intermediate image by applying the first sub-instruction to the reference image. It then applies the second sub-instruction to this intermediate image to produce the final output.
- **Consistency Loss (`L_multi`):** A loss function is applied to the final generated image, penalizing deviations from the ground-truth target image. By backpropagating through both generation steps, this loss encourages the model to maintain consistency and accurately apply changes sequentially. This process amplifies initially minor errors, forcing the model to learn fine-grained details more effectively.
- **Architectural Foundation:** The approach is model-agnostic and is implemented on top of a pre-trained Stable Diffusion model, using a ControlNet-like architecture for pose conditioning and LoRA layers for efficient fine-tuning. The total loss combines the multi-round loss with standard single-round generation and self-reconstruction losses for stable training.

## 3. Datasets Used / Presented
The method was evaluated on two multi-modal fashion retrieval datasets, where each sample is a triplet of (reference image, text modification, target image).
- **FashionIQ:** Contains over 17,500 training queries and 5,500 evaluation queries across three categories (dresses, shirts, toptees). It was used for training and evaluating both image generation and retrieval performance.
- **Fashion200k:** Consists of over 200,000 fashion images. Triplets were constructed by comparing image-caption pairs. It was used to further validate the model's retrieval performance.

## 4. Main Results
The proposed method demonstrates significant improvements in semantic alignment and generation quality, especially for local modifications.
- **Generation Quality & Alignment:** On the FashionIQ dataset, the proposed model achieved a CLIP Score of 0.71, outperforming ControlNet (0.66) and Stable Diffusion (0.64), while maintaining a competitive FID score for image fidelity.
- **Retrieval Performance:** When using the generated image to retrieve the ground-truth target, the model showed substantial gains. On FashionIQ, it achieved a Recall@50 of 56.10%, a significant improvement over ControlNet's 44.52% (+11.58% absolute). Similar large improvements were observed on the Fashion200k dataset.
- **Author's Takeaway:** The multi-round regularization successfully addresses the error accumulation problem in sequential editing, leading to higher-fidelity results that are better aligned with user instructions, particularly for fine-grained details often missed by single-round models.

## 5. Ablation Studies
Several ablation studies were performed on the FashionIQ dataset to validate the contributions of different components.
- **Effect of Multi-round Loss (`L_multi`):** Removing the multi-round loss (the "Baseline" model) caused a significant drop in retrieval accuracy, with Recall@50 falling from 56.08% to 47.98%. This confirms that the multi-round constraint is critical for improving semantic alignment.
- **Robustness to Ill-formed Text:** The model was tested on text with swapped sentence order, rotated word order, and masked words. The proposed method consistently produced more stable and accurate results (higher CLIP similarity to the target) compared to baselines, demonstrating its robustness. For instance, in the sentence order swap experiment, the proposed model achieved a consistency score of 0.882, compared to the baseline's 0.837.
- **Hyperparameter λ Schedule:** The study tested different schedules for the weight (λ) of the multi-round loss. Linearly decreasing λ from 1 to 0 during training yielded the best balance of semantic alignment (Recall@50 of 56.08%) and image fidelity (FID of 8.58), outperforming fixed or increasing schedules.

## 6. Paper Figures
![Fig. 2: A schematic overview of our framework. (a) Multi-round Generation: The multi-round regularization is achieved by a skip loss only supervising on final output ˜ z y 0 and ground truth z y 0 . Starting with the encoding of the reference image x into a latent embedding z x 0 , we conduct a complete denoising process twice to get ˜ z y 0 . The information of x is traced by the blue line, while the pink line indicates the flow of text information. (b) Single-round Reconstruction: In single-round reconstruction, the objective is to reconstruct the target image y by denoising on perturbed ground truth y alongside the corresponding text condition T and pose P . (c) A brief illustration of Pose-Conditioned Diffusion. Given the input c , P and z τ , the diffusion is to generate ˜ z 0 via iteratively denosing on the time step τ . We adopt an extra LoRA layer, which is concatenated into each attention block of the U-net decoder.]({{ '/images/01-2024/Instilling_Multi-round_Thinking_to_Text-guided_Image_Generation/figure_2.jpg' | relative_url }})
![Qualitative Comparison. We first qualitatively compare our method with two common generative models, e.g ., ControlNet and Stable Diffusion (SD). In the single-round generation, as shown in Figure 3 , the misalignment between text and the generated image is highlighted by a dashed box with the corresponding color. We observe three primary points. (1) The Stable Diffusion and ControlNet have more failure cases in capturing the detailed text meaning, such as editing sleeve length and changing the color depicted. (2) The main difference between the baseline and ControlNet is the semantic condition from the reference image. The enhancement in image generation and local editing is mainly due to the better preservation of semantic patterns by our condition fusion. (3) Besides, we could observe that the baseline model is also imperfect. For instance, the baseline misses keywords like “ruffles” with flat texture, while being sensitive to “casual” with an undefined logo. In contrast, the proposed method with the multi-round loss regularizes the training and shows a more consistent generation quality. On the other hand, we also evaluate these models in a multi-round generation by splitting the text as we elucidated in our methodology. Compared to the single-round generation, the split text is more challenging since it is more sparse in meaning. As shown in Figure 4 , Stable Diffusion and ControlNet lost the original reference information after two rounds of generation. Both models change the dress to the common category, i.e ., a T-shirt. In contrast, the baseline model performs well on “dress”, but it still misses the fine-grained “crinoline dress” pattern from the reference image. The main reason is that the intermediate results of the three methods, i.e ., Stable Diffusion, ContorlNet and baseline already miss some key patterns. Compared with these methods, our multi-round regularized model mainly changes color and texture in the second round of generation, maintaining consistency in local editing. We also could observe that the output result is controllable as it aligns well with the text modification. Quantitative Comparison. We study the generation quality and semantic alignment in this part, consolidating our observation from the qualitative evaluation. In particular, we train and evaluate models on FashionIQ and Fashion200k]({{ '/images/01-2024/Instilling_Multi-round_Thinking_to_Text-guided_Image_Generation/figure_3.jpg' | relative_url }})
