---
title: KV_Inversion:_KV_Embeddings_Learning_for_Text-Conditioned_Real_Image_Action_Editing
layout: default
date: 2023-09-28
---
## KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing
**Authors:**
- Jiancheng Huang, h-index: 13, papers: 31, citations: 852
- Shifeng Chen

**ArXiv URL:** http://arxiv.org/abs/2309.16608v1

**Citation Count:** 24

**Published Date:** 2023-09-28

![Fig. 1: Comparing with different concurrent image editing methods on the real natural world images, it is obvious that the object in the editing result of our KV Inversion can meet the edit prompt corresponding to the action editing, while retaining the original real image object.]({{ '/images/09-2023/KV_Inversion:_KV_Embeddings_Learning_for_Text-Conditioned_Real_Image_Action_Editing/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the problem of "action editing" in text-conditioned real image editing. Concurrent methods often fail at this specific task; they either cannot produce an edited image that accurately reflects the action described in the text prompt (e.g., changing "a dog standing" to "a dog running") or they fail to preserve the visual identity and texture of the original object. The paper aims to create a method that can successfully edit the action of a subject in a real image while maintaining its original appearance, without requiring fine-tuning of the entire diffusion model or training on large-scale datasets.

## 2. Key Ideas and Methodology
The core idea of the paper, named KV Inversion, is that the content (texture and identity) of an object in an image generated by a diffusion model is primarily controlled by the Key (K) and Value (V) projections in the self-attention layers of the U-Net. The authors propose a method to learn these K and V embeddings from a single source image to preserve its content during editing.

The methodology consists of three stages:
1.  **Inversion Stage:** Standard DDIM inversion is used on the source image to obtain a sequence of noisy latents, which serve as a supervision target for reconstruction.
2.  **Tuning Stage:** This is the main learning phase. The model learns a set of "KV embeddings" for the self-attention layers. For each step of the reverse diffusion process, these embeddings are optimized to ensure the reconstructed image is as close as possible to the original. This is achieved by introducing a "Content Preserving self-attention" (CP-attn) layer that modifies the original K and V values with these learnable parameters.
3.  **Editing Stage:** To generate the final edited image, the target text prompt provides the new action via cross-attention. Simultaneously, the learned KV embeddings are injected into the CP-attn layers to ensure the object's original content is preserved.

## 3. Datasets Used / Presented
The method does not require training on a large-scale dataset. Instead, it operates on a per-image basis. The experiments are conducted using two publicly available pre-trained models:
*   **Stable Diffusion:** Used for editing real-world photographic images.
*   **Anything-V3:** An anime-style model used to demonstrate the method's generality on artistic images.

For evaluation, the authors use a collection of real-world images (e.g., dogs, cats, horses, a Spiderman toy) and anime character images to demonstrate qualitative results.

## 4. Main Results
The paper presents its results qualitatively through visual comparisons with other state-of-the-art methods like InstructPix2Pix, PnP, MasaCtrl, and FastComposer.
*   In action editing tasks on both real and anime images, KV Inversion successfully alters the subject's pose to match the text prompt (e.g., making a character run or squat) while robustly preserving the subject's identity, color, and texture.
*   Competing methods often fail by either not changing the action, significantly distorting the subject, or replacing the subject with a different one.
*   The author-claimed impact is that KV Inversion is a training-free and tuning-free method that effectively solves the action editing problem for real images by decoupling content preservation from action modification.

## 5. Ablation Studies
The authors perform ablation studies to analyze the impact of their proposed Content Preserving self-attention (CP-attn) mechanism.

*   **Timestep Ablation:** The authors varied the reverse diffusion timesteps during which CP-attn is applied. Applying it for too few steps (late in the process) resulted in poor reconstruction and an edited image that did not resemble the original. Applying it for too many steps (starting early) made the result overly faithful to the source image, preventing effective action editing and leading to unnatural poses.
*   **Layer Ablation:** The authors varied which layers of the U-Net (encoder, middle, decoder) were upgraded with CP-attn. Upgrading all layers made the texture too rigid and hindered editing. Upgrading only the encoder or middle layers led to poor content preservation. The best results for both reconstruction and editing were achieved when CP-attn was applied only to the decoder layers.

## 6. Paper Figures
![Fig. 2: Pipeline of the proposed KV Inversion, which can be divided into 3 stages, inversion stage, tuning stage and editing stage. Inversion stage is for getting supervision. Tunning stage is for learning KV embedding for content preserving and editing stage is using the learned content for consistency editing results. itself as well as the text embedding on 5 images of the editing object, so the GPU memory and time cost required is too high. ELITE [43] requires training on a large dataset, which is even more expensive on time and GPU. MasaCtrl does not require training and finetuning, but its reconstruction performance on real images is unsatisfactory, making it difficult to editing. In our method, we aim to solve the problem of action editing without using multiple images of the same object for finetuning (generally known as Tuning-free in this field), without training the diffusion model itself, and without training on a large dataset for a long time (generally known as Training-free in this field), and to propose a solution to the mentioned challenges in the above setting. The core problem is how to preserve the content of the original object when the action changes. Our KV Inversion is divided into 3 stages. The fundamental difference with the concurrnet works [6, 12, 27] is that KV Inversion directly learns Key and Value at the self-attention layer to preserve the content of the source image by these learnable parameters (KV embeddings), which we call upgrading the original self-attention to Content Preserving self-attention (CPattn). Then, in the editing stage, we use the edit text prompt to introduce the action information and the learned KV embeddings to preserve the texture and identity of the object. We further control the timesteps of upgrading the CP-attn and the segmentation mask obtained by Segment Anything (SAM) [18], thus achieving a faster and more controllable editing. Our main contributions are summarized as follows. 1) We propose a training-free text-conditoned image action editing method KV Inversion to solve the action editing problem. 2) We design an upgrade version of self-attention named content preserving self-attention, which can preserve the texture and identity of the original object and then be used to fill the editing image. 3) Comprehensive Experiments show that KV Inversion can achieve satisfactory performance in real image editing.]({{ '/images/09-2023/KV_Inversion:_KV_Embeddings_Learning_for_Text-Conditioned_Real_Image_Action_Editing/figure_2.jpg' | relative_url }})
![Fig. 3: The intermediate predicted x 0 during the sampling process. KV Inversion can ensure the performance of both reconstruction and editing.]({{ '/images/09-2023/KV_Inversion:_KV_Embeddings_Learning_for_Text-Conditioned_Real_Image_Action_Editing/figure_3.jpg' | relative_url }})
![Fig. 4: Real image editing results of different editing methods on real human shape images. It is obvious that our KV Inversion can achieve very good action editing performance on humanoid objects, and the semantics of the actions corresponding to its limbs are almost imperceptible to editing.]({{ '/images/09-2023/KV_Inversion:_KV_Embeddings_Learning_for_Text-Conditioned_Real_Image_Action_Editing/figure_4.jpg' | relative_url }})
![Fig. 5: Real image editing results on anime character images. It is clear that our KV inversion method can achieve very good action editing performance on anime characters and their limbs correspond to natural action semantics.]({{ '/images/09-2023/KV_Inversion:_KV_Embeddings_Learning_for_Text-Conditioned_Real_Image_Action_Editing/figure_5.jpg' | relative_url }})
![Fig. 6: Ablation study of upgrading self-attention layer to CP-attn in different reverse timesteps (top) and U-Net places (bottom).]({{ '/images/09-2023/KV_Inversion:_KV_Embeddings_Learning_for_Text-Conditioned_Real_Image_Action_Editing/figure_6.jpg' | relative_url }})
