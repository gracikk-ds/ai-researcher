---
title: Iterative_Multi-granular_Image_Editing_using_Diffusion_Models
layout: default
date: 2023-09-01
---
![Figure 1. We introduce EMILIE : Iterative Multi granular Image Editor, a diffusion model that can faithfully follow a series of image editing instructions from a user. In this example, we see how the image of the fox has been semantically modified according to the provided edit instructions. Optionally, a user can instruct EMILIE with exact location of the desired edit, as illustrated in the last column.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address two primary gaps in existing diffusion-based image editing models that limit their practical use in creative workflows. First, current models are "one-shot," meaning they cannot handle a sequence of edits iteratively; a naive iterative application accumulates visual artifacts and degrades image quality. Second, these models lack a straightforward mechanism for users to control the spatial granularity of edits, such as making a change to a specific local region versus the entire image. The paper aims to create a more practical tool that supports both iterative editing and multi-granular spatial control.

## 2. Key Ideas and Methodology
The paper proposes **EMILIE (Iterative Multi-granular Image Editor)**, a training-free framework that extends a pre-trained diffusion model (InstructPix2Pix) to address the identified gaps. Its methodology is based on two core ideas:
-   **Latent Iteration Strategy:** To enable high-fidelity iterative editing, EMILIE passes the *latent representation* generated by the diffusion model from one step to the next, instead of passing the re-encoded edited image. This approach avoids the compounding of noise and artifacts that occurs during repeated encoding-decoding cycles.
-   **Multi-granular Control via Gradient Modulation:** To control the spatial extent of edits, the diffusion model is interpreted as an Energy-Based Model (EBM). This allows the noise prediction (equivalent to the energy gradient) to be selectively applied to user-specified regions using a binary mask. This "gradient control" effectively localizes the edit without requiring any model retraining.

## 3. Datasets Used / Presented
-   **IMIE-Bench (Iterative Multi-granular Image Editing Benchmark):** A new benchmark dataset introduced by the authors to evaluate iterative editing. It consists of 40 images curated from the LAION dataset, each paired with a sequence of four semantically consistent text-based edit instructions. Some instructions are also paired with masks to simulate local editing tasks.
-   **EditBench:** An existing public benchmark used to specifically evaluate the multi-granular (localized) editing performance of EMILIE against other state-of-the-art methods.

## 4. Main Results
-   **Iterative Editing Performance:** In a user study on IMIE-Bench, generations from EMILIE were preferred 62.5% of the time over baselines like iterative InstructPix2Pix (19.3%) and concatenating prompts (18.2%), demonstrating superior visual quality and semantic consistency across multiple edits.
-   **Multi-granular Editing Performance:** When evaluated on EditBench against specialized local editing methods, EMILIE's results were preferred by users 81.8% of the time. Quantitatively, EMILIE also achieved higher CLIP (0.311) and BLIP (0.620) scores, indicating better alignment between the generated image and the text prompt.
-   **Author-claimed impact:** The proposed latent iteration and gradient control methods successfully adapt a pre-trained diffusion model for practical, high-quality, iterative, and spatially-controlled editing, better supporting real-world creative workflows.

## 5. Ablation Studies
An ablation study was performed on the **gradient control** mechanism to validate its contribution to localized editing. An experiment was conducted where a local edit was performed with and without the proposed gradient modulation.
-   **Impact:** Without gradient control, the edit instruction was applied globally, changing the entire image's characteristics. With gradient control enabled, the edit was correctly localized to the masked region. This confirms that the gradient modulation component is essential for achieving multi-granular control.

## 6. Paper Figures
![Figure 2. While iteratively passing an image through the diffusion model [ 2 ], we see noisy artifacts being added in successive steps (we illustrate 4 , 8 , 12 and 16 steps) in Row 1. We see a similar phenomena while only auto-encoding the same image iteratively (here, the diffusion model is not used) in Row 2. Finally, when we iterate in the latent space (where z e +1 img is passed iteratively), we see that the successive images are robust to such noisy artifacts.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_2.jpg' | relative_url }})
![Figure 3. The figure shows how our proposed latent iteration framework is able to semantically modify the first image, consistent with the text caption provided by the user at each time step. Please see Sec. 4.2 for more details.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_3.jpg' | relative_url }})
![Figure 5. The figure compares the fourth edit step in Fig. 3 between image space and latent space iteration. Green boxes 2 , 3 and 5 shows improved preservation of semantic concepts from the previous edit step, while box 4 shows how newer related concepts are added, while keeping the image less noisy (box 1 ). Kindly zoom in for fine details.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_5.jpg' | relative_url }})
![Figure 6. Here we compare EMILIE with three baselines on images from IMIE-Bench. ‘Iterative PnP’ and ‘Iterative iP2P’ refers to Plug and Play [ 28 ] and Instruct Pix2Pix [ 2 ], where the latest edited image is recursively passed on in the next edit step. ‘Concatenating Captions’ refers to the setting where all the edit instructions that have been received so far, is concatenated and send through Instruct Pix2Pix [ 2 ] to edit the original image. We see that EMILIE is able to consistently reduce the noisy artefact accumulation and retain semantics of earlier steps better. Please see more results in the Supplementary material.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_6.jpg' | relative_url }})
![Figure 7. We illustrate the ability of EMILIE to do local editing on images from EditBench [ 30 ] benchmark. We compare against recent state-of-the-art methods, DiffEdit (ICLR ‘23) [ 4 ] and Blended Latent Diffusion (SIGGRAPH ‘23) [ 1 ]. Our simple gradient modification approach is able to consistently improve the quality of generation when compared to these approaches.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_7.jpg' | relative_url }})
![Figure 8. We re-purpose the local editing capability of EMILIE to insert objects into specific locations of a source image. For this, we combine EMILIE with GracoNet [ 36 ] which predicts the location of the object to be inserted. We can see from the results that the insertions done by EMILIE is more composed to the background.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_8.jpg' | relative_url }})
![Figure 9. Here we experiment with turning off the gradient control strategy explained in Sec. 4.1 for local edits. Without modulating the gradients, the characteristics of the edit instruction gets globally applied on the image, whereas EMILIE is able to easily localise the edits effectively.]({{ '/images/09-2023/Iterative_Multi-granular_Image_Editing_using_Diffusion_Models/figure_9.jpg' | relative_url }})
