---
title: GalaxyEdit:_Large-Scale_Image_Editing_Dataset_with_Enhanced_Diffusion_Adapter
layout: default
date: 2024-11-21
---
![Figure 1. Visual illustration of outputs generated by our proposed model.]({{ '/images/11-2024/GalaxyEdit:_Large-Scale_Image_Editing_Dataset_with_Enhanced_Diffusion_Adapter/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the scarcity of large-scale, diverse datasets for instruction-based image editing, specifically for object addition and removal tasks. Generating such datasets is often hindered by significant manual effort, high costs, and limitations in automation. Consequently, models trained on existing, often synthetic, datasets struggle to generalize to real-world images and complex editing instructions.

## 2. Key Ideas and Methodology
The paper introduces two main contributions:

1.  **GalaxyEdit Dataset:** A large-scale dataset for object addition and removal, created using a novel automated pipeline. This pipeline leverages a combination of modern AI tools: open-set object detectors (RAM, GroundingDINO), a dense captioner (GRiT) for attribute-rich descriptions, and an inpainting model (LaMa). A key innovation is its ability to generate a wide variety of instructions for each image edit, including simple, attribute-based, spatial, and multi-instance commands.

2.  **ControlNet-Vxs Adapter:** A new lightweight diffusion adapter that enhances the `ControlNet-xs` architecture. The core idea is that the simple linear connections in `ControlNet-xs` are insufficient for complex tasks like image editing. The authors replace this linear fusion mechanism with non-linear Volterra Neural Network (VNN) layers. These VNNs act as a more powerful "bridge" for information exchange between the control network and the frozen base model, enabling better adaptation to intricate visual edits.

## 3. Datasets Used / Presented
*   **GalaxyEdit (Presented):** A new dataset for instruction-based object addition and removal. The authors generated a training set of 800,000 samples and a test set of 1,000 samples, using images from the COCO dataset as a source.
*   **MagicBrush (Used):** A public benchmark dataset used to evaluate the generalization performance of their model against baselines.
*   **LAION (Used):** A dataset of 1 million images used for training and evaluating the adapter on the Canny-to-image generation task.

## 4. Main Results
The authors demonstrate the superiority of their dataset and model through extensive experiments.

*   A diffusion model fine-tuned on the **GalaxyEdit** dataset significantly outperformed leading methods like IP2P, InstInpaint, and PIPE. On the GalaxyEdit test set, it achieved an **11.2% better FID score for object addition** and a **26.1% better FID score for object removal** compared to the next-best models.
*   Human evaluators rated the outputs from the GalaxyEdit-trained model higher than all baselines, with average scores of 4.0/5 for removal and 4.1/5 for addition, citing superior instruction following.

## 5. Ablation Studies
The authors performed ablation studies to validate the effectiveness of their proposed `ControlNet-Vxs` adapter.

*   **Adapter Architecture Comparison:** `ControlNet-Vxs` was compared against the baseline `ControlNet-xs` on the GalaxyEdit dataset. The proposed VNN-enhanced model achieved a significantly better FID score for both the remove task (a reduction of 11.4%) and the add task, confirming that the non-linear Volterra fusion improves performance on complex editing tasks.
*   **Task Generalization:** The adapters were also evaluated on a Canny-to-image generation task. `ControlNet-Vxs` again outperformed `ControlNet-xs`, achieving a better FID score (15.1 vs. 17.1). This shows that the benefit of the Volterra fusion is not limited to image editing and generalizes to other types of conditional image generation.

## 6. Paper Figures
![Figure 2. Proposed data generation pipeline for GalaxyEdit dataset.]({{ '/images/11-2024/GalaxyEdit:_Large-Scale_Image_Editing_Dataset_with_Enhanced_Diffusion_Adapter/figure_2.jpg' | relative_url }})
![Figure 3. ControlNet-Vxs Model Architecture.]({{ '/images/11-2024/GalaxyEdit:_Large-Scale_Image_Editing_Dataset_with_Enhanced_Diffusion_Adapter/figure_3.jpg' | relative_url }})
![Figure 4. Qualitative results on Object Removal (Left) and Object Addition (Right). We compare our model against leading methods for add and remove showcasing diverse instruction following capability while maintaining high fidelity to the input image.]({{ '/images/11-2024/GalaxyEdit:_Large-Scale_Image_Editing_Dataset_with_Enhanced_Diffusion_Adapter/figure_4.jpg' | relative_url }})
![Figure 5. Category-wise results on GalaxyEdit test set]({{ '/images/11-2024/GalaxyEdit:_Large-Scale_Image_Editing_Dataset_with_Enhanced_Diffusion_Adapter/figure_5.jpg' | relative_url }})
![Figure 6. Qualitative Comparison of ControlNet-xs and ControlNet-Vxs.]({{ '/images/11-2024/GalaxyEdit:_Large-Scale_Image_Editing_Dataset_with_Enhanced_Diffusion_Adapter/figure_6.jpg' | relative_url }})
