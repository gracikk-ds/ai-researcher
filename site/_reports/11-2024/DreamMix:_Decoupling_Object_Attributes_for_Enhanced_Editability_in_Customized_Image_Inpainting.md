---
title: DreamMix:_Decoupling_Object_Attributes_for_Enhanced_Editability_in_Customized_Image_Inpainting
layout: default
date: 2024-11-26
---
## DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting
**Authors:**
- Yicheng Yang, h-index: 1, papers: 2, citations: 4
- Huchuan Lu, h-index: 8, papers: 19, citations: 739

**ArXiv URL:** http://arxiv.org/abs/2411.17223v1

**Citation Count:** None

**Published Date:** 2024-11-26

![Figure 1. DreamMix on various subject-driven image customization tasks. (a) Identity Preservation : DreamMix precisely inserts a target object into any scene, achieving high-fidelity and harmonized composting results. (b) Attribute Editing : DreamMix allows users to modify object attributes such as color, texture, and shape or add accessories based on textual instructions. (c) Small Object Inpainting : DreamMix effectively performs small object insertion and editing while preserving fine-grained details and visual harmony.]({{ '/images/11-2024/DreamMix:_Decoupling_Object_Attributes_for_Enhanced_Editability_in_Customized_Image_Inpainting/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Subject-driven image inpainting methods have advanced significantly, allowing users to insert specific objects into scenes. However, existing approaches primarily focus on preserving the object's identity and struggle to edit its attributes (e.g., color, texture, shape) using text prompts. This "identity overfitting" limits the creative flexibility of these tools. The authors aim to solve this by creating a model that can both faithfully insert an object and allow for fine-grained, text-driven attribute modifications.

## 2. Key Ideas and Methodology
The paper introduces **DreamMix**, a diffusion-based model that decouples object identity from its attributes to enhance editability. Its methodology is built on three core components:
- **Disentangled Inpainting Framework (DIF):** A two-stage denoising process that first performs **Local Content Generation (LCG)** to accurately insert the object, followed by **Global Context Harmonization (GCH)** to ensure the object blends seamlessly and harmoniously with the background scene.
- **Attribute Decoupling Mechanism (ADM):** An automated finetuning strategy that uses a Vision-Language Model (VLM) to generate a diverse set of descriptive prompts and corresponding images from a few subject examples. This prevents the model from memorizing the subject's original attributes and improves its generalization for editing.
- **Textual Attribute Substitution (TAS):** A testing-time module that uses orthogonal decomposition to isolate the user's target attribute descriptions from the object's core identity in the text embedding. This reduces conceptual interference and enables more precise editing.

## 3. Datasets Used / Presented
- **DreamBooth Dataset:** 30 subjects from this dataset were used to provide the reference objects for finetuning and evaluation.
- **COCO-val2017:** Images from this dataset were used as backgrounds. The authors created a new benchmark by combining the 30 subjects with over 4,000 high-quality image-mask pairs from COCO.
- **COCO-test2017:** Used as the reference distribution for calculating the Fréchet Inception Distance (FID) to evaluate image realism.

## 4. Main Results
DreamMix demonstrates superior performance over existing methods in both identity preservation and attribute editing.
- **Quantitative Comparison:** In attribute editing, DreamMix achieves a CLIP-T score of 0.289 and a DINO score of 0.695, outperforming specialized methods like IP-Adapter and LAR-Gen. For identity preservation, it achieves a CLIP-I of 0.728, surpassing few-shot finetuning baselines like DreamBooth (0.644).
- **User Study:** DreamMix was overwhelmingly preferred by users, chosen in 74% of attribute editing tasks and 55% of identity preservation tasks over six other competing methods. This highlights its effectiveness in producing high-quality, editable, and identity-consistent results.

## 5. Ablation Studies
The authors conducted systematic ablation studies to validate the contribution of each proposed component.
- **Disentangled Inpainting Framework (DIF):** Adding DIF to the baseline model significantly improved performance across all metrics for both identity preservation (e.g., CLIP-I increased from 0.644 to 0.723) and attribute editing (e.g., CLIP-T increased from 0.235 to 0.265), demonstrating its effectiveness in generating higher-quality and more accurate inpainting.
- **Textual Attribute Substitution (TAS):** Incorporating TAS further improved text-driven editing precision (CLIP-T increased to 0.275) by effectively isolating attribute information, without degrading identity preservation.
- **Attribute Decoupling Mechanism (ADM):** The full model, including ADM, achieved the best attribute editing performance (CLIP-T of 0.289), confirming that the diverse training data generated by ADM successfully mitigates identity overfitting and enhances editability.

## 6. Paper Figures
![Figure 2. Overview of DreamMix . During finetuning, we use the source data { x s , p s } along with regular data { x r , p r } constructed via an attribute decoupling mechanism (Sec. 3.3 ), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ a disentangled inpainting framework (Sec. 3.2 ), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose a textual attribute substitution module (Sec. 3.4 ) to generate a decomposed text embedding to enhance the editability of our method during testing.]({{ '/images/11-2024/DreamMix:_Decoupling_Object_Attributes_for_Enhanced_Editability_in_Customized_Image_Inpainting/figure_2.jpg' | relative_url }})
![Figure 3. Pipeline of Attribute Decoupling Mechanism (ADM). We obtain the attribute word list using a VLM agent [ 24 ] and create regular data with more diverse text formats and image contents.]({{ '/images/11-2024/DreamMix:_Decoupling_Object_Attributes_for_Enhanced_Editability_in_Customized_Image_Inpainting/figure_3.jpg' | relative_url }})
![Figure 4. Visual comparison between different methods. From left to right are input image and subject, visual results of our DreamMix , IP-Adapter [ 40 ], DreamBooth [ 33 ], TIGIC [ 18 ], AnyDoor [ 6 ], and LAR-Gen [ 26 ].]({{ '/images/11-2024/DreamMix:_Decoupling_Object_Attributes_for_Enhanced_Editability_in_Customized_Image_Inpainting/figure_4.jpg' | relative_url }})
![Figure 5. Effect of different values of λ in disentangled inpainting framework. λ = 1 means only GCH stage is performed while λ = 0 means only LCG stage is used. λ is set to 0.7 in our experiments.]({{ '/images/11-2024/DreamMix:_Decoupling_Object_Attributes_for_Enhanced_Editability_in_Customized_Image_Inpainting/figure_5.jpg' | relative_url }})
![Figure 6. Visual examples for ablation studies on identity preservation (top row) and attribute editing (bottom row).]({{ '/images/11-2024/DreamMix:_Decoupling_Object_Attributes_for_Enhanced_Editability_in_Customized_Image_Inpainting/figure_6.jpg' | relative_url }})
