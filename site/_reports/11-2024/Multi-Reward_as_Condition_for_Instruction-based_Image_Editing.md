---
title: Multi-Reward_as_Condition_for_Instruction-based_Image_Editing
layout: default
date: 2024-11-06
---
## Multi-Reward as Condition for Instruction-based Image Editing
**Authors:**
- Xin Gu
- Sijie Zhu, h-index: 4, papers: 8, citations: 54

**ArXiv URL:** http://arxiv.org/abs/2411.04713v2

**Citation Count:** 8

**Published Date:** 2024-11-06

![Figure 1: Existing image editing datasets and our method. Best viewed with zoom-in.]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the problem of low-quality training data in instruction-based image editing. Predominant datasets, like InsPix2Pix, are generated using text-to-image models not specifically trained for editing tasks. This results in training triplets (instruction, original image, edited image) that suffer from significant noise, including inaccurate instruction following, poor preservation of unchanged details, and the introduction of visual artifacts. This noisy supervision limits the performance of state-of-the-art editing models.

## 2. Key Ideas and Methodology
The core idea is to rectify the inaccurate supervision by conditioning the editing model on multi-perspective rewards, rather than attempting to refine the ground-truth images themselves.

The methodology involves three main steps:
1.  **Reward Data Generation:** The authors create a new dataset, **RewardEdit-20K**, by evaluating 20,000 training triplets from InsPix2Pix using GPT-4o. For each triplet, they generate quantitative scores (0-5) and descriptive text feedback from three perspectives: (1) instruction following, (2) detail preserving, and (3) generation quality.
2.  **Multi-Reward Conditioning:** They propose a novel training framework that integrates this reward information as an auxiliary condition. During training, the reward scores and text descriptions are encoded into a single "reward condition" embedding. This embedding is then fused into the diffusion model's U-Net architecture via an attention-based reward encoder and direct addition to the U-Net blocks.
3.  **Inference:** During inference, the model is guided to produce high-quality results by setting the reward condition to the highest possible scores (e.g., 5 for all perspectives) and providing no text description for failure points.

## 3. Datasets Used / Presented
*   **RewardEdit-20K (Presented):** A dataset of 20,000 training samples from InsPix2Pix, annotated with multi-perspective reward scores and text feedback using GPT-4o. It is used to train the reward-conditioned model.
*   **Real-Edit (Presented):** A new evaluation benchmark consisting of 80 high-quality, real-world photos from Unsplash and 560 diverse, challenging editing instructions (7 per image) generated by GPT-4o. It is used for performance evaluation.
*   **MagicBrush (Used):** An existing benchmark used for additional evaluation to compare with other state-of-the-art methods.

## 4. Main Results
The proposed method, when applied to the state-of-the-art SmartEdit model (becoming Reward-SmartEdit), demonstrates significant improvements on the Real-Edit benchmark as evaluated by GPT-4o:
*   **Instruction Following Accuracy:** Increased from 64% to 69%.
*   **Detail Preserving Accuracy:** Increased from 66% to 74%.
*   **Generation Quality Accuracy:** Increased from 45% to 49%.

Human evaluations confirmed this trend, with Reward-SmartEdit outperforming the baseline SmartEdit across all three metrics (e.g., following score improved from 3.09 to 3.34 out of 5). The authors claim their method effectively rectifies noisy supervision and achieves new state-of-the-art performance.

## 5. Ablation Studies
*   **Reward Data Types:** The impact of using only reward scores, only reward text, or both was tested. The baseline model (no reward) had a 49% following accuracy. Using scores alone improved it to 61%, text alone to 60%, and using both achieved the best result of 63%. This confirms that both score and text components are valuable.
*   **Reward Integration Methods:** The study compared using attention in the reward encoder, addition in the U-Net, or both. The baseline (49% following accuracy) improved to 60% with attention only, 57% with addition only, and 63% when combining both methods, validating the dual-integration approach.
*   **Inference Reward Scores:** The model's output was tested by setting different reward scores during inference. As scores were lowered from [5,5,5] to [0,0,0], performance degraded significantly across all metrics (e.g., following accuracy dropped from 63% to 54%). This shows the model successfully learned to interpret the reward scores and can produce controllably different quality outputs.

## 6. Paper Figures
![Figure 2: Generation process of reward data. Given the editing triplets, reward data was generated using GPT-4o by setting prompts from different perspectives.]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_2.jpg' | relative_url }})
![Figure 3: Distribution of reward score.]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_3.jpg' | relative_url }})
![Figure 4: Word cloud of reward text.]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_4.jpg' | relative_url }})
![Figure 5: The overall framework of our approach. The original image x is first encoded into an image condition by the VAE encoder. This image condition c I is then concatenated with latent noise Z t to serve as the query for the reward encoder, with the reward condition c R as the key/value. The resulting latent noise, containing reward information, is used as the input for the U-Net module. Meanwhile, the instruction is encoded into a text condition c T by the text encoder, which is fed into each block of the U-Net. To further enhance reward guidance, we incorporate the reward condition after each block. Finally, the U-Netâ€™s output is decoded by the VAE decoder into the edited image y .]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_5.jpg' | relative_url }})
![Figure 6: Distribution of different categories of images in Real-Edit.]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_6.jpg' | relative_url }})
![Figure 7: An example in Real-Edit.]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_7.jpg' | relative_url }})
![Figure 8: Evaluation process. The generated edited image, original image, and instruction are input into GPT. Three prompts are designed to evaluate from three different aspects. For each aspect, determine whether the criteria are met and assign a score (ranging from 0 to 5).]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_8.jpg' | relative_url }})
![Figure 9: Qualitative results on Real-Edit. The three scores below the image are given by GPT-4o in three aspects: instruction following, detail preservation, and generation quality.]({{ '/images/11-2024/Multi-Reward_as_Condition_for_Instruction-based_Image_Editing/figure_9.jpg' | relative_url }})
