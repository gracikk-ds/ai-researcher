---
title: response_mgie_gemini-2
layout: default
---
![Figure 1: We introduce MLLM-Guided Image Editing (MGIE) to improve instruction-based image editing for various editing aspects. The top is the input instruction, and the right is the jointly derived expressive instruction by MGIE.]({{ '/images/mgie/figure_1.png | relative_url' }})

*Figure 1: We introduce MLLM-Guided Image Editing (MGIE) to improve instruction-based image editing for various editing aspects. The top is the input instruction, and the right is the jointly derived expressive instruction by MGIE.*


## 1. Motivation of the Paper
    - The authors address the problem that instruction-based image editing models often fail when human commands are too brief or ambiguous. Current methods, which rely on text encoders like CLIP, are trained for static descriptions and struggle to capture the *transformative intent* of an editing instruction. This gap limits their ability to perform reasonable and complex edits as expected by users.

## 2. Key Ideas and Methodology
    - **Core Hypothesis:** Multimodal Large Language Models (MLLMs) can bridge the gap between ambiguous user commands and the explicit guidance needed for high-quality image editing. By reasoning over both the input image and the instruction, an MLLM can generate a more detailed and actionable "expressive instruction."
    - **High-level Approach:** The proposed model, MLLM-Guided Image Editing (MGIE), consists of two main components trained end-to-end:
        1.  **MLLM for Guidance:** An MLLM takes the input image and a user's brief instruction (e.g., "make it more healthy"). It learns to derive a concise, expressive instruction that details the intended edit (e.g., "The pizza includes vegetable toppings, such as tomatoes and herbs").
        2.  **Diffusion Model for Editing:** The expressive instruction, along with latent visual tokens generated by the MLLM, is passed to a latent diffusion model. This guidance conditions the diffusion process to manipulate the input image according to the detailed plan, achieving the desired edit.
    - **Key Foundations:** The method is built upon pre-trained foundation models, specifically LLaVA (an MLLM) and Stable Diffusion (a latent diffusion model). The core innovation is the joint training framework that teaches the MLLM to generate effective editing guidance for the diffusion model.

## 3. Datasets Used / Presented
    - **Pre-training:** IPr2Pr, a large-scale dataset with 1 million synthetic image-instruction-goal triplets.
    - **Evaluation:** The model is evaluated on four diverse, human-annotated datasets:
        - **EVR:** 5.7K triplets for Photoshop-style modifications.
        - **GIER:** 29.9K triplets for global photo optimization (e.g., exposure, color).
        - **MA5k:** 24.8K triplets for global photo adjustments (e.g., contrast, brightness).
        - **MagicBrush:** 10.5K triplets for fine-grained local editing.

## 4. Main Results
    - In both zero-shot and fine-tuned settings, MGIE consistently outperforms the baseline (InstructPix2Pix) and a text-only LLM-guided variant (LGIE) across all four evaluation datasets and multiple metrics (L1, DINO, CVS, SSIM, LPIPS). For instance, on the fine-tuned MagicBrush dataset, MGIE achieves a 95.3 CVS score, compared to 93.8 for InstructPix2Pix.
    - Human evaluations show that users strongly prefer MGIE's results in terms of instruction following, relevance to the ground truth, and overall quality. Furthermore, the expressive instructions generated by MGIE were rated as more practical and less prone to hallucination than those from the text-only LGIE.
    - **Author-claimed impact:** Expressive instructions derived with visual context are crucial for high-quality instruction-based image editing, and the proposed MGIE framework effectively provides this guidance, leading to notable improvements in performance and user preference. The evidence strongly supports the authors' claims.

## 5. Ablation Studies
    - **Effectiveness of End-to-End (E2E) Training:** The authors compared their E2E approach to two alternatives: using expressive instructions with a frozen InstructPix2Pix model (FZ) and fine-tuning it (FT). The E2E approach performed best, demonstrating that jointly training the MLLM and the diffusion model is critical for learning to extract and apply relevant guidance.
    - **Importance of Visual Perception:** MGIE (which sees the image) was compared to LGIE (which only sees the text instruction). MGIE consistently outperformed LGIE, indicating that visual context is essential for generating high-quality, relevant editing instructions and avoiding misleading guidance.
    - **Role of Instruction Loss (L_ins):** An ablation removing the loss for generating expressive instructions (`L_ins`) showed a significant drop in performance. This highlights the importance of explicitly training the MLLM to produce concise and relevant guidance, rather than relying on its raw, often lengthy output.
    - **Summarized vs. Full Instructions:** The authors found that using summarized expressive instructions (the default for MGIE) yields better results than using the full, verbose output from the MLLM. The summary strikes an effective balance between being descriptive and concise.

## 6. Paper Figures
![Figure 10: CLIP-S across images and expressive instructions (full / short / summarized).]({{ '/images/mgie/figure_10.png | relative_url' }})

*Figure 10: CLIP-S across images and expressive instructions (full / short / summarized).*


![Figure 11: CLIP-S across images and expressive instructions by the “ how ” or “ what ” prompt.]({{ '/images/mgie/figure_11.png | relative_url' }})

*Figure 11: CLIP-S across images and expressive instructions by the “ how ” or “ what ” prompt.*


![Figure 2: Overview of MLLM-Guided Image Editing ( MGIE ), which leverages MLLMs to enhance instruction-based image editing. MGIE learns to derive concise expressive instructions and provides explicit visual-related guidance for the intended goal. The diffusion model jointly trains and achieves image editing with the latent imagination through the edit head in an end-to-end manner.]({{ '/images/mgie/figure_2.png | relative_url' }})

*Figure 2: Overview of MLLM-Guided Image Editing ( MGIE ), which leverages MLLMs to enhance instruction-based image editing. MGIE learns to derive concise expressive instructions and provides explicit visual-related guidance for the intended goal. The diffusion model jointly trains and achieves image editing with the latent imagination through the edit head in an end-to-end manner.*


![Figure 3: Trade-off curve for image editing . We set α X as 7.5 and vary α V in [1 . 0 , 2 . 2] . For both edit (X-axis) and input consistency (Yaxis), higher is better.]({{ '/images/mgie/figure_3.png | relative_url' }})

*Figure 3: Trade-off curve for image editing . We set α X as 7.5 and vary α V in [1 . 0 , 2 . 2] . For both edit (X-axis) and input consistency (Yaxis), higher is better.*


![Figure 4: CLIP-S across images (input / goal) and expressive instructions.]({{ '/images/mgie/figure_4.png | relative_url' }})

*Figure 4: CLIP-S across images (input / goal) and expressive instructions.*


![Figure 5: Human eval of expressive instructions quality.]({{ '/images/mgie/figure_5.png | relative_url' }})

*Figure 5: Human eval of expressive instructions quality.*


![Figure 6: Human eval of image editing results in terms of instruction following, ground-truth relevance, and overall quality.]({{ '/images/mgie/figure_6.png | relative_url' }})

*Figure 6: Human eval of image editing results in terms of instruction following, ground-truth relevance, and overall quality.*


![Figure 7: Qualitative comparison between InsPix2Pix, LGIE, and our MGIE. For the 1st example, MGIE can showcase the clear “ lightning ” in the sky and its reflection on the water. For the 2nd one, although LGIE accurately targets the Christmas tree, only MGIE removes it in the background. For photo optimization (the 3rd example), InsPix2Pix fails to adjust the brightness, and LGIE makes the whole photo white and obviously distinct. In contrast, MGIE follows the instruction to brighten as well as sharpen it. Moreover, in the 4th one, MGIE puts the “ glaze ” only on the donuts, but baselines even draw the entire image in strawberry pink.]({{ '/images/mgie/figure_7.png | relative_url' }})

*Figure 7: Qualitative comparison between InsPix2Pix, LGIE, and our MGIE. For the 1st example, MGIE can showcase the clear “ lightning ” in the sky and its reflection on the water. For the 2nd one, although LGIE accurately targets the Christmas tree, only MGIE removes it in the background. For photo optimization (the 3rd example), InsPix2Pix fails to adjust the brightness, and LGIE makes the whole photo white and obviously distinct. In contrast, MGIE follows the instruction to brighten as well as sharpen it. Moreover, in the 4th one, MGIE puts the “ glaze ” only on the donuts, but baselines even draw the entire image in strawberry pink.*


![Figure 8: Qualitative comparison of expressive instructions by LGIE and our MGIE. Due to the limitation of the single modality, LGIE can only have language-based insight but may derive irrelevant or even wrong explanations for image editing ( e.g. , “ two people still in the foreground ” for GIER). With access to images, MGIE provides explicit visual imagination after the editing such as “ baby on the beach with a shark ” or “ bring out details of leaves and trunk ”. More surprisingly, we can link “ lightsaber or spaceship ” from Star Wars and describe “ chewing on the stick ” for the dog, which is aligned with the intended goal.]({{ '/images/mgie/figure_8.png | relative_url' }})

*Figure 8: Qualitative comparison of expressive instructions by LGIE and our MGIE. Due to the limitation of the single modality, LGIE can only have language-based insight but may derive irrelevant or even wrong explanations for image editing ( e.g. , “ two people still in the foreground ” for GIER). With access to images, MGIE provides explicit visual imagination after the editing such as “ baby on the beach with a shark ” or “ bring out details of leaves and trunk ”. More surprisingly, we can link “ lightsaber or spaceship ” from Star Wars and describe “ chewing on the stick ” for the dog, which is aligned with the intended goal.*


![Figure 9: CLIP-S across images and expressive instructions by different sizes of MGIE.]({{ '/images/mgie/figure_9.png | relative_url' }})

*Figure 9: CLIP-S across images and expressive instructions by different sizes of MGIE.*
