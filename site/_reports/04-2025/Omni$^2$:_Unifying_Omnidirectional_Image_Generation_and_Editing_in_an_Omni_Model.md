---
title: Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model
layout: default
date: 2025-04-15
---
## Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model
**Authors:**
- Liu Yang
- Patrick Le Callet

**ArXiv URL:** http://arxiv.org/abs/2504.11379v1

**Citation Count:** 2

**Published Date:** 2025-04-15

![but it is both time-consuming and inefficient, and may generate inconsistent views. Moreover, due to the unique depth and spatial characteristics of omnidirectional images, conventional 2D editing models struggle to understand the spatial relationships between viewports or even within individual ODI views, making the splitand-edit approach impractical. Therefore, dedicated ODI editing dataset and method are necessary to advance research in this domain. Built upon the rapid progress in 2D image generation and editing, integrating these tasks into a unified framework has become increasingly popular [ 31 , 37 , 39 ]. In this paper, we aim to unify ODI generation and editing into an efficient model. To this end, we first introduce Any2Omni , the first comprehensive dataset for omnidirectional image generation and editing tasks. As shown in Table 1, our dataset integrates multiple input modalities for ODI generation tasks. For the newly defined omnidirectional image editing tasks, we start by proposing a simple yet effective pipeline capable of generating high-quality, object-level indoor editing samples. Additionally, we introduce two scene-level ODI editing tasks utilizing existing ODI datasets [ 44 ]. Overall, our Any2Omni dataset comprises over 60,000 training samples, covering 9 categories of omnidirectional image generation and editing tasks with various input conditions. Based on Any2Omni, we further introduce the first omni model for omnidirectional image generation and editing, termed Omni 2 . Omni 2 adopts a simple yet effetive Transformer-based framework to support 360 ◦ × 180 ◦ high-quality omnidirectional image synthesis under a variety of multimodal input conditions. In contrast to existing diffusion-based ODI generation methods, which incorporate additional attention blocks for multi-view consistency [ 29 , 42 ], we introduce a novel approach by executing viewport-based bidirectional attention within a unified Transformer. Our model demonstrates superior performance across a wide range of ODI generation and editing tasks with various input conditions, as shown in Fig. 1. The main highlights of this work include:]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The paper addresses the significant challenge of creating and modifying 360° omnidirectional images (ODIs), which are crucial for virtual and augmented reality applications. Capturing real-world ODIs is expensive, and existing 2D image synthesis models perform poorly due to the unique equirectangular projection (ERP) format and wide field-of-view of ODIs. Previous ODI-specific generation methods are limited to single tasks (like text-to-image) and often produce inconsistent or incomplete results. Furthermore, the field of omnidirectional image editing is almost entirely unexplored, primarily due to the lack of dedicated datasets and tailored models.

## 2. Key Ideas and Methodology
The authors introduce **Omni²**, a unified model designed to handle both ODI generation and editing from diverse multimodal inputs. The core methodology is to decompose the complex task of full-image synthesis into a more manageable one:
- **Viewport-based Processing:** Instead of processing the entire warped ODI at once, the model splits it into six overlapping perspective viewports. It then generates or edits these viewports individually within a shared context before stitching them back into a seamless 360° image.
- **Unified Transformer Architecture:** A single Transformer-based diffusion model is used as the denoising network for all tasks. This promotes efficiency and knowledge sharing across different generation and editing capabilities.
- **Bidirectional Attention for Consistency:** To ensure the generated viewports are coherent and free of seams, the model employs a novel viewport-based bidirectional attention mechanism. This allows each viewport to attend to all others during the generation process, enforcing global consistency.
- **Task-Specific Loss:** The model is trained with a flow-matching objective. For editing tasks, a weighted loss function is used to focus model updates on the specific regions being modified, while a standard loss is used for global changes like decoration.

## 3. Datasets Used / Presented
The paper presents **Any2Omni**, the first large-scale, comprehensive dataset for both omnidirectional image generation and editing.
- **Name & Size:** Any2Omni contains over 60,000 training samples.
- **Domain & Content:** It covers 9 distinct tasks, including generation (Text2Image, Semantic2Image, Depth2Image, Inpainting, Outpainting) and editing (Object-level Add/Remove, Light Modification, Decoration).
- **Construction:** The dataset was created by collecting and processing existing ODI data (e.g., from Structured3D, SUN360). A novel pipeline was developed for the editing subset, using models like GroundedSAM for segmentation and InternVL2-5 for generating detailed, human-like instructions.

## 4. Main Results
Omni² demonstrates state-of-the-art performance across all evaluated tasks, establishing a new benchmark for ODI synthesis.
- **Text-to-Image Generation:** Omni² achieves a Fréchet Inception Distance (FID) of 47.32, significantly outperforming the previous best model, PanFusion (80.66). User studies also rated Omni² highest for image quality and text consistency.
- **Outpainting & Editing:** The model produces more visually coherent and semantically plausible results in outpainting tasks compared to other methods. In editing, it successfully executes complex instructions that cause leading 2D editing models to fail, highlighting the effectiveness of its ODI-native design.
- **Takeaway:** The authors claim that Omni² is the first model to successfully unify a wide range of ODI generation and editing tasks, proving the viability of a single "omni model" for omnidirectional vision.

## 5. Ablation Studies
- **Bidirectional Attention Mask:** An experiment was run using a standard causal attention mechanism instead of the proposed bidirectional one. This resulted in generated images with severe distortions and visible seams between viewports, confirming that the bidirectional attention is critical for maintaining scene consistency.
- **Loss Function:** The study validated the task-specific loss design. For object editing, using a normal loss instead of the weighted loss caused the model to simply copy the input image without making changes. Conversely, applying the weighted loss to the scene-wide decoration task resulted in heavy, undesirable image distortions.
- **LoRA Rank:** The model's performance was tested with different LoRA ranks (8, 16, 32). The chosen rank of 16 yielded the best overall quantitative results for the text-to-image task, validating the hyperparameter selection.

## 6. Paper Figures
![Figure 2: We present a simple yet effective pipeline for constructing high-quality object-level indoor omnidirectional image editing dataset. Our pipeline is mainly consisted of two parts, i.e. , image pair generation and instruction refinement. (a) The ODI input undergoes viewport splitting to generate six perspective images. (b) Viewport image is processed through a segmentation model for instance-level segmentation and class labeling, followed by dual-stage filtering for quality control. (c) A selected instance is removed via inpainting [ 28 ]. (d) The edited viewport image is seamlessly stitched with other perspective images to form edited ODI. (e) InternVL2-5 [ 9 ] is deployed to refine editing instructions, adding positional details and enhancing linguistic diversity.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_2.jpg' | relative_url }})
![Figure 3: Overview of proposed Omni 2 . Input prompt and Image are tokenized through text tokenizer and viewport tokenizer seperately before feeding into a simple yet effective transformer for viewport image generation. During inference, the generated viewports are seamlessly integrated to reconstruct a high-quality omnidirectional image.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_3.jpg' | relative_url }})
![Figure 4: Text to ODI comparison between Text2Light [ 8 ], MVDiffusion [ 29 ], PanFusion [ 42 ] and ours. We highlight the left-right inconsistency , repeating objects in different views and top-bottom blurriness/missing with corresponding color boxes. Objects that are missing in some baselines but present in our method are bolded and highlghted in red in the prompts.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_4.jpg' | relative_url }})
![Figure 5: Outpainting comparison between SIG-SS [12], OmniDreamer [2], PanoDiffusion [36], PanoDiff [32], and ours.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_5.jpg' | relative_url }})
![(b) “A 360 view of a beautiful under water scenario.” Figure 6: Text-instructed outpainting comparison between PanoDiff [32] and ours.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_6.jpg' | relative_url }})
![(b) “Add a yellow bookshelf on my left.” Figure 7: Visual comparison of directly applying existing 2D editing methods [5, 18, 43] on ODIs versus our method.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_7.jpg' | relative_url }})
![Figure 8: Visual comparison of directly applying existing 2D editing methods [ 5 , 18 , 43 ] on designated views of ODIs versus our method. Only the designated view is presented.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_8.jpg' | relative_url }})
![Figure 9: Visual results of ablation studies. Top: ablation on viewport-based bidirectional attention mask; Middle: ablation on weighed loss for object-level editing tasks; Bottom: ablation on normal loss for decoration task.]({{ '/images/04-2025/Omni$^2$:_Unifying_Omnidirectional_Image_Generation_and_Editing_in_an_Omni_Model/figure_9.jpg' | relative_url }})
