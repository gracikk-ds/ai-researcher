---
title: Routing_to_the_Right_Expertise:_A_Trustworthy_Judge_for_Instruction-based_Image_Editing
layout: default
date: 2025-04-10
---
## Routing to the Right Expertise: A Trustworthy Judge for Instruction-based Image Editing
**Authors:**
- Chenxi Sun
- Fuzheng Zhang, h-index: 2, papers: 10, citations: 12

**ArXiv URL:** http://arxiv.org/abs/2504.07424v1

**Citation Count:** 0

**Published Date:** 2025-04-10

![Figure 1: Conceptual architecture of the JURE framework. JURE takes the original image, editing instruction “Add a black cat behind the boy”, edited images generated by different IIE models (IP2P, HQ Edit, EMU Edit) as the initial input. In this example, JURE iteratively reasons about to verify (i) object presence (whether a cat was added), (ii) attribute accuracy (if added, whether it is black), (iii) spatial correctness (whether it appears behind the boy), and (iv) visual integrity (ensuring no unwanted edits or artifacts appear elsewhere). During each iteration (highlighted by different colors), the Orchestrator routes to the right expert, incrementally stores their output to the Context Dictionary for future reference, and dynamically decides its action for the next iteration. For instance, after detecting that IP2P failed to retain the boy in the first iteration, subsequent spatial analysis involving the boy and the cat is performed only for the HQ and EMU edits. Finally, it aggregates all expert responses to produce a final judgment.]({{ '/images/04-2025/Routing_to_the_Right_Expertise:_A_Trustworthy_Judge_for_Instruction-based_Image_Editing/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the critical challenge of accurately and reliably evaluating Instruction-based Image Editing (IIE) models. While these models have advanced significantly, existing evaluation methods fall short. Conventional metrics like PSNR or CLIP fail to capture semantic nuances and alignment with human perception. At the same time, human evaluation is expensive, slow, and prone to inconsistency. Recent automated evaluators are often too specialized for a narrow range of tasks or lack flexibility. The paper identifies a gap for a trustworthy, explainable, and general-purpose automated evaluation framework for IIE that can achieve strong alignment with human judgment.

## 2. Key Ideas and Methodology
The paper introduces **JUdgement through Routing of Expertise (JURE)**, a framework that treats evaluation as a decomposable process. Instead of relying on a single, monolithic model to judge an edited image, JURE breaks down the evaluation into atomic sub-tasks.

The core methodology rests on two components:
1.  **Expert Pool**: A modular set of specialized models, each possessing a distinct "atomic expertise." For example, one expert handles object detection (DINO-X), another handles segmentation (SAM-2), and others perform style or similarity analysis (GPT-4o). These experts are designed to be easily updated or replaced.
2.  **Orchestrator**: A powerful Multimodal Large Language Model (the paper uses OpenAI's `o1`) that acts as the central decision-maker. It analyzes the editing instruction and images, dynamically routes evaluation queries to the most relevant expert(s) in the pool, aggregates their structured feedback, and synthesizes a final, explainable judgment with a score.

The process is iterative and context-aware: the Orchestrator calls experts sequentially, uses their output to inform subsequent steps, and can even chain experts together (e.g., using bounding boxes from an object detector to guide a segmentation model). This structured routing mechanism is designed to be more reliable and interpretable than a single "black-box" judge.

## 3. Datasets Used / Presented
The study utilizes the **Emu Edit test set**. From an initial pool of 2,022 instances, the authors filtered and categorized them into nine distinct IIE sub-tasks (e.g., Object Addition, Style Change), resulting in a set of 1,924 samples. For the primary experiments, a subset of **120 samples** was randomly selected for human evaluation. Nine human annotators were tasked with rating the outputs of three different IIE models (Emu Edit, HQ Edit, InstructPix2Pix) on a 1-5 Likert scale to establish a human judgment baseline.

## 4. Main Results
The primary metric for evaluation was the **Cohen's Kappa (κ) correlation**, which measures inter-rater agreement.
*   **Human-Human Agreement**: The agreement among the nine human annotators was variable, with κ scores ranging from **0.45 to 0.78**. This range was established as the benchmark for human-level performance.
*   **Baseline vs. Human**: A powerful standalone MLLM (`OpenAI o1`) used as a direct judge showed very poor alignment with human annotators, achieving negative κ scores (from -0.22 to -0.12).
*   **JURE vs. Human**: The proposed **JURE-o1** framework achieved a κ score of **0.54** against human judgments. This score falls squarely within the human-human agreement range, demonstrating that the framework successfully achieves human-level judgment quality.

The authors conclude that the expert routing mechanism is the key factor behind this significant performance gain, effectively closing the gap between automated evaluation and human perception for IIE tasks.

## 5. Ablation Studies
While a formal ablation section is not present, the paper provides two key analyses that serve a similar purpose:
1.  **Framework Ablation**: The comparison between the `OpenAI o1` baseline and the full `JURE-o1` framework acts as an ablation of the entire expert routing system. The dramatic improvement in Cohen's Kappa (from negative values to 0.54) demonstrates that the addition of the Expert Pool and the Orchestrator's routing logic is critical to achieving high-quality, human-aligned judgments.
2.  **Expert Routing Analysis**: The paper analyzes the invocation frequency of each expert across different IIE sub-tasks. The results confirm that the Orchestrator makes logical, task-aware decisions. For instance, the `Object Detection` expert was most frequently called for object manipulation tasks, while the `Style Analysis` expert was dominant for style change instructions. This analysis validates that the routing mechanism correctly identifies and utilizes the most relevant expertise for a given evaluation, confirming the effectiveness of the framework's core design.

## 6. Paper Figures
![Figure 2: Inter-Evaluator Agreement (Cohen’s Kappa) Heatmap. Values represent agreement levels between evaluators, where higher values indicate stronger alignment.]({{ '/images/04-2025/Routing_to_the_Right_Expertise:_A_Trustworthy_Judge_for_Instruction-based_Image_Editing/figure_2.jpg' | relative_url }})
![Figure 3: Expert Invocation Frequency in JURE-o1.]({{ '/images/04-2025/Routing_to_the_Right_Expertise:_A_Trustworthy_Judge_for_Instruction-based_Image_Editing/figure_3.jpg' | relative_url }})
