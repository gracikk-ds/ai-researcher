---
title: MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World
layout: default
date: 2025-04-21
---
## MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World
**Authors:**
- Ankit Dhiman, h-index: 4, papers: 10, citations: 50
- R Venkatesh Babu

**ArXiv URL:** http://arxiv.org/abs/2504.15397v1

**Citation Count:** None

**Published Date:** 2025-04-21

![Figure 1. Our model MirrorFusion 2.0, trained on our enhanced dataset SynMirrorV2 surpasses previous state-of-the-art diffusion-based inpainting models at the task of generating mirror reflections. All images were created by appending the prompt: “A perfect plane mirror reflection of ” to the object description. All text prompts can be found in the supplementary.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Diffusion models, despite their success in image editing, often fail to adhere to physical laws, particularly in generating realistic mirror reflections. Existing methods struggle to generalize to diverse object poses, positions, and complex multi-object scenes. This is largely due to the limited diversity and realism of the synthetic datasets they are trained on, creating a gap between generated images and physical reality.

## 2. Key Ideas and Methodology
The paper's core hypothesis is that enhancing the diversity of a synthetic training dataset can significantly improve a diffusion model's ability to generate physically accurate mirror reflections. The authors introduce **MirrorFusion 2.0**, a model that formulates reflection generation as a depth-conditioned inpainting task.

The methodology has two main components:
1.  **Enhanced Data Generation:** A new synthetic dataset, **SynMirrorV2**, is created with key augmentations: random object positioning, randomized rotations, and ensuring objects are grounded on surfaces. To handle complex scenes, the pipeline also pairs objects semantically (e.g., a chair with a table) to learn spatial relationships and occlusions.
2.  **Curriculum Training:** The model is trained using a three-stage curriculum to improve generalization. It is first trained on synthetic single-object scenes, then fine-tuned on synthetic multi-object scenes, and finally fine-tuned on a real-world dataset to bridge the synthetic-to-real domain gap. The model architecture is a dual-branch network based on BrushNet, which conditions a frozen U-Net on depth and mask information.

## 3. Datasets Used / Presented
*   **SynMirrorV2 (Presented):** A new large-scale synthetic dataset containing 207,610 images. It was created using 3D assets from Objaverse and ABO datasets and features single and multiple objects with varied poses, positions, and backgrounds. It was used for the first two stages of training.
*   **MirrorBenchV2 (Presented):** A new test set derived from SynMirrorV2, containing 2,991 single-object and 300 multi-object renderings to evaluate model performance.
*   **MSD (Used):** A real-world dataset used for the final fine-tuning stage and for qualitative evaluation on complex, real-world scenes.
*   **GSO (Used):** The Google Scanned Objects dataset was used for qualitative evaluation to test generalization to real-world scanned objects.

## 4. Main Results
The proposed model, MirrorFusion 2.0, demonstrates superior performance over the previous state-of-the-art baseline (MirrorFusion).
*   **Quantitative:** On the MirrorBenchV2 single-object test set, MirrorFusion 2.0 achieved a PSNR of 18.79 and an LPIPS of 0.108, outperforming the baseline's 18.31 PSNR and 0.122 LPIPS.
*   **Qualitative:** The model generates geometrically correct reflections that accurately capture object orientation and spatial relationships in both single and multi-object scenes, where the baseline often fails.
*   **User Study:** In a direct comparison, 84% of users preferred the reflections generated by MirrorFusion 2.0 over the baseline method.

## 5. Ablation Studies
*   **Impact of Multi-Object Data:** A model was trained without the multi-object data split. This version struggled to generate plausible reflections in multi-object scenes, leading to artifacts like object blending. Including the multi-object data improved performance across all metrics (e.g., PSNR on the multi-object test set increased from 17.77 to 18.00).
*   **Architectural Choice:** The proposed dual-branch architecture was compared against a standard Stable Diffusion Inpainting model adapted to accept depth maps (SDI+Depth). The dual-branch model produced superior results, avoiding the significant color leakage and artifacts observed in the single-branch alternative.

## 6. Paper Figures
![Figure 10. Limitations. Our method performs well in multi-object scenes (more than two objects) but retains some artifacts, which can be reduced by synthesizing the dataset through the proposed data-generation pipeline and further increasing the diversity and scale.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_10.jpg' | relative_url }})
![Figure 2. We observe that current state-of-the-art T2I models, SD3.5 [ 2 ] (top row) and Flux [ 22 ] (bottom row), face significant challenges in producing consistent and geometrically accurate reflections when prompted to generate reflections in the scene. 1. Introduction]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_2.jpg' | relative_url }})
![Figure 3. Dataset Generation Pipeline. Our dataset generation pipeline introduces key augmentations such as random positioning, rotation, and grounding of objects within the scene using the 3D-Positioner. Additionally, we pair objects in semantically consistent combinations to simulate complex spatial relationships and occlusions, capturing realistic interactions for multi-object scenes.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_3.jpg' | relative_url }})
![Figure 4. Comparison on MirrorBenchV2. The baseline fails to maintain accurate reflections and spatial consistency, showing (a) incorrect chair orientation and (b) distorted reflections of multiple objects. In contrast, our method correctly renders (a) the chair and (b) the sofas with accurate position, orientation, and structure, demonstrating superior performance.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_4.jpg' | relative_url }})
![Figure 5. Comparison on GSO [ 13 ] dataset. In (a), the baseline method misrepresents object structure, while our method preserves spatial integrity and produces realistic reflections. In (b), the baseline yields incomplete and distorted reflections of the mug, whereas our approach generates accurate geometry, color, and detail, showing superior performance on out-of-distribution objects.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_5.jpg' | relative_url }})
![Figure 6. Results on MirrorBenchV2. We compare our method with the baseline MirrorFusion [ 12 ] on MirrorBenchV2. The baseline method shown struggles with pose variations, even in single-object scenes, and fails to produce accurate reflections for multiple objects. In contrast, our method handles variations in the object orientation effectively and generates geometrically accurate reflections, even in complex, multi-object scenarios.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_6.jpg' | relative_url }})
![Figure 7. Real World Scenes. We show results for MirrorFusion [ 12 ], our method and our method fine-tuned on the MSD [ 52 ] dataset. We observe that our method can generate reflections capturing the intricacies of complex scenes, such as a cluttered cable on the table and the presence of two mirrors in a 3D scene.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_7.jpg' | relative_url }})
![Figure 8. Impact of adding multiple objects. We observe that training without multiple objects leads to (a) poor reflection generation and (b) artifacts like object blending, supporting the need for finetuning the model on such scenarios.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_8.jpg' | relative_url }})
![Figure 9. Comparison with SDI+Depth baseline. We observe color leakage issues in “SDI+Depth” generations. A dual-branch architecture proves to be a better choice, yielding superior outcomes.]({{ '/images/04-2025/MirrorVerse:_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World/figure_9.jpg' | relative_url }})
