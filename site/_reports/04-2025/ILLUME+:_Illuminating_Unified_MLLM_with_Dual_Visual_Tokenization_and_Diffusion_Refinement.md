---
title: ILLUME+:_Illuminating_Unified_MLLM_with_Dual_Visual_Tokenization_and_Diffusion_Refinement
layout: default
date: 2025-04-02
---
![Figure 1: ILLUME+ can understand and generate images at any resolution. Compared to our previous work, ILLUME [63], it demonstrates improved texture preservation in image editing tasks.]({{ '/images/04-2025/ILLUME+:_Illuminating_Unified_MLLM_with_Dual_Visual_Tokenization_and_Diffusion_Refinement/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing unified Multimodal Large Language Models (MLLMs) struggle to simultaneously excel at the three core capabilities of visual understanding, image generation, and image editing. Models either prioritize semantic understanding at the cost of texture preservation (hindering editing), or they focus on high-fidelity generation but lag in deep comprehension. Furthermore, some approaches decouple the input and output representations, which limits their ability to handle complex, interleaved image-text tasks. The authors aim to address this gap by creating a single, unified MLLM that can seamlessly perform all three tasks without compromising performance on any single one.

## 2. Key Ideas and Methodology
The core of the paper is ILLUME+, a unified MLLM built on three key architectural and training principles:
1.  **Dual Vision Tokenizer (DualViTok):** A novel tokenizer with two parallel branches. A *semantic branch* uses a pre-trained text-aligned encoder to capture high-level concepts, while a *pixel branch* uses a MoVQGAN-like architecture to preserve fine-grained textures. This dual representation ensures the model can both understand image content and reconstruct it faithfully for editing.
2.  **Unified MLLM with Coarse-to-Fine Generation:** The model uses a continuous-input, discrete-output scheme. It takes continuous features from DualViTok as input to avoid information loss during understanding tasks. For generation, it produces discrete tokens in a coarse-to-fine sequence—generating semantic tokens first, followed by pixel tokens—to improve text-visual alignment.
3.  **Diffusion Decoder for Refinement:** An optional diffusion model (based on SDXL) is used as a high-fidelity image decoder. It takes the discrete tokens generated by the MLLM and refines them into high-quality images, also enabling efficient super-resolution (e.g., from 256x256 to 512x512).

The model is trained using a progressive procedure that gradually increases image resolution and task complexity across three stages: visual embedding initialization, unified image-text alignment, and supervised fine-tuning.

## 3. Datasets Used / Presented
The model was trained on a large and diverse collection of data across several stages:
-   **Tokenizer & Diffusion Training:** Trained on a 63M image dataset (and a 10M subset for the diffusion decoder) comprising natural images (COYO, Wukong), aesthetic images, portraits, and documents/charts to learn a robust visual representation.
-   **MLLM Pre-training:** Trained on a 70M sample dataset combining image-text pairs (COYO, LLaVA-SFT), editing data (UltraEdit, SEED-Edit), and text-only data (OpenOrca) to align vision and language modalities.
-   **MLLM Supervised Fine-tuning (SFT):** Fine-tuned on a 9.7M instruction dataset (EMOVA-SFT, M4-Instruct, OmniEdit) to improve performance on specific downstream tasks.

## 4. Main Results
ILLUME+ demonstrates strong, often state-of-the-art, performance across understanding, generation, and editing benchmarks, despite using a relatively small 3B parameter LLM.
-   **Understanding:** On general benchmarks (e.g., MMBench, SEED), the 3B model is competitive with or outperforms 7B models like Janus-Pro-7B. It shows exceptional performance on document-oriented tasks (DocVQA, OCRBench), surpassing most existing unified models.
-   **Generation:** It achieves a state-of-the-art FID score of 6.00 on the MJHQ-30K benchmark, outperforming previous unified models and specialized generation models. It also scores highest (0.72) on the advanced compositional generation tasks in GenAI-bench.
-   **Editing:** On the Emu Edit benchmark, ILLUME+ surpasses specialized editing models, achieving a CLIP-T score of 0.872, indicating superior alignment between the edited image and the text instruction.

The authors claim that ILLUME+ provides a scalable and effective foundation for unifying understanding, generation, and editing in a single MLLM.

## 5. Ablation Studies
The authors conducted several ablation studies to validate their design choices:
-   **Dual vs. Single Tokenization:** Comparing the proposed DualViTok to single-branch (semantic-only or pixel-only) tokenizers showed that the dual approach is critical. The dual model significantly outperformed single-branch models in both image reconstruction (rFID of 1.83 vs. 2.08/5.48) and understanding (MMBench score of 70.9 vs. 50.4/30.7).
-   **Continuous vs. Discrete Input:** Using continuous features from the tokenizer as input to the LLM, rather than discrete tokens, dramatically improved performance on all understanding benchmarks. For example, the MMBench score increased from 66.6 to 70.9, and the VQA-text score rose from 42.4 to 56.2.
-   **DualViTok Components:** Systematically adding architectural improvements to the tokenizer, such as DC blocks for downsampling and a larger pixel codebook, progressively improved image reconstruction quality, reducing the rFID from a baseline of 1.83 to 1.33.
-   **Quantization Method:** Using SimVQ for quantization instead of vanilla VQ improved the reconstruction rFID from 2.24 to 1.83 and achieved 100% codebook utilization, confirming its effectiveness.

## 6. Paper Figures
![Figure 2: Characteristics comparison among existing unified models. Existing methods explore distinct paradigms to balance visual understanding, generation, and editing capabilities. Early approaches using VQGAN discretization struggle in understanding and context-aware generation tasks due to limited semantic alignment. Later frameworks incorporate semantic encoders, achieving better alignment but compromising texture preservation essential for fine-grained editing. ILLUME+ deep-integrates image understanding, generation, and editing into a single, unified architecture, enabling more intelligent and flexible interactions and task execution.]({{ '/images/04-2025/ILLUME+:_Illuminating_Unified_MLLM_with_Dual_Visual_Tokenization_and_Diffusion_Refinement/figure_2.jpg' | relative_url }})
![Figure 3: Architecture of ILLUME+. (a) The dual vision tokenizer preserves both semantic and texture information. (b) The diffusion refiner decodes discrete tokens into high-quality images. (c) The unified MLLM enables deep semantic interactions and context-aware image generation. (d) We introduce an unambiguous image representation of discrete tokens in a chain-of-thought pattern (semantic tokens first, followed by pixel tokens), resulting in improved generation performance.]({{ '/images/04-2025/ILLUME+:_Illuminating_Unified_MLLM_with_Dual_Visual_Tokenization_and_Diffusion_Refinement/figure_3.jpg' | relative_url }})
![Figure 4: Illustration of our progressive training pipeline. We first pre-train the dual-tokenizer system by reconstruction of the semantic and pixel information. We then fine-tune the diffusion model as a high-quality image decoder. The MLLM training consists of three main stages that gradually increase task resolution and complexity.]({{ '/images/04-2025/ILLUME+:_Illuminating_Unified_MLLM_with_Dual_Visual_Tokenization_and_Diffusion_Refinement/figure_4.jpg' | relative_url }})
