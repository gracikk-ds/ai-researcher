---
title: High-Resolution_Image_Editing_via_Multi-Stage_Blended_Diffusion
layout: default
date: 2022-10-24
---
## High-Resolution Image Editing via Multi-Stage Blended Diffusion
**Authors:**
- Johannes Ackermann
- Minjun Li, h-index: 8, papers: 11, citations: 504

**ArXiv URL:** http://arxiv.org/abs/2210.12965v1

**Citation Count:** None

**Published Date:** 2022-10-24

![Figure 1: Our approach performs high-resolution text-guided image editing in multiple stages. In the ﬁrst stage a), we apply Blended Diffusion [2], given a masked region and a text prompt. In each following stage b), we ﬁrst upscale the image using an off the shelf super-resolution model and then use Blended Diffusion, starting at an intermediate diffusion step, to improve the image quality and ensure consistency with the input prompt. c) When the output resolution of a stage is too large to ﬁt into the GPU memory, we split the image into multiple segments, apply upscaling and Blended Diffusion to them separately and alpha-composite the results.]({{ '/images/10-2022/High-Resolution_Image_Editing_via_Multi-Stage_Blended_Diffusion/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Diffusion models have demonstrated impressive capabilities in text-guided image editing. However, their high computational cost restricts their application to low-resolution images (e.g., 512x512 pixels). Directly applying these pre-trained models to high-resolution inputs leads to incoherent results, such as repeated patterns and artifacts. The authors address this practical problem by proposing a method to perform coherent, high-fidelity image editing on megapixel-scale images using an existing low-resolution diffusion model.

## 2. Key Ideas and Methodology
The paper introduces a multi-stage pipeline that iteratively refines an image edit from low to high resolution.

*   **Core Principle:** The core idea is to perform the initial creative edit at a low resolution where the diffusion model is effective, and then use subsequent stages to upscale the image while refining details and maintaining global consistency.
*   **Approach:**
    1.  **First Stage:** The image is cropped and downscaled. The initial edit is performed using Blended Diffusion combined with the Repaint technique to ensure good blending with the surrounding context. The best result from a batch of generations is selected via CLIP reranking.
    2.  **Intermediate Stage(s):** The low-resolution edit is upscaled using a super-resolution model (Real-ESRGAN). Blended Diffusion is then applied again, starting from an intermediate noise step, to enhance details and enforce consistency with the text prompt. A key step is to low-pass filter the unedited background to match the sharpness of the upscaled region, which prevents blurring artifacts.
    3.  **High-Resolution Handling:** For resolutions too large for GPU memory, the image is processed in overlapping segments, which are then seamlessly recombined using alpha-compositing.
*   **Refinement:** Decoder optimization is performed after each stage to minimize visual seams between the edited and original parts of the image.

## 3. Datasets Used / Presented
The authors do not present a new dataset. Their method is built upon the pre-trained **Stable Diffusion v1.4** model, which was trained on the LAION dataset. For evaluation, they use a variety of publicly available images from sources like Wikimedia Commons and Unsplash, as well as images generated by the authors. They also use the **DALL-E 2** web interface as a baseline for comparison.

## 4. Main Results
The proposed method was qualitatively evaluated against two strong baselines: (1) applying Blended Diffusion directly at the highest possible resolution (960x960) and (2) using DALL-E 2 to inpaint the target region in multiple segments.

*   The baselines produced globally inconsistent results, such as generating multiple heads on a statue or floating, misaligned objects.
*   In contrast, the authors' multi-stage approach successfully generated globally coherent and detailed high-resolution images, effectively overcoming the limitations of direct high-resolution editing.

## 5. Ablation Studies
The authors performed two main ablation studies to validate their design choices:

*   **Upscaling Method:** They compared their full upscaling process (Real-ESRGAN + Blended Diffusion with a low-pass filtered background) against simpler methods like bilinear upscaling, ESRGAN alone, or omitting the low-pass filter. Their proposed method produced significantly better blending and visual fidelity. Notably, removing the low-pass filter resulted in a blurrier edited region.
*   **Repaint Steps:** They analyzed the impact of the number of Repaint steps (R) in the initial stage. Using no repaint steps (R=0) often led to poor blending between the edited and unedited areas. They found that a moderate number of steps (R=5) provided a robust balance, improving results without introducing artifacts from excessive repainting.

## 6. Paper Figures
![Figure 2: Comparison of our approach to two baselines for the prompt “Statue of Roman Emperor, Canon 5D Mark 3, 35mm, ﬂickr”. From left to right: Blended Diffusion applied to 960x960 pixels followed by bilinear upscaling, Dall-E 2 editing in multiple segments, our proposed approach. The size of the mask is 1166x2297 pixels. Applying Blended Diffusion directly to the higher resolution leads to incoherent generation with repeated elements. Similarly, Dall-E 2 generates two statues, with one ﬂoating above the other. Our approach is able to generate a detailed, coherent image.]({{ '/images/10-2022/High-Resolution_Image_Editing_via_Multi-Stage_Blended_Diffusion/figure_2.jpg' | relative_url }})
![Figure 3: Our approach, with all intermediate results being shown. Best viewed zoomed in.]({{ '/images/10-2022/High-Resolution_Image_Editing_via_Multi-Stage_Blended_Diffusion/figure_3.jpg' | relative_url }})
![Figure 4: Comparison of our approach to two baselines. Left: we directly apply Blended Diffusion to the highest resolution we can ﬁt into the VRAM (960x960 pixels) and then bilinearly upscale the output. Middle: We use the Dall-E 2 web UI to edit the image at its full resolution. Due to the edited region being larger than the 1024x1024 generation window, we have to apply Dall-E to multiple independent segments. Right: Our proposed approach. We ﬁnd that directly applying Blended Diffusion leads to repeated elements (two heads, two mountains, two pictures) and fails to produce ﬁne details (hair). Dall-E 2 produces visually high-ﬁdelity images, but fails to produce globally coherent images (ﬂoating statues, four paintings). Our method produces globally consistent images while providing a similar visual ﬁdelity. Note that the full images are downscaled. The zoomed-in regions measure 512x512 pixels and are shown at full resolution.]({{ '/images/10-2022/High-Resolution_Image_Editing_via_Multi-Stage_Blended_Diffusion/figure_4.jpg' | relative_url }})
![Figure 5: Ablation of different upscaling methods, applied after the Blended Diffusion in the ﬁrst stage with ﬁxed seeds. a) Bilinear upscaling, b) ESRGAN, c) ESRGAN + unconditional diffusion, d) ESRGAN + text-conditional diffusion, e) ESRGAN + text-conditional Blended Diffusion, f) ESRGAN + text-conditional Blended Diffusion with a low-pass ﬁltered background. Note that the images are downscaled from the full resolution.]({{ '/images/10-2022/High-Resolution_Image_Editing_via_Multi-Stage_Blended_Diffusion/figure_5.jpg' | relative_url }})
![Figure 6: Comparison of different repaint steps R without clip reranking. Without repaint, the edited region often does not blend well with the background, while too many repaint steps can also cause unrealistic images (corgi, hair). We generally ﬁnd R = 5 to work best. Note that the images are downscaled from the full resolution.]({{ '/images/10-2022/High-Resolution_Image_Editing_via_Multi-Stage_Blended_Diffusion/figure_6.jpg' | relative_url }})
