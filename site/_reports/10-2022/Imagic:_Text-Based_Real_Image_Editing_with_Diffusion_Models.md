---
title: Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models
layout: default
date: 2022-10-17
---
![“A children’s drawing of a waterfall” Figure 1. Imagic – Editing a single real image. Our method can perform various text-based semantic edits on a single real input image, including highly complex non-rigid changes such as posture changes and editing multiple objects. Here, we show pairs of 1024 ˆ 1024 input (real) images, and edited outputs with their respective target texts.]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Current text-based image editing methods are limited in their capabilities. They often restrict users to specific types of edits (e.g., style transfer, object addition), work only on synthetically generated images, or require additional inputs beyond the image and text prompt, such as masks or multiple views of an object. The authors address this gap by developing a method that can perform complex, non-rigid semantic edits (like changing an animal's posture) on a single, real-world, high-resolution image, using only a target text description for the desired change.

## 2. Key Ideas and Methodology
The core idea is to leverage a pre-trained text-to-image diffusion model to find a representation that combines the content of the input image with the semantics of the target text. The method, named Imagic, operates in a three-stage process:

1.  **Text Embedding Optimization:** The model first optimizes a text embedding, starting from the embedding of the target text, to find a new embedding (`e_opt`) that best reconstructs the original input image.
2.  **Model Fine-Tuning:** The diffusion model's weights are then fine-tuned using this optimized embedding (`e_opt`) to ensure high-fidelity reconstruction of the input image's specific details and appearance.
3.  **Interpolation and Generation:** Finally, the model linearly interpolates between the optimized embedding (`e_opt`) and the original target text embedding (`e_tgt`). This new, interpolated embedding is fed into the fine-tuned diffusion model to generate the final edited image, which balances fidelity to the original image with alignment to the target text.

The framework is built upon powerful diffusion models like Imagen and Stable Diffusion.

## 3. Datasets Used / Presented
The authors use high-resolution, free-to-use images from public sources like Unsplash and Pixabay for qualitative demonstrations. For quantitative evaluation and comparison, they introduce a new benchmark:

*   **TEdBench (Textual Editing Benchmark):** A novel collection of 100 pairs of input images and target text prompts. This benchmark is specifically designed to evaluate complex, non-rigid text-based image edits and was used to conduct a human perceptual study.

## 4. Main Results
Qualitative results demonstrate that Imagic can successfully perform a wide range of complex edits—such as making a dog sit, a bird spread its wings, or two parrots kiss—while preserving the subject's identity and background details.

In a quantitative user study on the TEdBench benchmark, Imagic was compared against three leading methods (SDEdit, DDIB, and Text2LIVE). Human evaluators showed a strong preference for Imagic's results, choosing it over the alternatives more than 70% of the time in all head-to-head comparisons. The authors claim Imagic is the first technique to successfully apply such sophisticated, non-rigid edits to a single real image from a text prompt alone.

## 5. Ablation Studies
*   **Model Fine-Tuning:** The authors compared results with and without the model fine-tuning step. Without fine-tuning, the model failed to faithfully reconstruct the original image and lost important details during the edit. This demonstrated that fine-tuning is essential for preserving the image's specific characteristics.
*   **Number of Text Embedding Optimization Steps:** The effect of optimizing the text embedding for 10, 100, and 1000 steps was evaluated. Optimizing for only 10 steps was insufficient to capture the input image's essence. While 1000 steps offered marginal improvement over 100, it significantly increased runtime. Thus, 100 steps was found to be the optimal trade-off.
*   **Interpolation Intensity (η):** The study analyzed the trade-off between fidelity to the input image and alignment with the target text by varying the interpolation parameter `η`. Results showed that values between 0.6 and 0.8 typically produce the most satisfactory edits, balancing both objectives effectively.

## 6. Paper Figures
![“Pizza with pepperoni” Figure 10. Failure cases. Insufﬁcient consistency with the target text (top), or changes in camera viewing angle (bottom).]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_10.jpg' | relative_url }})
![Figure 2. Different target texts applied to the same image. Imagic edits the same image differently depending on the input text.]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_2.jpg' | relative_url }})
![Figure 3. Schematic description of Imagic . Given a real image and a target text prompt: (A) We encode the target text and get the initial text embedding e tgt , then optimize it to reconstruct the input image, obtaining e opt ; (B) We then ﬁne-tune the generative model to improve ﬁdelity to the input image while ﬁxing e opt ; (C) Finally, we interpolate e opt with e tgt to generate the ﬁnal editing result.]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_3.jpg' | relative_url }})
![Figure 4. Multiple edit options. Imagic utilizes a probabilistic model, enabling it to generate multiple options with different random seeds.]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_4.jpg' | relative_url }})
![Figure 5. Smooth interpolation. We can smoothly interpolate between the optimized text embedding and the target text embedding, resulting in a gradual editing of the input image toward the required text as η increases (See animated GIFs in supplementary material).]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_5.jpg' | relative_url }})
![Figure 6. Method comparison. We compare SDEdit [ 38 ], DDIB [ 62 ], and Text2LIVE [ 8 ] to our method. Imagic successfully applies the desired edit, while preserving the original image details well. into the 1024 ˆ 1024 resolution. By cascading these 3 models [ 23 ] and using classiﬁer-free guidance [ 24 ], Imagen constitutes a powerful text-guided image generation scheme. We optimize the text embedding using the 64 ˆ 64 diffusion model and the Adam [ 34 ] optimizer for 100 steps and a ﬁxed learning rate of 1 e ´ 3 . We then ﬁne-tune the 64 ˆ 64 diffusion model by continuing Imagen’s training for 1500 steps for our input image, conditioned on the optimized embedding. In parallel, we also ﬁne-tune the 64 ˆ 64 Ñ 256 ˆ 256 SR diffusion model using the target text embedding and the original image for 1500 steps, in order to capture high-frequency details from the original image. We ﬁnd that ﬁne-tuning the 256 ˆ 256 Ñ 1024 ˆ 1024 model adds little to no effect to the results, therefore we opt to use its pre-trained version conditioned on the target text. This entire optimization process takes around 8 minutes per image on two TPUv4 chips. Afterwards, we interpolate the text embeddings according to Equation 3 . Because of the ﬁne-tuning process, using η “ 0 will generate the original image, and as η increases, the image will start to align with the target text. To maintain both image ﬁdelity and target text alignment, we choose an intermediate η , usually residing between 0 . 6 and 0 . 8 (see Figure 9 ). We then generate with Imagen [ 53 ] with its pro-]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_6.jpg' | relative_url }})
![Figure 7. Embedding interpolation. Varying η with the same seed, using the pre-trained (top) and ﬁne-tuned (bottom) models.]({{ '/images/10-2022/Imagic:_Text-Based_Real_Image_Editing_with_Diffusion_Models/figure_7.jpg' | relative_url }})
