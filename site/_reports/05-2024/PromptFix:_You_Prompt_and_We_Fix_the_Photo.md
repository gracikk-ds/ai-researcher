---
title: PromptFix:_You_Prompt_and_We_Fix_the_Photo
layout: default
date: 2024-05-27
---
## PromptFix: You Prompt and We Fix the Photo
**Authors:**
- Yongsheng Yu
- Jiebo Luo

**ArXiv URL:** http://arxiv.org/abs/2405.16785v2

**Citation Count:** 28

**Published Date:** 2024-05-27

![Figure 1: We propose PromptFix, a unified diffusion model capable of performing multiple imageprocessing tasks. It can understand user-customized editing instructions and perform the corresponding tasks with high quality. One of the key advantages of PromptFix is high-frequency information preservation, ensuring that image details are maintained throughout VAE decoding. PromptFix can handle various images with different aspect ratios.]({{ '/images/05-2024/PromptFix:_You_Prompt_and_We_Fix_the_Photo/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Existing instruction-guided diffusion models excel at high-level image editing but struggle with low-level image processing tasks like restoration. This is due to two primary problems: (1) a lack of large-scale, diverse datasets that pair degraded images with corresponding natural language instructions, and (2) the inherent architecture of diffusion models (specifically VAE compression) and their stochastic sampling process, which often leads to the loss of fine-grained, high-frequency details (e.g., text, textures) that are critical to preserve in restoration tasks.

## 2. Key Ideas and Methodology
The paper introduces PromptFix, a unified framework designed to perform a wide variety of image processing tasks based on user instructions. The core methodology is built on two key innovations:

*   **High-frequency Guidance Sampling (HGS):** To prevent the loss of fine details, the authors propose a novel sampling method. During the denoising process, it explicitly calculates a "fidelity constraint" by comparing the high-frequency components (extracted via Fourier and Sobel filters) of the original input image with the currently predicted output. This guidance signal is used to correct the diffusion trajectory, ensuring that important details from unprocessed areas are preserved in the final image.
*   **VLM-based Auxiliary Prompt Module:** To improve the model's understanding of image degradation and enhance its zero-shot capabilities, an auxiliary prompt is generated by a Vision-Language Model (VLM) like LLaVA. This prompt describes both the semantic content of the degraded image and its specific flaws (e.g., "blurry," "hazy," "low-light"). This auxiliary information is fed into the diffusion model through separate cross-attention layers, supplementing the user's primary instruction and enabling effective blind restoration even without an explicit command.

## 3. Datasets Used / Presented
The authors construct and present a new large-scale dataset named **PromptFix Dataset**.
*   **Size and Content:** It contains approximately 1.01 million "input-goal-instruction" triplets.
*   **Domain:** The dataset is tailored for low-level image processing and editing, covering seven main tasks: image dehazing, colorization, super-resolution, low-light enhancement, snow removal, watermark removal, and object removal/creation.
*   **Creation:** The dataset was created by collecting image pairs from various existing datasets (e.g., Mulan, RESIDE, LOL, Laion-5b) and then using GPT-4 to generate diverse and high-quality textual instructions for each pair.

## 4. Main Results
PromptFix was benchmarked against other instruction-driven and generalist restoration models on seven different tasks.
*   **Quantitative Results:** In quantitative comparisons using LPIPS (lower is better) and ManIQA (higher is better), PromptFix consistently outperformed other instruction-driven methods. For instance, in object removal, PromptFix achieved an LPIPS/ManIQA of 0.054/0.810, surpassing InstructDiff (0.071/0.811). It also demonstrated superior performance in colorization, watermark removal, and super-resolution.
*   **Takeaway:** The authors claim that PromptFix sets a new benchmark for instruction-guided image processing, demonstrating robust performance across a wide variety of tasks and excelling in multi-task and blind restoration scenarios where other models falter.

## 5. Ablation Studies
Several ablation studies were conducted to validate the contributions of PromptFix's key components:
*   **High-frequency Guidance Sampling (HGS):** A qualitative comparison showed that without HGS, the model failed to preserve fine textual details on an object during low-light enhancement. With HGS, these details were successfully retained. Quantitatively, adding HGS improved performance, for example, reducing the LPIPS score from 0.2068 to 0.1600.
*   **VLM-guided Blind Restoration:** The model was tested on restoration tasks without any user instructions, relying solely on the VLM-generated auxiliary prompt. The results showed that PromptFix achieved performance comparable to or better than fully-supervised baselines, demonstrating strong zero-shot restoration capabilities. For example, in dehazing, it achieved an LPIPS of 0.148, competitive with specialist models.
*   **Multi-task Processing:** When tested on images with three combined degradations, PromptFix significantly outperformed all baselines (e.g., InstructP2P, AirNet) across all metrics (PSNR, SSIM, LPIPS, ManIQA), highlighting its versatility.
*   **Instruction Prompt Generalization:** The model showed only a negligible performance drop when tested with out-of-training human instructions of varying lengths, confirming its robustness and generalization to novel commands.

## 6. Paper Figures
![Figure 2: The architecture of our proposed PromptFix.]({{ '/images/05-2024/PromptFix:_You_Prompt_and_We_Fix_the_Photo/figure_2.jpg' | relative_url }})
![Figure 3: Qualitative comparison between PromptFix and other instruct-driven diffusion methods (InstructP2P [ 7 ], InstructDiff [ 21 ], and MGIE [ 19 ]) for image processing, as well as low-level generalist techniques (PromptIR [50], AirNet [36], and Diff-Plugin [44]) for image restoration.]({{ '/images/05-2024/PromptFix:_You_Prompt_and_We_Fix_the_Photo/figure_3.jpg' | relative_url }})
![Figure 4: Qualitative analysis of VLM-guided blind restoration for desnowing, dehazing, and lowlight enhancement. The results are obtained from PromptFix without explicit task instructions, relying solely on the input image. The auxiliary prompt, automatically generated by a VLM during inference, includes semantic captions and defect descriptions, indicated by < blue > and < yellow > tags, respectively.]({{ '/images/05-2024/PromptFix:_You_Prompt_and_We_Fix_the_Photo/figure_4.jpg' | relative_url }})
![Figure 5: Preservation of low-level image details using the proposed High-frequency Guidance Sampling (HGS) method, compared to previous VAE-based baselines [ 19 , 21 , 44 ] utilizing stablediffusion architecture.]({{ '/images/05-2024/PromptFix:_You_Prompt_and_We_Fix_the_Photo/figure_5.jpg' | relative_url }})
