---
title: FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction
layout: default
date: 2024-09-26
---
![Fig. 1. Comparison between mask-based paradigm and mask-free paradigm, the former requires the user to provide the source mask to specify the editing area, while the latter conditions the diffusion model with language instructions, without the need for the masks. Reference-based inpainting conditions the model on reference image embedding, no longer supporting natural language. We use multi-modal instruction to introduce reference image features while still retaining the perception of natural language.]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
Current instruction-driven image editing methods often struggle to accurately capture a user's visual intent using text alone. While reference-based editing offers a more precise way to specify visual concepts (e.g., a specific object's appearance), existing approaches have significant drawbacks. They either force the user to provide a manual mask to define the editing area, which is cumbersome, or they fail to faithfully reproduce the fine-grained details of the reference image. Furthermore, there is a lack of high-quality datasets specifically designed for this task. The authors aim to bridge this gap by developing a mask-free, reference-based editing framework that is both user-friendly and capable of high-fidelity concept reproduction.

## 2. Key Ideas and Methodology
The paper introduces **FreeEdit**, a novel framework for reference-based image editing that operates without requiring manual masks. Its core idea is to interpret user-friendly multi-modal instructions (a combination of text and a reference image) to guide the editing process.

The methodology is built around a diffusion model with three key components:
1.  **Multi-modal Instruction Encoder:** A Q-Former module processes the text instruction and reference image to generate a unified embedding. This embedding implicitly localizes the editing region and specifies the desired change, removing the need for a mask.
2.  **Detail Extractor:** A separate U-Net-based encoder extracts fine-grained, multi-scale features from the reference image to ensure the edited object's appearance and identity are accurately preserved.
3.  **Decoupled Residual Refer-Attention (DRRA):** To integrate the reference details without compromising the model's original editing capabilities, the authors propose the DRRA module. Unlike prior work that merges reference attention with self-attention, DRRA injects reference features as a separate, residual connection. This preserves the integrity of the original self-attention mechanism and allows for better control over the reference's influence.

## 3. Datasets Used / Presented
The authors introduce **FreeBench**, a new, large-scale dataset created specifically for reference-based, instruction-driven editing.
*   **Construction:** Built upon the OpenImages dataset, it uses a novel "twice-repainting" scheme to generate triplets of (original image, edited image, reference image) that maintain strong identity consistency for the edited object.
*   **Content:** It contains **131,160** filtered edits, covering tasks like object addition, replacement, and removal. Each entry is paired with a detailed multi-modal instruction generated using multiple MLLMs.
*   **Usage:** FreeBench is used to train and evaluate the FreeEdit model. The model is also fine-tuned using the **MagicBrush** dataset and evaluated on the **Emu Edit** and **VitonHD** test sets for other tasks.

## 4. Main Results
FreeEdit demonstrates superior performance in reference-based editing compared to both mask-based and mask-free baselines.
*   **Quantitative:** On a custom benchmark, FreeEdit achieves the best performance among mask-free methods and is highly competitive with mask-based methods. It obtains the lowest L2 error (0.0286) of all tested models and achieves high scores for reference similarity (CLIP-R: 78.12) and original image preservation (DINO-I: 55.16), despite not using a mask.
*   **Qualitative:** Visual results show that FreeEdit generates edits that are more accurate and harmonious than those from other methods. It successfully follows complex instructions that cause other models to fail or produce artifacts.
*   **Author-claimed Impact:** FreeEdit significantly advances the state-of-the-art for reference-based editing by providing a versatile, mask-free solution that achieves high-quality results through convenient language instructions.

## 5. Ablation Studies
The authors conduct extensive ablation studies to validate each component of their framework.
*   **Refer-Attention Module:** Comparing their proposed **DRRA** against a naive self-attention and a combined attention mechanism (RASA) shows that DRRA is most effective. It improves reference similarity (DINO-R score from 51.26 to 56.71) by successfully injecting details without disrupting the model's core functionality.
*   **Dataset Diversity:** Training the model with a mix of addition, replacement, and removal tasks, rather than just one type, improves overall performance and image quality (CLIP-IQA score increases from 0.652 to 0.705).
*   **Instruction Recaptioning:** Using MLLMs to generate detailed instructions, instead of simple category labels, improves the model's ability to preserve the original image structure (DINO-I score increases from 51.54 to 53.38).
*   **Quality Tuning:** A final fine-tuning stage on a curated high-quality dataset significantly boosts performance across all metrics, particularly in preserving the original image content (CLIP-I score from 76.95 to 81.59).

## 6. Paper Figures
![Fig. 2. The overall pipeline of our proposed FreeEdit, which consists of three components: (a) Multi-modal instruction encoder. (b) Detail extractor. (c) Denosing U-Net. Text instruction and reference image are firstly fed into the multi-modal instruction encoder to generate multi-modal instruction embedding. The reference image is additionally fed into the detail extractor to obtain fine-grained features. The original image latent is concatenated with the noise latent to introduce the original image condition. Denosing U-Net accepts the 8-channel input and interacts with the multi-modal instruction embedding through cross-attention. The DRRA modules which connect the detail extractor and the denoising U-Net, are used to integrate fine-grained features from the detail extractor to promote ID consistency with the reference image. (d) The editing examples obtained using FreeEdit.]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_2.jpg' | relative_url }})
![Fig. 4. Pipeline for dataset construction and examples of training samples. (a) Image triplet construction. We repaint the source image in the existing real-world segmentation dataset twice to form the image triplet. (b) Instruction Construction. We use multiple powerful MLLMs to caption the generated image, and combine the resulting local descriptions with instruction templates to form edit instructions. (c) Examples of the training dataset. The item in the dataset contains images before and after editing and a multi-modal instruction.]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_4.jpg' | relative_url }})
![Fig. 5. Statistics for the FreeBench dataset. The first four parent classes in FreeBench are animals, food, kitchenware, and vehicles. FreeBench covers the vast majority of categories in daily life, allowing us to train a generalizable zero-shot reference-based image editing model.]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_5.jpg' | relative_url }})
![Fig. 6. Qualitative comparisons of FreeEdit with previous methods, including mask-based methods PaintByExample, AnyDoor, MimicBrush, and mask-free methods InstructPix2Pix, Kosmos-G. Mask-based methods require the user to manually provide the mask of the editing area. AnyDoor also needs to provide a foreground mask of the reference image, and the editing mask fed to AnyDoor will be processed as a box because its training is box-based. Mask-free methods are language-based and donâ€™t require additional mask input, where we take InstructPix2Pix with detailed instructions as a baseline for comparison. The inputs required for each method are marked below each line of images. S* denotes the specific visual concept in the reference image, and O* denotes the original image to be edited.]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_6.jpg' | relative_url }})
![Fig. 7. FreeEdit uses language instructions for convenient editing control without the introduction of manual masks, also capable of doing it when there are multiple objects in the original images.]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_7.jpg' | relative_url }})
![Fig. 8. User study results. Left: User study for the reference-based image editing task, we compare FreeEdit with existing mask-based methods AnyDoor [26], PaintByExample [24], MimicBrush [13]. Right: User study for the object removal task, we compare FreeEdit with InstructPix2Pix [21] and SD-Inpainting [9].]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_8.jpg' | relative_url }})
![Fig. 9. Qualitative ablation study on the refer-attention modules. w/o RA. denotes the primitive self-attention module without the introduction of reference features. RASA denotes Refer-Attention in Self-Attention. DRRA denotes Decoupled Residual Refer-Attention.]({{ '/images/09-2024/FreeEdit:_Mask-free_Reference-based_Image_Editing_with_Multi-modal_Instruction/figure_9.jpg' | relative_url }})
