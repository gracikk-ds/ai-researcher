---
title: Add-SD:_Rational_Generation_without_Manual_Reference
layout: default
date: 2024-07-30
---
## Add-SD: Rational Generation without Manual Reference
**Authors:**
- Lingfeng Yang, h-index: 8, papers: 21, citations: 536
- Jian Yang

**ArXiv URL:** http://arxiv.org/abs/2407.21016v1

**Citation Count:** 0

**Published Date:** 2024-07-30

![Fig. 1 The proposed Add-SD pipeline begins with the creation of a RemovalDataset containing image pairs via random instance removal. These datasets are then employed to fine-tune image-to-image generation using the Stable Diffusion Model. Next, generation occurs on the entire dataset by sampling rare classes to alleviate the long-tail issue. Finally, synthetic images are integrated into the original dataset to enhance downstream tasks.]({{ '/images/07-2024/Add-SD:_Rational_Generation_without_Manual_Reference/figure_1.jpg' | relative_url }})
## 1. Motivation of the Paper
The authors address the challenge of automatically adding objects to images in a realistic and rational manner (i.e., with plausible sizes, positions, and context) using only simple text instructions. Existing generative methods often require costly manual references like bounding boxes, or they struggle to maintain background consistency and generate rationally placed objects when adding something new to a scene. This paper aims to create a flexible object addition pipeline that overcomes these limitations, relying solely on text prompts without any manual layout design.

## 2. Key Ideas and Methodology
The core idea is a "reverse thinking" approach to create high-quality training data. Instead of generating an edited image from a text prompt, the method starts with a real image, removes an existing object, and then trains a model to perform the inverse operation. This ensures that the generated training pairs have perfect background consistency and the object's original placement serves as a rational ground truth.

The methodology, named **Add-SD**, consists of three stages:
1.  **Dataset Creation:** A new dataset, `RemovalDataset`, is created. For a given real image, an object instance is removed using an inpainting model (LaMa). This creates an `(original_image, removal_image)` pair. A corresponding text instruction (e.g., "Add a person.") is generated for this pair.
2.  **Model Fine-tuning:** A Stable Diffusion model is fine-tuned on the `RemovalDataset`. It learns to take the `removal_image` and the text instruction as input to reconstruct the `original_image`. This transforms the model into an instruction-guided object addition tool.
3.  **Synthetic Data Augmentation:** The trained Add-SD model is used to generate synthetic data for downstream tasks like object detection. To ensure rational additions, a "super-label" sampling strategy is used, where new objects are sampled from categories that are contextually similar to objects already present in the image. This is particularly used to generate more examples of rare classes to alleviate the long-tail problem.

## 3. Datasets Used / Presented
-   **RemovalDataset (Presented):** A new dataset constructed by the authors for fine-tuning the Add-SD model. It was created by removing objects from images sourced from several existing datasets: **COCO**, **LVIS**, **Visual Genome**, and **RefCOCO**.
-   **COCO & LVIS (Used for Evaluation):** These standard object detection and instance segmentation datasets were used for the downstream evaluation. The authors augmented these datasets with synthetic images generated by Add-SD and measured the performance improvement of object detectors trained on the combined data.

## 4. Main Results
-   **Image Editing Quality:** In a blind user study comparing Add-SD to methods like InstructPix2Pix and MagicBrush, Add-SD was strongly preferred for generation quality (48.8% preference), rationality (47.0%), and background consistency (63.6%). It also outperformed other methods on quantitative metrics like L1/L2 distance and CLIP/DINO similarity.
-   **Downstream Task Improvement:** The primary impact is shown in object detection. Augmenting the LVIS dataset with Add-SD's synthetic data improved the performance of a CenterNet2 detector by **+4.3 mAP on rare classes** and +1.0 mAP overall compared to the baseline. On the COCO dataset, it also achieved the highest scores among competing synthetic data augmentation methods.

## 5. Ablation Studies
-   **Synthetic Data Sampling Ratio:** The authors tested mixing different ratios of synthetic and real data for training a detector. A mixture containing 20% synthetic data yielded the best performance on COCO, improving the baseline by +0.9 box AP.
-   **Sampling Strategy:** A "Batch Sampling" strategy (where each training batch contains either only real or only synthetic data) was found to be slightly more effective than mixing both types of data within each batch.
-   **Super-label Selection Strategy:** Using the proposed context-aware "super-label" strategy to select which objects to add resulted in better detection performance (+0.3 box AP) compared to not using it, confirming that generating more contextually reasonable objects is beneficial.
-   **Scale of Synthetic Data:** Increasing the number of generated synthetic samples for LVIS from 100K to 200K further improved detection performance, demonstrating the scalability and generative diversity of the Add-SD model.

## 6. Paper Figures
![Fig. 2 Our pipeline uses an object removal operation, which ensures that image pairs have a consistent background.]({{ '/images/07-2024/Add-SD:_Rational_Generation_without_Manual_Reference/figure_2.jpg' | relative_url }})
![Fig. 3 Visualization of RemovalDataset. The first row shows the removal of a single object from COCO images. The second row involves removing partial instances to facilitate multiple generations. The LVIS dataset is similar to COCO but includes more fine-grained categories. The VG and RefCOCO datasets specify target captions containing attribute and relation information.]({{ '/images/07-2024/Add-SD:_Rational_Generation_without_Manual_Reference/figure_3.jpg' | relative_url }})
![Fig. 4 We design a super-label-based sampling strategy to restrict the category of the added object, ensuring rationality. Then, we randomly sample a sub-label within the super-label, assigning a higher weight to tail-class labels to alleviate the long-tail problem. After image generation, the annotations are inherited from the vanilla dataset (green) for the original instance and grounded (red) for the added instance.]({{ '/images/07-2024/Add-SD:_Rational_Generation_without_Manual_Reference/figure_4.jpg' | relative_url }})
![Fig. 5 Copy-Paste [ 31 ] and X-Paste [ 27 ] pass through a complex synthetic data generation pipeline and may present irrational augmentations. Our Add-SD synthetic augmentation is simple and effective.]({{ '/images/07-2024/Add-SD:_Rational_Generation_without_Manual_Reference/figure_5.jpg' | relative_url }})
![Fig. 6 Generation visualization and grounding results. Add-SD generates diverse synthetic images with new instances, which are then grounded to boxes for training on downstream tasks.]({{ '/images/07-2024/Add-SD:_Rational_Generation_without_Manual_Reference/figure_6.jpg' | relative_url }})
![Fig. 7 The instance count for each categories before and after generation and corresponding growth rate over the original on COCO and LVIS datasets.]({{ '/images/07-2024/Add-SD:_Rational_Generation_without_Manual_Reference/figure_7.jpg' | relative_url }})
